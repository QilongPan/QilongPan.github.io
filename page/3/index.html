<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="潘其龙">
<meta property="og:url" content="http://example.com/page/3/index.html">
<meta property="og:site_name" content="潘其龙">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="QilongPan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>潘其龙</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">潘其龙</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/" class="post-title-link" itemprop="url">DouZero:Mastering DouDizhu with Self-Play Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-22 21:51:20" itemprop="dateCreated datePublished" datetime="2021-06-22T21:51:20+08:00">2021-06-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-23 22:52:30" itemprop="dateModified" datetime="2021-06-23T22:52:30+08:00">2021-06-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    游戏是现实世界的抽象，在那里人造的代理学会与其他代理竞争和合作。尽管在各种完美和不完美信息游戏中取得了重大成就，但三人纸牌游戏斗地主（又名斗地主）仍未解决。斗地主是一个非常具有挑战性的领域，竞争、协作、信息不完善、状态空间大，尤其是大量可能的行为（合法行为在不同回合之间差异很大）。不幸的是，现代强化学习算法主要关注简单和小的动作空间，毫不奇怪，在斗地主上没有取得令人满意的进展。在这项工作中，我们提出了一个概念上简单但有效的斗地主 AI 系统，即 DouZero，它通过深度神经网络、动作编码和并行actor增强了传统的蒙特卡洛方法。从零开始，在单台四颗GPU的服务器上，训练天数超过了所有现有的斗地主AI程序，在344个AI代理中在Botzone排行榜中名列第一。通过构建 DouZero，我们展示了经典的 Monte-Carlo 方法可以在具有复杂动作空间的硬领域中提供强大的结果。发布了代码和在线演示<a target="_blank" rel="noopener" href="https://github.com/kwai/DouZero">repo</a>，希望这种见解可以激发未来的工作。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    游戏通常作为 AI 的基准，因为它们是许多现实世界问题的抽象。在完全信息博弈方面取得了显著成果。例如，AlphaGo (Silver et al., 2016)、AlphaZero (Silver et al., 2018) 和 MuZero (Schrittwieser et al., 2020) 已经在围棋游戏中取得了最先进的性能。最近的研究已经发展到更具挑战性的不完美信息游戏，其中代理在部分可观察的环境中与其他人竞争或合作。双人游戏中取得了鼓舞人心的进展，例如简单的 Leduc Hold’em and limit/no-limit Texas Hold’em (Zinkevich et al., 2008; Heinrich &amp; Silver, 2016; Moravck et al., 2017; Brown &amp; Sandholm, 2018)，多人游戏，例如多人Texas hold’em (Brown &amp; Sandholm, 2019b)、 Starcraft (Vinyals et al., 2019), DOTA (Berner et al., 2019), Hanabi (Lerer  et al., 2020), Mahjong (Li et al., 2020a), Honor of Kings (Ye et al., 2020b;a), and No-Press Diplomacy (Gray et al., 2020)。</p>
<p>​    这项工作旨在为斗地主（又名斗地主）构建人工智能程序，斗地主是中国最受欢迎的纸牌游戏，每天有数亿活跃玩家。斗地主有两个有趣的特性，它们对人工智能系统构成了巨大的挑战。首先，斗地主中的玩家需要在一个部分可观察的环境中与其他人竞争和合作，交流有限。具体来说，两名农民玩家将组队对抗地主玩家。扑克游戏的流行算法，例如 Counterfactual Regret Minimization (CFR) (Zinkevich et al., 2008)）及其变体，在这种复杂的三人游戏环境中通常没有声音。其次，斗地主拥有大量平均大小非常大的信息集，并且由于卡片的组合而具有非常复杂和庞大的动作空间，多达<script type="math/tex">10^{4}</script>个可能的动作（Zha et al., 2019a）。与德州扑克不同，斗地主中的动作不容易抽象，这使得搜索计算量大且常用的强化学习算法效果不佳。由于高估问题，Deep Q-Learning(DQN) (Mnih et al., 2015) 在非常大的动作空间中存在问题 (Zahavy et al., 2018)；策略梯度方法，例如 A3C (Mnih et al., 2016)，无法利用斗地主中的动作特征，因此无法像 DQN 那样自然地泛化不可见的动作 (Dulac-Arnold et al., 2015)。毫不奇怪，之前的工作表明，DQN 和 A3C 在斗地主上并没有取得令人满意的进展。在 (You et al., 2019) 中，即使经过 20 天的训练，DQN 和 A3C 对抗简单的基于规则的代理的胜率也低于 20%； (Zha et al., 2019a)中的 DQN 仅比均匀采样合法移动的随机代理略好。</p>
<p>​    之前已经做了一些努力，通过将人类启发式学习和搜索相结合来构建斗地主 AI。Combination Q-Network (CQN) (You et al., 2019)建议通过将动作解耦为分解选择和最终移动选择来减少动作空间。然而，分解依赖于人类启发式，并且非常缓慢。在实践中，经过二十天的训练，CQN 甚至无法击败简单的启发式规则。DeltaDou (Jiang et al., 2019)是第一个与顶级人类玩家相比达到人类水平表现的人工智能程序。它通过使用贝叶斯方法来推断隐藏信息并根据他们自己的策略网络对其他玩家的行为进行采样，从而实现了类似于 AlphaZero 的算法。为了抽象动作空间，DeltaDou 基于启发式规则预训练了一个kicker network(选择带牌，三带一，四带二等)。然而，kicker在斗地主中扮演着重要的角色，不能轻易抽象。kicker的错误选择可能会直接导致输掉比赛，因为它可能会破坏其他一些卡片类别，例如单顺。此外，贝叶斯推理和搜索在计算上是昂贵的。即使在使用监督回归启发式算法初始化网络时，训练 DeltaDou 也需要两个多月的时间(Jiang et al., 2019)。因此，现有的斗地主 AI 程序在计算上很昂贵，并且可能是次优的，因为它们高度依赖于人类知识的抽象。</p>
<p>​    在这项工作中，我们展示了 DouZero，这是一个概念上简单但有效的斗地主 AI 系统，没有抽象状态/动作空间或任何人类知识。 DouZero 使用深度神经网络、动作编码和并行actor增强了传统的蒙特卡罗方法 (Sutton &amp; Barto, 2018)。 DouZero 有两个可取的特性。首先，与 DQN 不同，它不容易受到高估偏差的影响。其次，通过将动作编码到 card matrices（卡片矩阵）中，它可以自然地泛化在整个训练过程中不常见的动作。这两个属性对于处理斗地主庞大而复杂的动作空间都至关重要。与许多树搜索算法不同，DouZero 基于采样，这使我们能够使用复杂的神经架构并在计算资源相同的情况下每秒生成更多数据。与之前许多依赖特定领域抽象的扑克 AI 研究不同，DouZero 不需要任何领域知识或底层动态知识。在只有 48 个内核和 4 个 1080Ti GPU 的单台服务器上从头开始训练，DouZero 在半天之内就超过了 CQN 和启发式规则，在两天内击败了我们的内部监督代理，并在 10 天内超越了 DeltaDou。广泛的评估表明，DouZero是迄今为止最强的斗地主人工智能系统。</p>
<p>​    通过构建 DouZero 系统，我们证明了经典的 Monte-Carlo 方法可以在需要推理巨大状态和动作空间上的竞争和合作的大型复杂纸牌游戏中取得出色的结果。我们注意到，一些工作还发现蒙特卡罗方法可以实现有竞争力的表现 (Mania et al., 2018; Zha et al., 2021a)并有助于稀疏奖励设置 (Guo et al., 2018; Zha et al.,  2021b)。与这些专注于简单和小型环境的研究不同，我们展示了蒙特卡洛方法在大型纸牌游戏中的强大性能。希望这一见解可以促进未来对解决多智能体学习、稀疏奖励、复杂动作空间和不完美信息的研究，我们发布了我们的环境和训练代码。与许多需要数千个 CPU 进行训练的扑克 AI 系统不同，例如 DeepStack (Moravcık et al., 2017)和 Libratus (Brown &amp; Sandholm, 2018)，DouZero 实现了合理的实验管道，只需要几天的训练大多数研究实验室都负担得起的单个 GPU 服务器。我们希望它可以激发该领域的未来研究并作为强大的基线。</p>
<h1 id="斗地主背景"><a href="#斗地主背景" class="headerlink" title="斗地主背景"></a>斗地主背景</h1><p>​    斗地主是一款流行的三人纸牌游戏，易学难精。 它在中国吸引了数亿玩家，每年举办许多比赛。 这是一个shedding-type（脱落类型）的游戏，玩家的目标是在其他玩家之前清空自己手中的所有牌。 两名农民玩家组队与另一位地主玩家作战。 如果农民玩家中的任何一个是第一个没有剩余牌的，则农民获胜。 每场比赛都有一个叫牌阶段，玩家根据手牌的强弱来竞标地主，还有一个打牌阶段，玩家轮流打牌。 我们在附录 A 中提供了详细介绍。</p>
<p>​    DouDizhu 仍然是多智能体强化学习的未解决基准 (Zha et al., 2019a; Terry et al., 2020)。两个有趣的特性使得斗地主特别难于解决。首先，农民需要合作对抗地主。例如，图10显示了一个典型的情况，底部的农民可以选择打一个小单张来帮助右边的农民获胜。其次，斗地主因卡牌组合而具有复杂而庞大的动作空间。有 27, 472种可能的组合，其中这些组合的不同子集对于不同的手牌是合法的。图1是一手牌的例子，它有391种合法组合，包括Solo（单牌）、Pair(对子)、Trio（三张）、Bomb（炸弹）、Plane（飞机）、Quad等。 动作空间不能轻易抽象，因为不正确的打牌可能会打破其他类别并直接导致在输掉一场比赛中。因此，构建斗地主AI具有挑战性，因为斗地主中的玩家需要在巨大的动作空间上进行竞争和合作的推理。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/fiture1.png" alt></p>
<h1 id="Deep-Monte-Carlo"><a href="#Deep-Monte-Carlo" class="headerlink" title="Deep Monte-Carlo"></a>Deep Monte-Carlo</h1><p>​    在本节中，我们重新审视Monte-Carlo (MC) 方法并介绍Deep Monte-Carlo (DMC)，它将 MC 与深度神经网络泛化为函数逼近。 然后我们将 DMC 与策略梯度方法（例如 A3C）和 DQN 进行讨论和比较，这些方法在斗地主中被证明是失败的 (You et al., 2019; Zha et al., 2019a)。</p>
<h2 id="Monte-Carlo-Methods-with-Deep-Neural-Networks"><a href="#Monte-Carlo-Methods-with-Deep-Neural-Networks" class="headerlink" title="Monte-Carlo Methods with Deep Neural Networks"></a>Monte-Carlo Methods with Deep Neural Networks</h2><p>​    蒙特卡洛 (MC) 方法是基于平均样本回报的传统强化学习算法 (Sutton &amp; Barto, 2018)。 MC 方法是为episodic任务设计的，其中experiences可以分为episodes并且所有episodes最终都终止。 为了优化策略<script type="math/tex">\pi</script>，每次访问 MC 可用于通过迭代执行以下过程来估计 Q-table <script type="math/tex">Q(s, a)</script>：</p>
<ol>
<li>使用<script type="math/tex">\pi</script>生成episode。</li>
<li>对于每个<script type="math/tex">s,a</script>出现在episode中，计算并更新<script type="math/tex">Q(s, a)</script>，并使用关于<script type="math/tex">s,a</script>的所有样本的平均回报。</li>
<li>对episode中的每个<script type="math/tex">s</script>，<script type="math/tex">\pi \left ( s \right )\leftarrow arg max_{a}Q\left ( s,a \right )</script>。</li>
</ol>
<p>​    Step 2 中的平均回报通常是通过折现累积奖励获得的。 与依赖bootstrapping的 Q-learning 不同，MC 方法直接逼近目标 Q 值。 在步骤 1 中，我们可以使用 epsilon-greedy 来平衡探索和利用。 上述过程可以与深度神经网络自然结合，从而导致深度蒙特卡罗（DMC）。 具体来说，我们可以用神经网络替换 Q 表，并在步骤 2 中使用均方误差 (MSE) 更新 Q 网络。</p>
<p>​    虽然 MC 方法被批评不能处理不完整的episode，并且由于高方差而被认为效率低下 (Sutton &amp; Barto, 2018)，但 DMC 非常适合斗地主。 首先，斗地主是一个episode任务，所以我们不需要处理不完整的episode。 其次，DMC 可以很容易地并行化，以每秒有效地生成许多样本，以缓解高方差问题。</p>
<h2 id="Comparison-with-Policy-Gradient-Methods"><a href="#Comparison-with-Policy-Gradient-Methods" class="headerlink" title="Comparison with Policy Gradient Methods"></a>Comparison with Policy Gradient Methods</h2><p>​    策略梯度方法，例如REINFORCE (Williams, 1992), A3C (Mnih et al., 2016), PPO (Schulman et al., 2017),  and IMPALA (Espeholt et al., 2018)，在强化学习中非常流行.他们的目标是直接使用梯度下降来建模和优化策略。在策略梯度方法中，我们经常使用类似分类器的函数逼近器，其中输出随动作数量线性缩放。虽然策略梯度方法在大动作空间中运行良好，但它们不能使用动作特征来推理以前未见过的动作 (Dulac-Arnold et al., 2015)。在实践中斗地主中的动作可以自然地编码成卡片矩阵，这对推理至关重要。例如，如果智能体因为选择了一个不错的kicker而获得了动作<script type="math/tex">3KKK</script>的奖励，它也可以将这些知识推广到未来看不见的动作，例如 <script type="math/tex">3JJJ</script>。此属性对于处理非常大的动作空间和加速学习至关重要，因为许多动作在模拟数据中并不常见。</p>
<p>​    DMC 可以通过将动作特征作为输入来自然地利用动作特征来概括看不见的动作。 虽然如果动作规模较大，执行复杂度可能会很高，但在斗地主的大多数状态下，只有一部分动作是合法的，因此我们不需要迭代所有动作。 因此，DMC 总体上是一种有效的斗地主算法。 虽然可以将动作特征引入到 actor-critic 框架中（例如，通过使用 Q 网络作为critic），但类分类器的 actor 仍然会受到大动作空间的影响。 我们的初步实验证实，这种策略不是很有效（见图 7）。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure7.png" alt></p>
<h2 id="Comparison-with-Deep-Q-Learning"><a href="#Comparison-with-Deep-Q-Learning" class="headerlink" title="Comparison with Deep Q-Learning"></a>Comparison with Deep Q-Learning</h2><p>​    最流行的基于值的算法是 Deep Q-Learning (DQN) (Mnih et al., 2015)，这是一种 bootstrapping方法，根据下一步中的Q 值更新Q 值。 虽然 DMC 和 DQN 都近似于 Q-values，但 DMC 在斗地主中有几个优点。</p>
<p>​    首先，在使用函数逼近时，由逼近 DQN 中的最大动作值引起的高估偏差难以控制（Thrun &amp; Schwartz 1993；Hasselt，2010），并且在动作空间非常大时变得更加明显 (Zahavy et al., 2018) . 虽然一些技术，例如double Q-learning (van Hasselt et al., 2016) 和experience replay (Lin, 1992)，可能会缓解这个问题，但我们在实践中发现 DQN 非常不稳定，并且经常在斗地主中发散。 然而蒙特卡洛估计不易受到偏差的影响，因为它无 bootstrapping即可直接逼近真实值 (Sutton &amp; Barto, 2018)。</p>
<p>​    其次，斗地主是一项具有长视野和稀疏奖励的任务，即智能体需要在没有反馈的情况下通过很长的状态链，并且只有在游戏结束时才会产生非零奖励。 这可能会减慢 Q-learning 的收敛速度，因为估计当前状态的 Q 值需要等到下一个状态的值接近其真实值 (Szepesvari, 2009; Beleznay et al., 1999)。与 DQN 不同，蒙特卡罗估计的收敛性不受episode长度的影响，因为它直接接近真实的目标值。</p>
<p>​    第三，由于动作空间大且可变，不方便在斗地主中高效实施 DQN。 具体来说，DQN 在每个更新步骤中的最大操作将导致高计算成本，因为它需要在非常昂贵的深度 Q 网络上迭代所有合法操作。 而且不同状态下的合法动作不同，这使得批量学习不方便。 结果我们发现 DQN 在 wall-clock time（现实时间）方面太慢了。 虽然蒙特卡洛方法可能存在高方差 (Sutton &amp; Barto, 2018)，这意味着它可能需要更多样本才能收敛，但它可以轻松并行化以每秒生成数千个样本，以缓解高方差问题并加速训练。 我们发现 DMC 的高方差被它提供的可扩展性大大抵消了，而且 DMC 在wall-clock time上非常有效。</p>
<h1 id="DouZero-System"><a href="#DouZero-System" class="headerlink" title="DouZero System"></a>DouZero System</h1><p>​    在本节中，我们通过首先描述state/action表示和神经架构来介绍 DouZero 系统，然后详细说明我们如何将 DMC 与多个进程并行化以稳定和加速训练。</p>
<h2 id="Card-Representation-and-Neural-Architecture"><a href="#Card-Representation-and-Neural-Architecture" class="headerlink" title="Card Representation and Neural Architecture"></a>Card Representation and Neural Architecture</h2><p>​    我们使用one-hot <script type="math/tex">4\times 15</script> 矩阵对每个卡片组合进行编码（图 2）。 由于花色在斗地主中是无关紧要的，我们用每一行来代表牌值（rank 和joker）。 图 3 显示了 Q-network的架构。 对于状态，我们提取几个卡片矩阵来表示手牌、其他玩家手牌的并集和最近的动作，以及一些one-hot向量来表示其他玩家的手牌数量和到目前为止炸弹的数量。 同样，我们使用一张卡片矩阵来编码动作。 对于神经架构，LSTM 用于对历史动作进行编码，并将输出与其他 state/action特征连接起来。 最后我们使用隐藏大小为 512 的六层 MLP 来生成Q-values。 我们在附录 C.1 中提供了更多详细信息。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure2.png" alt></p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure3.png" alt></p>
<h2 id="Parallel-Actors"><a href="#Parallel-Actors" class="headerlink" title="Parallel Actors"></a>Parallel Actors</h2><p>​    我们将 Landlord(地主)表示为 L，将在 Landlord 之前移动的玩家（上家）表示为 U，将在 Landlord 之后移动的玩家（下家）表示为 D。我们将 DMC 与多个actor进程和一个learner进程并行化，分别在<strong>Algorithm1</strong>和<strong>Algorithm2</strong>中进行了总结。 学习器为三个位置维护三个全局Q-network，并使用 MSE 损失更新网络，以根据actor进程提供的数据来近似目标值。 每个参与者维护三个本地Q-network，它们定期与全局网络同步。 actor将重复从游戏引擎中采样轨迹并计算每个state-action对的累积奖励。 learner和actor的交流是通过三个共享缓冲区实现的。 每个缓冲区被分成几个条目，其中每个条目由几个数据实例组成。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/algorithm1.png" alt></p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/algorithm2.png" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>​    这些实验旨在回答以下研究问题。 RQ1：DouZero 与现有的 DouDizhu 程序相比如何，例如基于规则的策略、监督学习、基于 RL 的方法和基于 MCTS 的解决方案（第 5.2 节）？ RQ2：如果我们考虑叫分阶段（第 5.3 节），DouZero 的表现如何？ RQ3：DouZero 的训练效率如何（第 5.4 节）？ RQ4：DouZero 与bootstrapping和actor critic方法（第 5.5 节）相比如何？ RQ5：DouZero 学习的打牌策略是否符合人类知识（第 5.6 节）？ RQ6：与现有程序（第 5.7 节）相比，DouZero 在推理方面是否具有计算效率？ RQ7：DouZero的两个农民能学会互相合作吗（5.8节）？</p>
<h2 id="实验装置"><a href="#实验装置" class="headerlink" title="实验装置"></a>实验装置</h2><p>​    扑克游戏中常用的策略强度衡量标准是可利用性(Johanson et al., 2011)。 但是在斗地主中，由于斗地主拥有巨大的状态/动作空间，并且有三个玩家，因此计算可利用性本身就很棘手。 为了评估性能，在 (Jiang et al., 2019)之后，我们发起了包括地主和农民两个对手的锦标赛。 我们通过对每一副牌打两次来减少差异。 具体来说，对于两个相互竞争的算法 A 和 B，对于给定的牌组，它们将首先分别扮演地主和农民的位置。 然后他们换边，即A占农民位置，B占地主位置，再次玩同一副牌。 为了模拟真实环境，在第 5.3 节中，我们进一步训练了一个带有监督学习的 bidding network（叫分网络），代理将根据手牌的强度在每场比赛中对地主进行叫分（更多细节见附录 C.2）。 我们考虑以下竞争算法。</p>
<ul>
<li><p><strong>DeltaDou:</strong>一个强大的人工智能程序，它使用贝叶斯方法来推断隐藏信息并使用 MCTS 搜索动作（Jiang 等，2019）。 我们使用作者提供的代码和预训练模型。 该模型经过两个月的训练，并显示出与顶级人类玩家相当的表现。</p>
</li>
<li><p><strong>CQN:</strong>Combinational Q-Learning (You et al., 2019) 是一个基于卡片分解和 Deep Q-Learning的程序。 我们使用开源代码和作者提供的预训练模型<a target="_blank" rel="noopener" href="https://github.com/qq456cvb/doudizhu-C">github</a>。</p>
</li>
<li><p><strong>SL:</strong> 监督学习基线。 我们在我们的斗地主游戏手机应用程序中内部收集了 226230 场来自联赛最高级别玩家的人类行家比赛。 然后我们使用与 DouZero 相同的状态表示和神经架构来训练监督代理，其中包含从这些数据生成的 49990075 个样本。 有关详细信息，请参阅附录 C.2。</p>
</li>
<li><p><strong>Rule-Based Programs:</strong>我们收集了一些基于启发式的开源程序，包括 RHCP4<a target="_blank" rel="noopener" href="https://blog.csdn.net/sm9sun/article/ details/70787814">link</a>，一个称为 RHCP-v2<a target="_blank" rel="noopener" href="https://github.com/deecamp2019-group20/">link</a></p>
<p>的改进版本，以及 RLCard 包中的规则模型<a target="_blank" rel="noopener" href="https://github.com/datamllab/rlcard">link</a> (Zha et al., 2019a)。 此外我们考虑了一个 Random 程序，它可以均匀地采样合法的移动。</p>
</li>
</ul>
<p><strong>Metrics</strong>下面 (Jiang et al., 2019)，给定算法 A 和对手 B，我们使用两个指标来比较 A 和 B 的性能：</p>
<ul>
<li><strong>WP</strong> (Winning Percentage):A赢的局数除以总局数。</li>
<li><strong>ADP</strong> (Average Difference in Points):A 和 B 之间每场比赛得分的平均差异。基础分数为 1。每个炸弹将使得分翻倍。</li>
</ul>
<p>我们在实践中发现这两个指标鼓励不同风格的策略。 例如如果使用 ADP 作为奖励，agent 往往会非常谨慎地出炸弹，因为出炸弹是有风险的，可能会导致更大的 ADP 损失。 相比之下，以 WP 为目标，agent 往往会积极地出炸弹，即使它会失败，因为炸弹不会影响 WP。 我们观察到，就 ADP 而言，使用 ADP 训练的代理的性能略好于使用 WP 训练的代理，反之亦然。 接下来，我们分别以 ADP 和 WP 为目标来训练和报告两个 DouZero 代理的结果（对于 WP，我们根据智能体是赢了还是输了游戏，对最后的时间步长给出了 +1 或 -1 的奖励。 对于 ADP，我们直接使用 ADP 作为奖励。 DeltaDou 和 CQN 分别以 ADP 和 WP 为目标进行训练）。 附录 D.2 中提供了对这两个目标的更多讨论。</p>
<p>​    我们首先通过让每对算法玩 10,000 副套牌来启动预赛。 然后，我们通过玩 100,000 副套牌来计算前 3 种算法的 Elo 评分，以进行更可靠的比较，即 DouZero、DeltaDou 和 SL。 如果算法在该套牌上进行的两场比赛中获得更高的 WP 或 ADP，则该算法将赢得该套牌。 我们用不同的随机采样套牌重复这个过程五次，并报告 Elo 分数的平均值和标准差。 对于叫分阶段的评估，每副牌打六次，不同位置的 DouZero、DeltaDou 和 SL 有不同的扰动。 我们用 100,000 副牌报告结果。</p>
<p><strong>Implementation Details.</strong>我们在具有 48 个 Intel(R) Xeon(R) Silver 4214R CPU @ 2.40GHz 处理器和四个 1080 Ti GPU 的服务器上运行所有实验。 我们使用 45个actor，这些actor分布在三个 GPU 上。 我们在剩余的 GPU 中运行一个学习器来训练Q-networks。 我们的实现基于TorchBeast framework (Kuttler et al., 2019)。 详细的训练曲线在附录 D.5 中提供。 每个共享缓冲区有 <script type="math/tex">B = 50</script>个条目，大小 <script type="math/tex">S = 100</script>，批量大小<script type="math/tex">M = 32</script> ，并且 <script type="math/tex">\epsilon= 0.01</script>。 我们设置折扣因子 <script type="math/tex">\gamma = 1</script>，因为斗地主只有一个非零的奖励在最后一个时间步，并且早期的动作非常重要。 我们使用 ReLU 作为 MLP 每一层的激活函数。 我们采用 RMSprop 优化器，其学习率 <script type="math/tex">\psi = 0.0001</script>，平滑常数为 0.99 且 <script type="math/tex">\epsilon = 10−5</script>。 我们训练 DouZero 30 天。</p>
<h2 id="Performance-against-Existing-Programs"><a href="#Performance-against-Existing-Programs" class="headerlink" title="Performance against Existing Programs"></a>Performance against Existing Programs</h2><p>​    为了回答 RQ1，我们将 DouZero 与离线基线进行比较，并在Botzone (Zhou et al., 2018)上报告其结果，这是一个 DouDizhu 比赛的在线平台（更多细节在附录 E 中提供）。</p>
<p>​    表 1 总结了 DouZero 和所有基线之间头对头完成的 WP 和 ADP。我们提出三点意见。首先，DouZero 主导了所有基于规则的策略和监督学习，这证明了在 DouDizhu 中采用强化学习的有效性。其次，DouZero 的性能明显优于 CQN。回想一下，CQN 类似地使用动作分解和 DQN 训练 Q 网络。 DouZero 的优越性表明 DMC 确实是在 DouDizhu 中训练 Q-networks 的有效方法。第三，DouZero优于DeltaDou，这是文献中最强的Doudizhu AI。我们注意到斗地主有很高的方差，即赢得一场比赛依赖于初始手牌的强度，这高度依赖于运气。因此，0.586 的 WP 和 0.258 的 ADP 表明对 DeltaDou 的显着改进。此外，DeltaDou 需要在训练和测试时进行搜索。而 DouZero 不做搜索，这验证了 DouZero 学习的 Q-networks 非常强大。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/table1.png" alt></p>
<p>​    图 4 的左侧显示了 DouZero、DeltaDou 和 SL 在玩 100, 000 副套牌时的 Elo 评分。 我们观察到 DouZero 在 WP 和 ADP 方面都明显优于 DeltaDou 和 SL。 这再次证明了DouZero的强劲表现。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure4.png" alt></p>
<p>​    图 4 的右侧说明了 DouZero 在 Botzone 排行榜上的表现。 我们注意到 Botzone 采用了不同的评分机制。 除了 WP 之外，它还为一些特定的卡片类别提供了额外的奖励，例如 Chain of Pair 和 Rocket（详见附录 E）。 如果以 Botzone 的评分机制为目标，DouZero 很有可能获得更好的性能，但我们直接上传了以 WP 为目标训练的 DouZero 的预训练模型。 我们观察到这个模型足够强大，可以击败其他机器人。</p>
<h2 id="Comparison-with-Bidding-Phase"><a href="#Comparison-with-Bidding-Phase" class="headerlink" title="Comparison with Bidding Phase"></a>Comparison with Bidding Phase</h2><p>​    为了研究 RQ2，我们使用人类专家数据训练一个具有监督学习的叫分网络。 我们将排名前 3 的算法，即 DouZero、DeltaDou 和 SL，放入 DouDizhu 游戏的三个席位。 在每场比赛中，我们随机选择第一个叫分者，并使用预先训练的叫分网络模拟叫分阶段。 为了公平比较，所有三种算法都使用相同的出价网络。 结果总结在表 2 中。虽然 DouZero 是在没有叫分网络的情况下在随机生成的牌组上训练的，但我们观察到 DouZero 在 WP 和 ADP 中都主导其他两种算法。 这证明了 DouZero 在需要考虑投标阶段的现实世界比赛中的适用性。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/table2.png" alt></p>
<h2 id="Analysis-of-Learning-Progress"><a href="#Analysis-of-Learning-Progress" class="headerlink" title="Analysis of Learning Progress"></a>Analysis of Learning Progress</h2><p>​    为了研究 RQ3，我们在图 5 中可视化了 DouZero 的学习进度。我们使用 SL 和 DeltaDou 作为对手来绘制 WP 和 ADP随着训练天数的变化。我们做出以下两点观察。首先，DouZero 在 WP 和 ADP 方面分别在一天和两天的训练中优于 SL。我们注意到 DouZero 和 SL 使用完全相同的神经架构进行训练。因此，我们将 DouZero 的优越性归功于自我对弈强化学习。虽然 SL 也表现良好，但它依赖于大量数据，不灵活并且可能限制其性能。其次，DouZero 在 WP 和 ADP 方面分别在 3 天和 10 天的训练中优于 DeltaDou。我们注意到 DeltaDou 是用启发式的监督学习初始化的，并且训练了两个多月。而 DouZero 从零开始，只需要几天的训练就可以击败 DeltaDou。这表明没有搜索的无模型强化学习在斗地主中确实有效。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure5.png" alt></p>
<p>​    我们进一步分析了使用不同数量的actor时的学习速度。 图 6 报告了使用 15、30 和 45 个 actor 时针对 SL 的性能。 我们观察到，使用更多的 actor 可以加速wall-clock time的训练。 我们还发现所有三种设置都显示出相似的样本效率。 未来，我们将探索在多台服务器上使用更多actor的可能性，以进一步提高训练效率。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure6.png" alt></p>
<h2 id="Comparison-with-SARSA-and-Actor-Critic"><a href="#Comparison-with-SARSA-and-Actor-Critic" class="headerlink" title="Comparison with SARSA and Actor-Critic"></a>Comparison with SARSA and Actor-Critic</h2><p>​    为了回答 RQ4，我们实现了两个基于 DouZero 的变体。 首先，我们用Temporal-Difference (TD)  目标替换 DMC 目标。 这导致了SARSA的深度版本。 其次，我们实现了一个具有动作特征的 Actor-Critic 变体。 具体来说，我们使用 Q-network 作为具有动作特征的critic，并训练策略作为具有动作掩码的演员来消除非法动作。</p>
<p>​    图 7 显示了 SARSA 和 Actor-Critic 单次运行的结果。 首先，我们没有观察到使用 TD 学习的明显好处。 我们观察到 DMC 在wall-clock time和样本效率方面的学习速度略快于 SARSA。 可能的原因是 TD 学习在稀疏奖励设置中不会有太大帮助。 我们相信需要更多的研究来了解 TD 学习何时会有所帮助。 其次，我们观察到 Actor-Critic 失败了。 这表明简单地向评论家添加动作特征可能不足以解决复杂的动作空间问题。 未来，我们将研究是否可以将动作特征有效地融入到Actor-Critic框架中。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure7.png" alt></p>
<h2 id="Analysis-of-DouZero-on-Expert-Data"><a href="#Analysis-of-DouZero-on-Expert-Data" class="headerlink" title="Analysis of DouZero on Expert Data"></a>Analysis of <strong>DouZero</strong> on Expert Data</h2><p>​    对于 RQ5，我们在整个训练过程中计算 DouZero 在人类数据上的准确性。 我们将使用 ADP 训练的模型报告为客观模型，因为收集人类数据的游戏应用程序也采用 ADP。 图 8 显示了结果。 我们做了以下两个有趣的观察。 首先，在早期阶段，即训练的前五天，准确率不断提高。 这表明智能体可能已经学会了一些与人类专业知识相一致的策略，纯粹是自我游戏。 其次，经过五天的训练，准确率急剧下降。 我们注意到 ADP 对 SL 的影响在五天后仍在改善。 这表明智能体可能已经发现了一些人类无法轻易发现的新颖更强的策略，这再次验证了自我对弈强化学习的有效性。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure8.png" alt></p>
<h2 id="Comparison-of-Inference-Time"><a href="#Comparison-of-Inference-Time" class="headerlink" title="Comparison of Inference Time"></a>Comparison of Inference Time</h2><p>​    为了回答 RQ6，我们在图 9 中报告了每一步的平均推理时间。为了公平比较，我们在 CPU上评估了所有算法。 我们观察到 DouZero 比 DeltaDou、CQN、RHCP 和 RHCP-v2 快几个数量级。 这是意料之中的，因为 DeltaDou 需要执行大量的蒙特卡罗模拟，而 CQN、RHCP 和 RHCP-v2 需要昂贵的卡片分解。 而 DouZero 在每一步只执行一次神经网络的前向传递。 DouZero 的高效推理使我们能够每秒生成大量样本用于强化学习。 它还使得在实际应用程序中部署模型变得经济实惠。</p>
<h2 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h2><p>​    为了调查 RQ7，我们进行了案例研究以了解 DouZero 所做的决定。 我们从 Botzone 转储比赛日志，并用预测的 Q 值可视化最重要的动作。 我们在附录 F 中提供了大部分案例研究，包括好的和坏的案例。</p>
<p>​    图 10 显示了两个农民合作击败地主的典型案例。 右边的农民只剩下一张牌了。 在这里，最底层的农民可以玩一个小Solo来帮助其他农民获胜。 在研究 DouZero 预测的前三个动作时，我们进行了两个有趣的观察。 首先，我们发现DouZero 输出的所有top动作都是小Solo，有很高的取胜信心，说明DouZero 的两个Peasant可能已经学会了合作。 其次，动作 4 的预测 Q 值 (0.808) 远低于动作 3 (0.971) 的 Q 值。 一个可能的解释是仍然有一个 4，所以玩 4 可能不一定有助于农民获胜。 在实践中，在这种特定情况下，其他农民的唯一牌的等级不高于 4。 总的来说，在这种情况下，行动 3 确实是最好的举措。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure10.png" alt></p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>​    <strong>Search for Imperfect-Information Games.</strong>Counterfactual Regret Minimization (CFR) (Zinkevich et al., 2008)是一种领先的扑克游戏迭代算法，有许多变体 (Lanctot et al., 2009; Gibson et al., 2012; Bowling et al., 2015; Moravcık et al., 2017; Brown &amp; Sandholm, 2018; 2019a; Brown et al., 2019; Lanctot et al., 2019; Li et al., 2020b)。 然而，遍历斗地主的博弈树是计算密集型的，因为它有一棵巨大的树，有很大的分支因子。 此外，大多数先前的研究都集中在零和设置上。 虽然已经做出了一些努力来解决合作环境，例如blueprint policy (Lerer et al., 2020)，但对竞争和合作的推理仍然具有挑战性。 因此，斗地主还没有看到有效的类似 CFR 的解决方案。</p>
<p>​    <strong>RL for Imperfect-Information Games.</strong>最近的研究表明，强化学习 (RL) 可以在扑克游戏中获得有竞争力的表现 (Heinrich et al., 2015; Heinrich &amp; Silver, 2016; Lanctot et al., 2017)。 与 CFR 不同，强化学习基于采样，因此可以轻松推广到大型游戏。 RL 已成功应用于一些复杂的不完美信息游戏，例如Starcraft (Vinyals et al., 2019), DOTA (Berner et al., 2019) and Mahjong (Li et al., 2020a)）。 最近，RL+search 被探索并证明在扑克游戏中是有效的 (Brown et al., 2020)。 DeltaDou 采用了类似的思路，首先推断隐藏信息，然后使用 MCTS 将 RL 与n DouDizhu中的搜索结合起来（Jiang et al., 2019）。 然而，DeltaDou 的计算成本很高，并且严重依赖于人类的专业知识。 在实践中，即使没有搜索，我们的 DouZero 在几天的训练中也优于 DeltaDou。</p>
<h1 id="总结和未来工作"><a href="#总结和未来工作" class="headerlink" title="总结和未来工作"></a>总结和未来工作</h1><p>​    这项工作为斗地主提供了一个强大的人工智能系统。 一些独特的特性使得斗地主特别难以解决，例如，巨大的状态/动作空间以及关于竞争和合作的推理。 为了应对这些挑战，我们通过深度神经网络、动作编码和并行actor增强了经典的蒙特卡洛方法。 这导致了一个纯 RL 解决方案，即 DouZero，它在概念上简单但有效且高效。 广泛的评估表明，DouZero是迄今为止斗地主最强的人工智能程序。 我们希望简单的蒙特卡洛方法可以在如此困难的领域产生强大的策略的见解将激励未来的研究。</p>
<p>​    对于未来的工作，我们将探索以下方向。 首先，我们计划尝试其他神经架构，例如卷积神经网络和 ResNet (He et al., 2016)。 其次，我们将在强化学习的循环中参与叫分。 第三，我们将在训练和/或测试时将 DouZero 与搜索相结合 (Brown et al., 2020)，并研究如何平衡 RL 和搜索。 第四，探索off-policy学习，提高训练效率。 具体来说，我们将研究是否以及如何通过经验回放来提高wall-clock time和样本效率(Lin, 1992; Zhang &amp; Sutton, 2017; Zha et al., 2019b; Fedus et al., 2020)。 第五，我们将尝试对农民的合作进行明确建模（ (Panait &amp; Luke, 2005; Foerster et al., 2016; Raileanu et al., 2018; Lai et al., 2020)。 第六，我们计划尝试可扩展的框架，例如 SEED RL (Espeholt et al., 2019)。 最后但并非最不重要的一点是，我们将测试 Monte-Carlo 方法在其他任务上的适用性。</p>
<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h1 id="CQN-代码阅读记录doudizhu-c"><a href="#CQN-代码阅读记录doudizhu-c" class="headerlink" title="CQN 代码阅读记录doudizhu-c"></a>CQN 代码阅读记录doudizhu-c</h1><h2 id="TensorPack"><a href="#TensorPack" class="headerlink" title="TensorPack"></a>TensorPack</h2><h3 id="AutoEncoder"><a href="#AutoEncoder" class="headerlink" title="AutoEncoder"></a>AutoEncoder</h3><h4 id="main-py"><a href="#main-py" class="headerlink" title="main.py"></a>main.py</h4><p>训练编码解码器，将onehot60的走步编码为</p>
<h3 id="MA-Hierachical-Q"><a href="#MA-Hierachical-Q" class="headerlink" title="MA_Hierachical_Q"></a>MA_Hierachical_Q</h3><h4 id="env-py"><a href="#env-py" class="headerlink" title="env.py"></a>env.py</h4><h5 id="Env"><a href="#Env" class="headerlink" title="Env"></a>Env</h5><p>包含所有只能体的历史出牌，手牌，地主，地主牌，当前玩家等信息。</p>
<ul>
<li>get_state_prob：返回剩余牌分别属于另外两个玩家的概率</li>
</ul>
<h4 id="expreplay-py"><a href="#expreplay-py" class="headerlink" title="expreplay.py"></a>expreplay.py</h4><p>产生数据放入缓冲区中，进行经验回放。</p>
<p><strong>ReplayMemory</strong></p>
<p>缓冲区，用于存储训练数据。具有添加数据、取数据等方法。</p>
<p><strong>ExpReplay</strong></p>
<ul>
<li>get_combinations：手牌大于10张时：1 通过舞蹈链算法得到手中牌能组成的所有组合。2 找出手中牌能得到的所有合法走步（大过上家牌） 3 从所有组合中挑选出具有第二步中合法走步的组合。被动出牌时，_fine_mask是一个长度为一个组合的最大走步数量的值（MAX_NUM_GROUPS = 21）,如果组合中对应下标的走步能管上上家牌则为True，否则为False。主动出牌时fine_mask为空。当手牌小于等于10张时，采取另一种方式，详情见代码。fine_mask的大小为(comb数量，MAX_NUM_GROUPS )</li>
<li>get_state_and_action_spaces：首先通过get_combinations得到所有的组合情况。当组合情况数量大于指定的数量时（MAX_NUM_COMBS = 100），进行采样出最大数量组合。然后得到所有组合情况中的走步，然后通过encoding进行编码为state。当得到的组合情况下于MAX_NUM_COMBS 时，需要将最后一个组合重复多次，达到MAX_NUM_COMBS 。最后的state的大小为(MAX_NUM_COMBS ,MAX_NUM_GROUPS ,)</li>
<li>_populate_exp:返回值old_s表示状态,get_state_and_action_spaces的结果，act表示动作下标，reward表示奖励，isOver表示游戏是否结束，_comb_mask游戏结束为True，未结束为False。fine_mask表示与get_combinations解释一样。</li>
</ul>
<h4 id="DQNModel-py"><a href="#DQNModel-py" class="headerlink" title="DQNModel.py"></a>DQNModel.py</h4><p>总共分为两层模型。</p>
<p>第一层模型输入：手牌、上家牌、下家牌、其它两家每张牌的概率prob_state、当前手牌的所有组合、当前手牌所有组合中合法走步对应的fine_mask，输出最好的组合下标。</p>
<p>第二层模型输入：输入第一层模型输出的最好组合下标所对应的state，重复很多次作为输入。输出为最好的action所对应的下标。</p>
<h4 id="predictor-py"><a href="#predictor-py" class="headerlink" title="predictor.py"></a>predictor.py</h4><ul>
<li>predict通过模型输出最好走步，预测步骤同上DQNModel.py。</li>
</ul>
<h2 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h2><ul>
<li>经验回放采用prioritized replay dqn的方式</li>
</ul>
<h2 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h2><ul>
<li>模拟采取的方式是把其它两个玩家的牌混合在一起，然后提前发好牌，然后当作明牌进行模拟。上面这个过程会进行指定次数，而不是只进行一次。并且在模拟的过程中会设置一个提前结束的阈值，即达到该阈值后提前退出。在探索利用的过程使用的是UCT公式。</li>
<li>将doudizhu_base所得到得.so动态库放入每个项目build文件夹中</li>
<li>第一个模型传入所有的组合（一个组合有很多走步），通过模型选择出最好的拆分方式。通过手牌等历史信息再通过第二个模型选择最优走步。</li>
<li>模型配置在core/table.py里面</li>
<li>训练：python  main.py  —gpu 0 —load /home/pc/doudizhu-C-master/TensorPack/MA_Hierarchical_Q/train_log/DQN-60-MA-SELF_PLAY/checkpoint</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/21/Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/21/Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks/" class="post-title-link" itemprop="url">Bag of Tricks for Image Classification with Convolutional Neural Networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-06-21 22:28:16 / Modified: 22:49:30" itemprop="dateCreated datePublished" datetime="2021-06-21T22:28:16+08:00">2021-06-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">图像分类</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    最近在图像分类研究中取得的大部分进展都归功于训练过程的改进，例如数据增强和优化方法的变化。 然而，在文献中，大多数改进要么作为实现细节被简要提及，要么仅在源代码中可见。 在本文中，我们将检查这些改进的集合，并通过消融研究凭经验评估它们对最终模型准确性的影响。 我们将证明，通过将这些改进结合在一起，我们能够显着改进各种 CNN 模型。 例如，我们将 ResNet-50 在 ImageNet 上的 top-1 验证准确率从 75.3% 提高到 79.29%。 我们还将证明图像分类精度的提高导致在其他应用领域（如目标检测和语义分割）中更好的迁移学习性能。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    自 2012 年 AlexNet [15] 推出以来，深度卷积神经网络已成为图像分类的主要方法。 从那时起，人们提出了各种新架构，包括 VGG [24]、NiN [16]、Inception [1]、ResNet [9]、DenseNet [13] 和 NASNet [34]。 同时，我们看到了模型精度提升的稳定趋势。 例如，ImageNet [23] 上的 top-1 验证准确率已从 62.5% (AlexNet) 提高到 82.7% (NASNet-A)。</p>
<p>​    然而这些进步不仅仅来自改进的模型架构，训练过程的改进，包括损失函数的变化、数据预处理和优化方法也发挥了重要作用。 过去几年已经提出了大量这样的改进，但受到的关注相对较少。 在文献中，大多数只是作为实现细节被简要提及，而其他的只能在源代码中找到。</p>
<p>​    在本文中，我们将研究一系列训练过程和模型架构改进，这些改进提高了模型的准确性，但几乎没有改变计算复杂性。 其中许多都是小“trick”，例如修改特定卷积层的步幅大小或调整学习率计划。 然而总的来说，它们有很大的不同。 我们将在多个网络架构和数据集上评估它们，并报告它们对最终模型准确性的影响。</p>
<p>​    我们的经验评估表明，一些技巧可以显着提高精度，将它们组合在一起可以进一步提高模型精度。 我们将应用所有技巧后的 ResNet-50 与表 1 中的其他相关网络进行比较。请注意，这些技巧在ImageNet上将 ResNet50 的 top-1 验证准确率从 75.3% 提高到 79.29%。 它还优于其他更新和改进的网络架构，例如 SE-ResNeXt-50。 此外，我们表明我们的方法可以推广到其他网络（Inception V3 [1] 和 MobileNet [11]）和数据集（Place365 [32]）。 我们进一步表明，使用我们的技巧训练的模型在其他应用领域（如目标检测和语义分割）中带来了更好的迁移学习性能。</p>
<p><img src="/2021/06/21/Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks/table1.png" alt></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/" class="post-title-link" itemprop="url">Counterfactual Multi-Agent Policy Gradients</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-16 21:19:44" itemprop="dateCreated datePublished" datetime="2021-06-16T21:19:44+08:00">2021-06-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-08-10 22:14:55" itemprop="dateModified" datetime="2021-08-10T22:14:55+08:00">2021-08-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93/" itemprop="url" rel="index"><span itemprop="name">多智能体</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    许多现实世界中的问题，如网络包路由和自动驾驶车辆的协调，很自然地被建模为合作的多智能体系统。非常需要新的强化学习方法，可以轻松地学习此类系统的分散策略。为此，我们提出了一种新的多智能体actor-critic方法，称为counterfactual multi-agent(COMA)策略梯度。COMA使用一个集中的critic来估计Q-function，并使用分散的actors来优化代理的策略。此外，为了解决多代理credit assignent的挑战，它使用了一个counterfactual(反事实的) baseline，边缘化一个单一代理的行动，同时保持其他代理的行动不变。COMA还使用了一种critic表示，允许在一次向前传递中有效地计算counterfactual baseline。我们在StarCraft unit micromanagement测试平台中评估 COMA，使用具有显着部分可观察性的分散变体。 在这种情况下，COMA 显着提高了其他多代理 actor-critic 方法的平均性能，并且性能最好的代理与可以访问完整状态的最先进的集中控制器相比具有竞争力。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    许多复杂的强化学习(RL)问题，如自动驾驶车辆的协调 (Cao et al. 2013)，网络分组传送 (Ye, Zhang, and Yang 2015),和分布式物流系统 (Ying and Dayong 2005)很自然地被建模为合作的多智能体系统。然而，为单个代理设计的RL方法通常在此类任务上表现不佳，因为代理的联合动作空间随着代理的数量呈指数级增长。</p>
<p>​    为了应对这种复杂性，通常需要诉诸去中心化策略，其中每个代理仅根据其本地动作观察历史来选择自己的动作。 此外，执行期间的部分可观察性和通信限制可能需要使用分散的策略，即使联合行动空间不是很大。</p>
<p>​    因此，非常需要能够有效学习分散策略的新 RL 方法。 在某些情况下，学习本身也可能需要去中心化。 然而，在许多情况下，学习可以在模拟器或实验室中进行，在那里可以获得额外的状态信息并且代理可以自由交流。 这种分散策略的集中训练是多智能体规划的标准范式Oliehoek, Spaan, and Vlassis 2008; Kraemer and Banerjee 2016)，最近被深度强化学习社区采用 (Foerster et al. 2016; Jorge, Kageback, and Gustavsson 2016).。 然而，如何最好地利用集中学习的机会的问题仍然存在。</p>
<p>​    另一个关键挑战是多智能体credit assignment(信用分配)(Chang, Ho, and Kaelbling 2003)：在合作环境中，联合行动通常只产生全局奖励，这使得每个智能体很难推断出自己对团队成功的贡献。 有时可以为每个代理设计单独的奖励函数。 然而，这些奖励在合作环境中通常不可用，并且通常无法鼓励个体代理为更大的利益做出牺牲。 这通常会严重阻碍多智能体在具有挑战性的任务中学习，即使智能体数量相对较少。</p>
<p>​    在本文中，我们提出了一种新的多智能体强化学习方法，称为counterfactual multi-agent（反事实多智能体 COMA) 策略梯度，以解决这些问题。 COMA 采用actor-critic(Konda and Tsitsiklis 2000)方法，其中actor即策略，通过遵循critic估计的梯度进行训练。 COMA 基于三个主要思想。</p>
<p>​    首先，COMA 使用集中式critic。 critic只在学习期间使用，而在执行期间只需要actor。 由于学习是集中式的，因此我们可以使用集中式critic，以联合动作和所有可用状态信息为条件，而每个代理的策略仅以自己的动作观察历史为条件。</p>
<p>​    其次，COMA使用了一个counterfactual baseline。这个想法的灵感来自difference rewards(Wolpert and Tumer 2002; Tumer and Agogino 2007),其中每个智能体从一个成形的奖励中学习，该奖励将全局奖励与当该智能体的动作替换为default action时收到的奖励进行比较。虽然差异奖励是执行多智能体信用分配的强大方法，但它们需要访问模拟器或估计奖励函数，并且通常不清楚如何选择默认动作。COMA 通过使用集中式critic来计算特定于代理的advantage function来解决这个问题，该function将当前联合行动的估计回报与边缘化单个代理的行为的反事实基线进行比较，同时保持其他代理的行为固定。这类似于计算aristocrat utility(贵族效用)（Wolpert and Tumer 2002)，但避免了策略和效用函数之间递归相互依赖的问题，因为反事实基线对策略梯度的预期贡献为零。因此，COMA 不依赖于额外的模拟、近似值或关于适当默认行为的假设，而是为每个代理计算单独的基线，该基线依赖于集中式critic来推理只有该代理的行为发生变化的反事实。</p>
<p>​    第三，COMA 使用critic表示，允许有效计算反事实基线。 在单个前向传递中，它计算给定代理的所有不同动作的 Q 值，条件是所有其他代理的动作。 因为所有代理都使用一个集中的critic，所以所有代理的所有 Q 值都可以在单个分批前向传递中计算。</p>
<p>​    我们在StarCraft unit micromanagement（参见1）的测试平台中评估 COMA，它最近成为具有高随机性、大状态动作空间和延迟奖励的具有挑战性的 RL 基准任务。 以前的工作 (Usunier et al. 2016; Peng et al. 2017)利用了集中控制策略，该策略以整个状态为条件，并且可以使用强大的宏观动作，使用星际争霸的内置规划器，结合移动和攻击动作 . 为了产生一个有意义的去中心化基准，证明即使在代理相对较少的情况下也具有挑战性，我们提出了一种变体，可以大规模减少每个代理的视野并删除对这些宏观动作的访问。</p>
<p>​    我们在这个新基准上的实证结果表明，与其他多代理 actor-critic 方法以及 COMA 本身的消融版本相比，COMA 可以显着提高性能。 此外，COMA 的最佳代理可以与最先进的集中控制器竞争，后者可以访问完整的状态信息和宏操作。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>​    尽管多智能体强化学习已应用于各种设置(Busoniu, Babuska, and De Schutter 2008; Yang and Gu 2004)，但它通常仅限于表格方法和简单环境。 一个例外是最近在深度多智能体强化学习方面的工作，它可以扩展到高维输入和动作空间。 Tampuu et al. (2015)将 DQN 与独立 Q-learning相结合 (Tan 1993; Shoham and Leyton-Brown 2009)来学习如何玩两人乒乓球。 最近Leibo et al. (2017)等人使用了相同的方法研究在sequential社会困境中合作和背叛的出现。</p>
<p>​    同样相关的还有智能体之间进行通信的出现，并通过梯度下降学习(Das et al. 2017; Mordatch and Abbeel 2017; Lazaridou, Peysakhovich, and Baroni 2016; Foerster et al. 2016; Sukhbaatar, Fergus, and others 2016) 。在这一系列工作中，在训练期间在智能体之间传递梯度和共享参数是利用集中训练的两种常见方式。 然而这些方法不允许在学习期间使用额外的状态信息，也不能解决多智能体信用分配问题。</p>
<p>​    Gupta, Egorov, and Kochenderfer (2017)研究了通过集中训练实现分散执行的actor-critic方法。 然而在他们的方法中，局部、每个代理、observations和actions以及多代理信用分配的演员和评论家条件都只能通过手工制作的局部奖励来解决。</p>
<p>​    RL 以前在StarCraft management中的大多数应用都使用集中式控制器，可以访问完整状态并控制所有单元，尽管控制器的架构利用了问题的多代理性质。 Usunier et al. (2016) 使用greedy MDP，它在每个时间步按顺序选择给定之前所有动作的代理的动作，并结合 zero-order optimisation(零阶优化)，而 Peng et al. (2017)使用依赖 RNN 在代理之间交换信息的 actor-critic 方法。</p>
<p>​    最接近我们的问题设置的是Foerster et al. (2017)的问题。他们也使用多代理表示和分散策略。 然而，他们在使用 DQN 时专注于稳定经验回放，并没有充分利用集中训练制度。 由于他们不报告绝对赢率，我们不直接比较性能。 然而， Usunier et al. (2016)  解决了与我们的实验类似的场景，并在完全可观察的环境中实现了 DQN 基线。 因此，在第 6 节中，我们报告了我们针对这些最先进基线的竞争表现，同时保持分散控制。Omidshafifiei et al. (2017)还解决了多智能体环境中经验回放的稳定性问题，但假设了一个完全分散的训练制度。</p>
<p>​    (Lowe et al. 2017)同时提出了一种使用集中式critic的多代理策略梯度算法。 他们的方法没有解决多代理信用分配问题。 与我们的工作不同，它为每个代理学习一个单独的集中式critic，并应用于具有连续动作空间的竞争环境。</p>
<p>​    我们的工作直接建立在difference rewards (Wolpert and Tumer 2002)的思想之上。 COMA 与这工作的关系在第 4 节中讨论。</p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>​    我们考虑一个完全合作的多智能体任务，它可以被描述为一个随机博弈 <script type="math/tex">G</script>，由一个元组<script type="math/tex">G= \left \langle S,U,P,r,Z,O,n,\gamma  \right \rangle</script>定义，其中<script type="math/tex">n</script>个智能体由<script type="math/tex">a\in A\equiv \left \{ 1,...,n \right \}</script>选择时序动作。 环境有一个真实的状态 <script type="math/tex">s\in S</script>。在每个时间步，每个智能体同时选择一个动作<script type="math/tex">u^{a}\in U</script>，形成一个联合动作<script type="math/tex">u\in U\equiv U^{n}</script>，根据状态转换函数在环境中引起转换 <script type="math/tex">P\left ( s^{'}|s,u \right ):S\times U\times S\rightarrow \left [ 0,1 \right ]</script>。 代理都共享相同的奖励函数<script type="math/tex">r\left ( s,u \right ):S\times U\rightarrow r</script>和<script type="math/tex">\gamma \in \left [ 0,1 \right )</script>是一个折扣因子。</p>
<p>​    我们考虑一个部分可观察的设置，其中代理根据观察函数 <script type="math/tex">O(s,a):S\times A\rightarrow Z</script>绘制观察<script type="math/tex">z\in Z</script>。每个代理都有一个动作观察历史<script type="math/tex">\tau ^{a}\in T\equiv \left ( Z\times U \right )^{\ast }</script>，它以随机策略<script type="math/tex">\pi ^{a\left ( u^{a}|\tau ^{a} \right )}:T\times U\rightarrow \left [ 0,1 \right ]</script>为条件。 我们用粗体表示代理的联合数量，并用上标<script type="math/tex">-a</script>表示给定代理<script type="math/tex">a</script>以外的代理的联合数量。</p>
<p>​    折扣回报为<script type="math/tex">R_{t}=\sum_{l=0}^{\infty }\gamma ^{l}r_{t+l}</script>。代理的联合策略引入一个值函数，即期望超过<script type="math/tex">R_{t}</script>、Vπ(st)=Est+1：∞，ut：∞[Rt|st]，以及<script type="math/tex">V^{\pi }\left ( s_{t} \right )=E_{s_{t+1}:\infty,u_{t}:\infty  }\left [ R_{t}|s_{t}\right ]</script>和action-value函数<script type="math/tex">Q^{\pi }(s_{t},u_{t})=E_{s_{t+1}:\infty,u_{t+1}:\infty  }\left [ R_{t}|s_{t},u_{t} \right ]</script>。优势函数由<script type="math/tex">A^{\pi }\left ( s_{t},u_{t} \right )=Q^{\pi }\left ( s_{t},u_{t} \right )-V^{\pi }\left ( s_{t} \right )</script>给出。</p>
<p>​    继之前的工作(Oliehoek, Spaan, and Vlassis 2008; Kraemer and Banerjee 2016; Foerster et al. 2016; Jorge, Kageback, and Gustavsson 2016)之后，我们的问题设置允许集中训练，但需要分散执行。 这是大量多智能体问题的自然范例，其中使用具有附加状态信息的模拟器进行训练，但智能体在执行期间必须依赖于局部动作观察历史。 为了以完整的历史为条件，深度强化学习代理可能会使用循环神经网络(Hausknecht and Stone 2015)，通常使用门控模型，例如 LSTM (Hochreiter and Schmidhuber 1997) 或 GRU(Cho et al. 2014)。</p>
<p>​    在第 4 节中，我们开发了一种新的多智能体策略梯度方法来解决这个问题。 在本节的其余部分，我们提供了有关单代理策略梯度方法的一些背景知识 (Sutton et al. 1999)。 这些方法通过对预期折扣总奖励<script type="math/tex">J=E_{\pi }\left [ R_{0} \right ]</script>的估计执行梯度上升来优化由<script type="math/tex">\theta ^{\pi }</script>参数化的单个代理的策略。 也许最简单的策略梯度形式是 REINFORCE (Williams 1992)，其中的梯度是：</p>
<script type="math/tex; mode=display">
g=E_{s_{0}:\infty ,u_{0}:\infty }\left [ \sum_{t=0}^{T}R_{t}\bigtriangledown _{\theta ^{\pi }}log\pi \left ( u_{t}|s_{t} \right ) \right ]</script><p>​    在actor-critic方法中 (Sutton et al. 1999; Konda and Tsitsiklis 2000; Schulman et al. 2015)，actor即策略，通过遵循依赖于critic(通常估计一个值函数)的梯度进行训练。特别是<script type="math/tex">R_{t}</script>被任何等价于<script type="math/tex">Q\left ( s_{t},u_{t} \right )-b\left ( s_{t} \right )</script> 的表达式替换，其中<script type="math/tex">b\left ( s_{t} \right )</script>是设计用于减少方差的基线 (Weaver and Tao 2001)。 一个常见的选择是 <script type="math/tex">b\left ( s_{t} \right )=V\left ( s_{t} \right )</script>，在这种情况下，<script type="math/tex">R_{t}</script>被<script type="math/tex">A(s_{t},u_{t})</script>替换。 另一种选择是用temporal difference(TD) 误差<script type="math/tex">r_{t}+\gamma V\left ( s_{t+1} \right )-V\left ( s \right )</script>替换 <script type="math/tex">R_{t}</script>，这是<script type="math/tex">A(s_{t},u_{t})</script>的无偏估计。 在实践中，梯度必须根据从环境中采样的轨迹来估计，并且（动作）值函数必须用函数逼近器来估计。 因此，梯度估计的偏差和方差在很大程度上取决于估计量的确切选择 (Konda and Tsitsiklis 2000)。</p>
<p>​    在本文中，我们使用<script type="math/tex">TD(\lambda)</script>(Sutton1988)适用于深度神经网络的变体，训练critic <script type="math/tex">f^{c}\left ( .,\theta ^{c} \right )</script> on-policy来估计<script type="math/tex">Q</script>或<script type="math/tex">V</script>。<script type="math/tex">TD(\lambda)</script>混合使用<script type="math/tex">n</script>步返回<script type="math/tex">G_{t}^{(n)}=\sum_{l=1}^{n}\gamma ^{l-1}r_{t+l}+\gamma ^{n}f^{c}\left ( ._{t+n},\theta ^{c} \right )</script>。特别是，critic参数<script type="math/tex">\theta^{c}</script>通过小批梯度下降进行更新，以最小化以下loss:</p>
<script type="math/tex; mode=display">
L_{t}\left ( \theta ^{c} \right )=\left ( y^{\left ( \lambda  \right )}-f^{c}\left ( ._{t},\theta ^{c} \right ) \right )^{2}</script><p>其中<script type="math/tex">y^{\left ( \lambda  \right )}=\left ( 1-\lambda  \right )\sum_{n=1}^{\infty }\lambda ^{n-1}G_{t}^{(n)}</script>和n-step返回<script type="math/tex">G_{t}^{(n)}</script>是用目标网络(Mnih et al. 2015)估计的bootstrapped值计算的，定期从<script type="math/tex">\theta^ {c}</script>复制参数。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>在本节中，我们将描述将策略梯度扩展到我们的多代理设置的方法。</p>
<h2 id="Independent-Actor-Critic"><a href="#Independent-Actor-Critic" class="headerlink" title="Independent Actor-Critic"></a>Independent Actor-Critic</h2><p>​    将策略梯度应用于多个智能体的最简单方法是让每个智能体独立学习，有自己的actor和critic，根据自己的action-observation历史。 这本质上是independent Q-learning(Tan 1993)背后的想法，它可能是最流行的多智能体学习算法，但用 actor-critic 代替了 Q-learning。 因此，我们称这种方法为independent actor-critic (IAC)。</p>
<p>​    在我们的 IAC 实现中，我们通过在代理之间共享参数来加速学习，即我们只学习一个actor和一个critic，所有智能体都使用它们。 智能体仍然可以表现不同，因为它们接收不同的观察，包括特定于智能体的 ID，从而演化出不同的隐藏状态。 学习保持独立，因为每个智能体的critic只估计一个局部价值函数，即以<script type="math/tex">u^{a}</script>而不是<strong>u</strong>为条件的价值函数。 虽然我们不知道这个特定算法以前的应用，但我们不认为它是一个重大贡献，而只是一个基线算法。</p>
<p>​    我们考虑 IAC 的两种变体。 首先，每个智能体的critic估计<script type="math/tex">V\left ( \tau ^{a} \right )</script>并遵循基于<script type="math/tex">TD</script>误差的梯度，如第 3 节所述。第二，每个智能体的critic估计<script type="math/tex">Q\left ( \tau ^{a},u^{a} \right )</script>并遵循一个梯度基于优势:<script type="math/tex">A\left ( \tau ^{a},u^{a} \right ) = Q\left ( \tau ^{a},u^{a} \right )-V\left ( \tau ^{a} \right )</script>，其中<script type="math/tex">V\left ( \tau ^{a} \right )=\sum_{u^{a}}^{}\pi \left ( u^{a}|\tau ^{a} \right )Q\left ( \tau ^{a},u^{a} \right )</script>。独立学习很简单，但在训练时缺乏信息共享使得学习依赖于多个智能体之间交互的协调策略变得困难，或者单个智能体难以估计其行为对团队奖励的贡献。</p>
<h2 id="Counterfactual-Multi-Agent-Policy-Gradients"><a href="#Counterfactual-Multi-Agent-Policy-Gradients" class="headerlink" title="Counterfactual Multi-Agent Policy Gradients"></a>Counterfactual Multi-Agent Policy Gradients</h2><p>​    上面讨论的困难之所以出现，是因为除了参数共享之外，IAC 未能利用学习集中在我们的环境中这一事实。 在本节中，我们提出了反事实多智能体 (COMA) 策略梯度，它克服了这一限制。 COMA 背后的三个主要思想：1) critic的集中化，2) 使用反事实基线，以及 3) 使用允许对基线进行有效评估的critic表示。 本节的其余部分描述了这些想法。</p>
<p>​    首先，COMA 使用集中式critic。 请注意，在 IAC 中，每个actor <script type="math/tex">\pi \left ( u^{a}|\tau ^{a} \right )</script> 和每个critic <script type="math/tex">Q\left (\tau ^{a}, u^{a}\right )</script>或 <script type="math/tex">V\left ( \tau ^{a} \right )</script>仅以代理自己的动作观察历史<script type="math/tex">\tau ^{a}</script>为条件。 但是，critic仅在学习期间使用，在执行期间只需要actor。 由于学习是集中式的，因此我们可以使用集中式critic，该critic以真实的全局状态<script type="math/tex">s</script>为条件（如果可用），否则使用联合动作观察历史<script type="math/tex">\tau</script>。 每个actor以自己的动作观察历史<script type="math/tex">\tau ^{a}</script>为条件，参数共享，如在 IAC 中。 图 1a 说明了这种设置。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/figure1.png" alt></p>
<p>​    使用这个集中式critic的一种普通的方法是让每个actor遵循基于从这个critic估计的<script type="math/tex">TD</script>误差的梯度：</p>
<script type="math/tex; mode=display">
g=\bigtriangledown _{\theta ^{\pi }}log\pi \left ( u|\tau _{t}^{a} \right )\left ( r+\gamma V\left ( s_{t+1} \right) -V\left ( s_{t} \right )\right )</script><p>​    然而，这种方法未能解决关键的信用分配问题。 由于 TD 误差仅考虑全局奖励，因此为每个actor计算的梯度并未明确判断该特定代理的行为如何对全局奖励做出贡献。 由于其他代理可能正在探索，该代理的梯度变得非常嘈杂，特别是当有很多代理时。</p>
<p>​    因此，COMA 使用counterfactual baseline（反事实基线）。 这个想法受到difference rewards (Wolpert and Tumer 2002)的启发，其中每个智能体从一个成形的奖励<script type="math/tex">D^{a}=r\left ( s,u \right )-r\left ( s,\left ( u^{-a},c^{a} \right ) \right )</script>中学习，该奖励将全局奖励与当代理 a 的动作被默认动作<script type="math/tex">c^{a}</script>替换时收到的奖励进行比较。 代理<script type="math/tex">a</script>改进<script type="math/tex">D^{a}</script>的任何动作也会提高真实的全局奖励<script type="math/tex">r\left ( s,u \right )</script>，因为<script type="math/tex">r\left ( s,\left ( u^{-a},c^{a} \right ) \right )</script>不依赖于代理<script type="math/tex">a</script>的动作。</p>
<p>​    Difference rewards是执行多智能体信用分配的有效方式。 但是，它们通常需要访问模拟器才能估计<script type="math/tex">r\left ( s,\left ( u^{-a},c^{a} \right ) \right )</script>。 当模拟器已经用于学习时，difference rewards会增加必须进行的模拟次数，因为每个代理的difference reward需要单独的反事实模拟。  Proper and Tumer (2012) and Colby, Curran, and Tumer (2015)建议使用函数近似而不是模拟器来估计差异奖励。 但是，这仍然需要用户指定的默认操作 <script type="math/tex">c^{a}</script>，这在许多应用程序中可能难以选择。 在 actor-critic 架构中，这种方法还会引入额外的近似误差源。</p>
<p>​    COMA 背后的一个关键见解是，可以使用集中式critic以避免这些问题的方式实现difference rewards。 COMA 学习了一个集中式critic <script type="math/tex">Q(s, u)</script>，它估计了以中央状态<script type="math/tex">s</script>为条件的联合动作<script type="math/tex">u</script>的<script type="math/tex">Q</script>值。 然后，对于每个代理 <script type="math/tex">a</script>，我们可以计算一个优势函数，将当前动作<script type="math/tex">u^{a}</script>的<script type="math/tex">Q</script>值与边缘化<script type="math/tex">u^{a}</script>的反事实基线进行比较，同时保持其他代理的动作<script type="math/tex">u^{-a}</script>固定：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula4.png" alt></p>
<p>​    因此，<script type="math/tex">A^{a}\left ( s,u^{a} \right )</script>为每个代理计算单独的基线，该基线使用集中式critic来推理反事实，其中只有 a 的动作发生变化，直接从代理的经验中学习，而不是依赖额外的模拟、奖励模型或 用户设计的默认操作。</p>
<p>​    这种优势与aristocrat utility (Wolpert and Tumer 2002)具有相同的形式。 然而，使用基于价值的方法优化aristocrat utility 会产生 self-consistency（自一致性）问题，因为策略和 utility function相互依赖。 因此，先前的工作侧重于使用默认状态和操作进行差异评估。 COMA 是不同的，因为反事实基线对梯度的预期贡献，与其他策略梯度基线一样为零。 因此，虽然基线确实取决于策略，但它的期望却不是。 因此，COMA 可以使用这种形式的优势而不会产生self-consistency问题。</p>
<p>​    虽然 COMA 的优势函数用critic的评估代替了潜在的额外模拟，但如果critic是深度神经网络，这些评估本身可能很昂贵。此外，在典型的表示中，这种网络的输出节点数将等于<script type="math/tex">\left | U \right |^{n}</script>，即联合动作空间的大小，这使得训练变得不切实际。为了解决这两个问题，COMA 使用了一种critic表示，可以有效地评估基线。特别是，其他代理的动作<script type="math/tex">u_{t}^{-a}</script>是网络输入的一部分，网络为每个代理 a 的动作输出 Q 值，如图 1c 所示。因此，对于每个代理，可以通过actor和critic的单次前向传递有效地计算反事实优势。此外，输出的数量只有<script type="math/tex">\left | U \right |</script>而不是 (<script type="math/tex">\left | U \right |^{n}</script>)。虽然网络有一个很大的输入空间，可以在代理和动作的数量上线性扩展，但深度神经网络可以很好地在这些空间上泛化。 </p>
<p>​    在本文中，我们专注于具有离散动作的设置。 然而通过使用蒙特卡罗样本估计（4）中的期望或使用使其具有分析性的函数形式（例如高斯策略和critic），可以轻松地将 COMA 扩展到连续动作空间。</p>
<p>​    以下引理建立了 COMA 收敛到局部最优策略。 证明直接来自单代理actor-critic算法的收敛 (Sutton et al. 1999; Konda and Tsitsiklis 2000)，并且服从相同的假设。</p>
<h3 id="Lemma-1"><a href="#Lemma-1" class="headerlink" title="Lemma 1"></a>Lemma 1</h3><p>​    对于一个具有遵循COMA策略梯度的兼容<script type="math/tex">TD(1)</script> critic的actor-critic算法。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula5.png" alt></p>
<p>在每次迭代的k处</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula6.png" alt></p>
<p>证明:COMA梯度如下</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula7.png" alt></p>
<p>其中<script type="math/tex">\theta</script>是所有actor策略的参数，例如<script type="math/tex">\theta =\left \{ \theta ^{1},...,\theta ^{\left | A \right |} \right \}</script>和<script type="math/tex">b\left ( s,u^{-a} \right )</script>是等式4中定义的反事实基线。</p>
<p>​    首先考虑该基线<script type="math/tex">b\left ( s,u^{-a} \right )</script>的预期贡献：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula9.png" alt></p>
<p>其中期望<script type="math/tex">E_{\pi}</script>是关于由联合策略<script type="math/tex">\pi</script>引起的状态行动分布的。现在让<script type="math/tex">d^{\pi}(s)</script>是Sutton et al. (1999)定义的折现遍历状态分布：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula10.png" alt></p>
<p>显然，每代理的基线虽然降低了方差，但不会改变预期的梯度，因此不会影响COMA的收敛性。</p>
<p>​    预期策略梯度的其余部分为：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula13.png" alt></p>
<p>将联合政策作为独立actor的产物来制定：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula15.png" alt></p>
<p>生成标准的单代理actor-critic策略梯度：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula16.png" alt></p>
<p>​    Konda and Tsitsiklis (2000)证明了遵循这个梯度的actor-critic收敛于期望返回<script type="math/tex">J^{\pi }</script>的局部最大值：</p>
<ol>
<li><p>策略<script type="math/tex">\pi</script>是可微的，</p>
</li>
<li><p><script type="math/tex">Q</script>和<script type="math/tex">\pi</script>的更新时间尺度都足够慢，而且<script type="math/tex">\pi</script>的更新速度比<script type="math/tex">Q</script>足够慢。</p>
</li>
<li><p><script type="math/tex">Q</script>使用了与<script type="math/tex">\pi</script>兼容的表示法</p>
<p>在几个进一步的假设中。 策略的参数化（即单代理联合动作学习器被分解为独立的actor）对于收敛来说并不重要，只要它保持可微分即可。 但是请注意，COMA 的中心化critic对于此证明的成立至关重要。</p>
</li>
</ol>
<h1 id="实验装置"><a href="#实验装置" class="headerlink" title="实验装置"></a>实验装置</h1><p>​    在本节中，我们将描述我们应用 COMA 的星际争霸问题，以及状态特征、网络架构、训练机制和消融的细节。</p>
<h2 id="Decentralised-StarCraft-Micromanagement"><a href="#Decentralised-StarCraft-Micromanagement" class="headerlink" title="Decentralised StarCraft Micromanagement"></a>Decentralised StarCraft Micromanagement</h2><p>​    《星际争霸》是一个具有随机动态的丰富环境，无法轻易模拟。 相比之下，许多更简单的多代理设置，例如 Predator-Prey (Tan 1993) 或 Packet World (Weyns, Helleboogh, and Holvoet 2005)，具有完整的模拟器，可以控制随机性，可以自由设置为任何状态，以便完美地 重播经历。 这使得通过额外的模拟计算差异奖励成为可能，尽管计算成本很高。 在星际争霸中，就像在现实世界中一样，这是不可能的。</p>
<p>​    在本文中，我们关注的是星际争霸中的micromanagement问题，它指的是在与敌人作战时对单个单位的定位和攻击命令的低级控制。 这个任务自然表现为一个多代理系统，其中每个星际争霸单元都被一个分散的控制器所取代。 我们考虑了几种由对称团队组成的场景：3 名海军陆战队员 (3m)、5 名海军陆战队员 (5m)、5 名幽灵 (5w) 或 2 名龙骑兵和 3 名狂热者 (2d 3z)。 敌方团队由星际争霸 AI 控制，它使用合理但次优的手工启发式方法。</p>
<p>​    我们允许代理从一组离散的操作中进行选择：move[direction], attack[enemy id],stop, and noop。在星际争霸游戏中，当一个单位选择攻击动作时，它会先移动到攻击范围内再开火，利用游戏内置的寻路来选择路线。 这些强大的attack-move 宏动作使控制问题变得相当容易。</p>
<p>​    为了创建一个更具挑战性的、更有意义的去中心化基准，我们对代理施加了一个有限的视野，等于远程单位武器的射程，如图 2 所示。这偏离了集中式星际争霸控制的标准设置有三个作用。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/figure2.png" alt></p>
<p>​    首先，它引入了显着的部分可观察性。 其次，这意味着单位只能在敌人范围内进行攻击，从而无法访问星际争霸宏操作。 第三，代理无法区分已死亡的敌人和超出范围的敌人，因此可以向这些敌人发出无效的攻击命令，从而导致不采取任何行动。 这大大增加了动作空间的平均大小，从而增加了探索和控制的难度。</p>
<p>​    在这些困难的条件下，即使单元数量相对较少的场景也变得更加难以解决。 如表 1 所示，我们与一个简单的手工编码启发式进行了比较，该启发式指示代理向前跑到范围内，然后集中火力，依次攻击每个敌人直到其死亡。 这种启发式在 5m 的全视野下实现了 98% 的胜率，但在我们的设置中只有 66%。 为了在这项任务中表现出色，代理必须通过正确定位和集中火力来学习合作，同时记住哪些敌方和盟军单位还活着或不在视野中。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/table1.png" alt></p>
<p>​    所有代理在每个时间步都会收到相同的全局奖励，等于对对手单位造成的伤害总和减去所受伤害的一半。 杀死对手会产生 10 分的奖励，赢得比赛会产生等于团队剩余总生命值加 200 的奖励。 这种基于伤害的奖励信号与 Usunier et al. (2016)使用的信号相当。 与 (Peng et al. 2017)不同，我们的方法不需要估计本地奖励。</p>
<h2 id="State-Features"><a href="#State-Features" class="headerlink" title="State Features"></a>State Features</h2><p>​    actor和critic接收不同的输入特征，分别对应于局部观察和全局状态。 两者都包括盟友和敌人的特征。 单位可以是盟友或敌人，而智能体是指挥盟友单位的分散控制器。</p>
<p>​    每个代理的局部观察仅从以它控制的单位为中心的地图的圆形子集中绘制，并包括该视野内的每个单位：distance, relative x, relative y, unit type and shield（盾牌）（开火后一个单位的冷却时间被重置，它必须在再次开火前掉落。 盾牌吸收伤害直到它们破裂，之后单位开始失去健康。 龙骑兵和狂热者有盾牌，但海军陆战队没有）。 所有特征都是通过它们的最大值归一化。 我们不包括有关单位当前目标的任何信息。</p>
<p>​    全局状态表示由相似的特征组成，但对于地图上的所有单元，无论视野如何。 不包括绝对距离，并且 x-y 位置是相对于地图中心而不是特定代理给出的。 全局状态还包括所有代理的生命值和冷却时间。 提供给集中式 Q-function critic的表示是全局状态表示与正在评估其行为的代理的局部观察的连接。 我们估计 V (s) 的中心化critic，因此是代理不可知的，接收与所有代理的观察连接的全局状态。 观察结果不包含新信息，但包括相对于该代理的自我中心距离。</p>
<h2 id="Architecture-amp-Training"><a href="#Architecture-amp-Training" class="headerlink" title="Architecture &amp; Training"></a>Architecture &amp; Training</h2><p>​    参与者由128-bit门控循环单元(GRU)(Cho et al. 2014)组成。使用全连接层来处理输入和从隐藏状态产生输出值<script type="math/tex">h_{t}^{a}</script>。IAC的critic使用额外的output heads附加到actor网络的最后一层。动作概率是通过最后一层<script type="math/tex">z</script>，通过一个有界的softmax分布产生的，任何给定动作的概率下界为<script type="math/tex">\epsilon /\left | U \right |:P\left ( u \right )=\left ( 1-\epsilon  \right )softmax(z)_{u}+\epsilon /\left | U \right |</script>。我们在750个episode中线性地将<script type="math/tex">\epsilon</script>从0.5退火到0.02。集中的critic是一个前馈网络，具有多个ReLU层和全连接层。超参数在5m场景上进行了粗略调整，然后用于所有其他地图。我们发现最敏感的参数是<script type="math/tex">TD\left ( \lambda  \right )</script>，但确定了<script type="math/tex">\lambda = 0.8</script>，它对COMA和我们的baselines都最有效。我们的实现使用了TorchCraft (Synnaeve et al. 2016)和Torch 7 (Collobert, Kavukcuoglu, and Farabet 2011)。伪代码和关于训练程序的进一步细节见补充资料。</p>
<p>​    我们尝试了在代理水平考虑的critic体系结构，并进一步利用内部参数共享。然而，我们发现可伸缩性的瓶颈不是critic的集中，而是多智能体探索的困难。因此，我们推迟对COMA critic的因素的进一步调查到未来的工作中。</p>
<h2 id="Ablations"><a href="#Ablations" class="headerlink" title="Ablations"></a>Ablations</h2><p>​    我们进行了消融实验，验证了COMA的三个关键元素。首先，我们通过比较两种IAC变体，IAC-Q和IAC-V来测试集中化critic的重要性。这些critic采用与actor相同的分散输入，并与actor网络共享参数，直到最后一层。然后，IAC-Q输出<script type="math/tex">\left | U \right |</script> Q-values，每个动作输出一个，而IAC-V输出单个state-value。请注意，我们仍然在代理之间共享参数，使用以自我为中心的观察和ID作为输入的一部分，以允许出现不同的行为。合作报酬函数仍然由所有代理共享。</p>
<p>​    其次，我们测试了学习Q而不是V的重要性。该方法central-V 的critic仍然使用中心状态，但学习V(s)，并使用TD误差来估计策略梯度更新的优势。</p>
<p>​    第三，我们测试了我们的反事实基线的作用。central-QV方法同时学习Q和V，并估计其优势为Q−V，用V取代了COMA的反事实基线。所有的方法都为actor使用相同的架构和训练方案，所有的critic都使用<script type="math/tex">TD\left ( \lambda  \right )</script>进行训练。</p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>​    图3显示了每种方法和每种星际争霸场景的平均胜率。对于每种方法，我们进行了35次独立的试验，每训练100个episode存储一次模型，用于通过200 episode评估每种方法，绘制出每个episode和试验的平均水平。还显示了性能上的一个标准偏差。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/figure3.png" alt></p>
<p>​    结果表明，COMA在所有情况下都优于IAC基线。有趣的是，IAC方法最终也在5m场景学习了合理的策略，尽管它们需要更多的episode来实现。这可能似乎违反直觉，因为在IAC方法中，actor和critic网络在他们的早期层中共享参数（参见第5节），这可能会加速学习。然而，这些结果表明，通过使用全局状态而提高策略评估的准确性，超过了训练单独网络的开销。</p>
<p>​    此外，在所有设置的训练速度和最终性能方面，COMA在central-QV中具有重要地位。这是一个强有力的指标，表明当我们使用中心Q-critic来训练分散的策略时，我们的反事实基线是至关重要的。</p>
<p>​    学习状态值函数具有不以联合动作为条件的明显优势。 尽管如此，我们发现 COMA 在最终性能方面优于central-V 基线。 此外，COMA 通常更快地实现良好的策略，这是预期的，因为 COMA 提供了一个成形的训练信号。 训练也比 central-V 更稳定，这是 COMA 梯度随着策略变得贪婪而趋于零的结果。 总的来说，COMA 是性能最好和最一致的方法。</p>
<p>​    Usunier et al. (2016) 报告了他们最好的代理的性能，他们用他们最先进的集中式控制器标记为 GMEZO（具有episodic zero-order optimisation(Zero-Order Optimization Methods with Applications to RL )零阶优化的贪婪 MDP），以及集中式 DQN 控制器，两者都给出了完整的视野和访问攻击移动宏动作。 这些结果在表 1 中与针对每个地图使用 COMA 训练的最佳代理进行了比较。 显然，在大多数情况下，这些代理的性能可与公布的最佳获胜率相媲美，尽管它们受到分散策略和本地视野的限制。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/table1.png" alt></p>
<h1 id="总结和未来工作"><a href="#总结和未来工作" class="headerlink" title="总结和未来工作"></a>总结和未来工作</h1><p>​    本文介绍了 COMA 策略梯度，这是一种使用集中式critic来估计多智能体 RL 中分散式策略的反事实优势的方法。 COMA 通过使用反事实基线来解决多代理信用分配的挑战，该基线将单个代理的行为边缘化，同时保持其他代理的行为不变。 我们在分散式StarCraft unit micromanagement基准测试中的结果表明，COMA 与其他多智能体 actor-critic 方法相比显着提高了最终性能和训练速度，并在最佳性能报告下与最先进的集中控制器保持竞争力。 未来的工作将扩展 COMA 以处理具有大量代理的场景，在这些场景中，集中式critic更难训练，探索更难协调。 我们还旨在开发更多样本高效的变体，这些变体适用于自动驾驶汽车等实际应用。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/01/deep-learning-tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/01/deep-learning-tutorial/" class="post-title-link" itemprop="url">deep_learning_tutorial</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 23:02:57" itemprop="dateCreated datePublished" datetime="2021-06-01T23:02:57+08:00">2021-06-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-15 22:31:01" itemprop="dateModified" datetime="2021-06-15T22:31:01+08:00">2021-06-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="深度学习笔记"><a href="#深度学习笔记" class="headerlink" title="深度学习笔记"></a>深度学习笔记</h1><p><a target="_blank" rel="noopener" href="https://www.zybuluo.com/hanbingtao/note/433855">转载至深度学习系列教程</a></p>
<h2 id="感知器-神经元"><a href="#感知器-神经元" class="headerlink" title="感知器(神经元)"></a>感知器(神经元)</h2><p>神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是阶跃函数；而当我们说神经元时，激活函数往往选择为sigmoid函数或者tanh函数等。</p>
<p>阶跃函数：</p>
<script type="math/tex; mode=display">
f(z)=\left\{\begin{matrix}
1 &z>0 \\ 
0 & otherwise
\end{matrix}\right.</script><p>神经网络示意图：</p>
<p><img src="/2021/06/01/deep-learning-tutorial/神经网络示意图.png" alt></p>
<p>上图中每个圆圈都是一个神经元，每条线表示神经元之间的连接。我们可以看到，上面的神经元被分成了多层，层与层之间的神经元有连接，而层内之间的神经元没有连接。最左边的层叫做<strong>输入层</strong>，这层负责接收输入数据；最右边的层叫<strong>输出层</strong>，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做<strong>隐藏层</strong>。</p>
<p>隐藏层比较多（大于2）的神经网络叫做深度神经网络。而深度学习，就是使用深层架构（比如，深度神经网络）的机器学习方法。</p>
<p>那么深层网络和浅层网络相比有什么优势呢？简单来说深层网络能够表达力更强。事实上，一个仅有一个隐藏层的神经网络就能拟合任何一个函数，但是它需要很多很多的神经元。而深层网络用少得多的神经元就能拟合同样的函数。也就是为了拟合一个函数，要么使用一个浅而宽的网络，要么使用一个深而窄的网络。而后者往往更节约资源。</p>
<p>深层网络也有劣势，就是它不太容易训练。简单的说，你需要大量的数据，很多的技巧才能训练好一个深层网络。这是个手艺活。</p>
<h3 id="感知器定义"><a href="#感知器定义" class="headerlink" title="感知器定义"></a>感知器定义</h3><p><img src="/2021/06/01/deep-learning-tutorial/感知器.png" alt></p>
<p>感知器组成部分：</p>
<ul>
<li><strong>输入权值</strong> ：一个感知器可以接收多个输入{x<sub>1</sub>,x<sub>2</sub>,…,x<sub>n</sub>|x<sub>i</sub>∈R}，每个输入上有一个<strong>权值</strong>w<sub>i</sub>∈R，此外还有一个<strong>偏置项</strong>b∈R，就是上图中的w<sub>0</sub>；</li>
<li><strong>激活函数f(x)</strong>：感知器的激活函数可以有很多选择，比如sigmoid函数；</li>
<li><strong>输出</strong>：感知器的输出由下面这个公式来计算。</li>
</ul>
<script type="math/tex; mode=display">
y=f(w*x+b)</script><h3 id="感知器训练"><a href="#感知器训练" class="headerlink" title="感知器训练"></a>感知器训练</h3><p>假设损失函数为均方差函数(MSE Mean Square Error)</p>
<script type="math/tex; mode=display">
J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}(a_{i}-y_{i})^{2}\\
J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}(w_{0}x_{0}+w_{1}x_{1}+...+w_{n}x_{n}-y_{i})^{2}</script><p>a<sub>i</sub>为预测值，y<sub>i</sub>为实际值。设m=1(训练样本为1条)时</p>
<script type="math/tex; mode=display">
\frac{\partial J(w,b)}{\partial w_{1}}=x_{1}(w_{0}x_{0}+w_{1}x_{1}+...+w_{n}x_{n}-y_{i})</script><p>其余参数导数求法相同。</p>
<p>使用梯度下降更新：</p>
<script type="math/tex; mode=display">
w_{i}=w_{i}-\alpha \frac{\partial J(w_{i},b)}{\partial w_{i}}</script><p>其中α为学习率。如果损失函数内为y<sub>i</sub>-a<sub>i</sub> ，则上面的梯度下降更新负号应为正号。</p>
<h2 id="梯度下降优化算法"><a href="#梯度下降优化算法" class="headerlink" title="梯度下降优化算法"></a>梯度下降优化算法</h2><p><img src="/2021/06/01/deep-learning-tutorial/梯度下降示例图.png" alt></p>
<p>函数y=f(x)的极值点就是它的导数f<sup>‘</sup>(x)=0的那个点。因此我们可以通过解方程f<sup>‘</sup>(x)=0,求得函数的极值点。</p>
<p>对于计算机来说，随便选择一个点开始，比如上图的点x<sub>0</sub>。接下来，每次迭代修改的为x<sub>1</sub>,<sub>2</sub>,…，经过数次迭代后最终达到函数最小值点。</p>
<p>你可能要问了，为啥每次修改的值，都能往函数最小值那个方向前进呢？这里的奥秘在于，我们每次都是向函数y=f(x)的<strong>梯度</strong>的<strong>相反方向</strong>来修改。什么是<strong>梯度</strong>呢？翻开大学高数课的课本，我们会发现<strong>梯度</strong>是一个向量，它指向<strong>函数值上升最快</strong>的方向。显然，梯度的反方向当然就是函数值下降最快的方向了。我们每次沿着梯度相反方向去修改的值，当然就能走到函数的最小值附近。之所以是最小值附近而不是最小值那个点，是因为我们每次移动的步长不会那么恰到好处，有可能最后一次迭代走远了越过了最小值那个点。步长的选择是门手艺，如果选择小了，那么就会迭代很多轮才能走到最小值附近；如果选择大了，那可能就会越过最小值很远，收敛不到一个好的点上。</p>
<p>梯度下降算法的公式</p>
<script type="math/tex; mode=display">
X_{new}=X_{old}-\alpha \bigtriangledown f(x)</script><p>α为学习率（步长）</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><p>神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是<strong>阶跃函数</strong>；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数。如下图所示：</p>
<p><img src="/2021/06/01/deep-learning-tutorial/sigmoid神经元.png" alt></p>
<p>计算一个神经元的输出的方法和计算一个感知器的输出是一样的。假设神经元的输入是向量<script type="math/tex">\overrightarrow{x}</script></p>
<p> ，权重向量是<script type="math/tex">\overrightarrow{w}</script>(偏置项是w<sub>0</sub>)，激活函数是sigmoid函数，则其输出y：</p>
<script type="math/tex; mode=display">
y=sigmoid(\vec{w}^{T}*\vec{x})\tag1</script><p>sigmoid函数定义如下：</p>
<script type="math/tex; mode=display">
sigmoid(x)=\frac{1}{1+e^{-x}}</script><p>将其带入前面的式子，得到</p>
<script type="math/tex; mode=display">
y=\frac{1}{1+e^{-\vec{w}^{T}\cdot \vec{x}}}</script><p>sigmoid函数是非线性函数，值域是(0,1)。函数图像如下图所示</p>
<p><img src="/2021/06/01/deep-learning-tutorial/sigmoid.jpg" alt></p>
<p>sigmoid函数的导数是：</p>
<script type="math/tex; mode=display">
令y=sigmoid(x)\\
y=\frac{1}{1+e^{-x}} \\
u(x)=1+e^{-x} \\
g(x)=e^{-x}\\
k(x)=-x\\
\frac{\mathrm{d} y}{\mathrm{d} x} = \frac{\mathrm{d} y}{\mathrm{d} u}\cdot \frac{\mathrm{d} u}{\mathrm{d} g}\cdot \frac{\mathrm{d} g}{\mathrm{d} k}\cdot \frac{\mathrm{d} k}{\mathrm{d} x}=-1\cdot u^{-2}\cdot 1\cdot e^{-x}\cdot -1=u^{-2}\cdot e^{-x}=\frac{e^{-x}}{(1+e^{-x})^{2}}=\frac{1}{1+e^{-x}}-\frac{1}{(1+e^{-x})^{2}}=y(1-y)\\
则y^{'}=y(1-y)</script><p>可以看到，sigmoid函数的导数非常有趣，它可以用sigmoid函数自身来表示。这样，一旦计算出sigmoid函数的值，计算它的导数的值就非常方便。</p>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p><img src="/2021/06/01/deep-learning-tutorial/神经网络图2.png" alt></p>
<p>神经网络其实就是按照<strong>一定规则</strong>连接起来的多个<strong>神经元</strong>。上图展示了一个<strong>全连接(full connected, FC)</strong>神经网络，通过观察上面的图，我们可以发现它的规则包括：</p>
<ul>
<li>神经元按照层来布局。最左边的层叫做<strong>输入层</strong>，负责接收输入数据；最右边的层叫<strong>输出层</strong>，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做<strong>隐藏层</strong>，因为它们对于外部来说是不可见的。</li>
<li>同一层的神经元之间没有连接。</li>
<li>第N层的每个神经元和第N-1层的<strong>所有</strong>神经元相连(这就是full connected的含义)，第N-1层神经元的输出就是第N层神经元的输入。</li>
<li>每个连接都有一个<strong>权值</strong>。</li>
</ul>
<p>上面这些规则定义了全连接神经网络的结构。事实上还存在很多其它结构的神经网络，比如卷积神经网络(CNN)、循环神经网络(RNN)，他们都具有不同的连接规则。</p>
<h3 id="计算神经网络输出"><a href="#计算神经网络输出" class="headerlink" title="计算神经网络输出"></a>计算神经网络输出</h3><p>神经网络实际上就是一个输入向量<script type="math/tex">\vec{x}</script>到输出向量<script type="math/tex">\vec{y}</script>的函数，即：</p>
<script type="math/tex; mode=display">
\vec{y}=f_{network}(\vec{x})</script><p>根据输入计算神经网络的输出，需要首先将输入向量<script type="math/tex">\vec{x}</script>的每个元素<script type="math/tex">x_{i}</script>的值赋给神经网络的输入层的对应神经元，然后根据<strong>式1</strong>依次向前计算每一层的每个神经元的值，直到最后一层输出层的所有神经元的值计算完毕。最后，将输出层每个神经元的值串在一起就得到了输出向量<script type="math/tex">\vec{y}</script>。</p>
<p>接下来举一个例子来说明这个过程，我们先给神经网络的每个单元写上编号。</p>
<p><img src="/2021/06/01/deep-learning-tutorial/神经网络过程图.png" alt></p>
<p>如上图，输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的4个节点，编号依次为4、5、6、7；最后输出层的两个节点编号为8、9。因为我们这个神经网络是<strong>全连接</strong>网络，所以可以看到每个节点都和<strong>上一层的所有节点</strong>有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为<script type="math/tex">w_{41},w_{42},w_{43}</script>。那么，我们怎样计算节点4的输出值<script type="math/tex">a_{4}</script>呢？</p>
<p>为了计算节点4的输出值，我们必须先得到其所有上游节点（也就是节点1、2、3）的输出值。节点1、2、3是<strong>输入层</strong>的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点1、2、3的输出值分别是<script type="math/tex">x_{1},x_{2},x_{3}</script>。我们要求<strong>输入向量的维度和输入层神经元个数相同</strong>，而输入向量的某个元素对应到哪个输入节点是可以自由决定的，你偏非要把赋值给节点2也是完全没有问题的，但这样除了把自己弄晕之外，并没有什么价值。</p>
<p>一旦我们有了节点1、2、3的输出值，我们就可以根据<strong>式1</strong>计算节点4的输出值<script type="math/tex">a_{4}</script>：</p>
<script type="math/tex; mode=display">
a_{4}=sigmoid(w_{41}x_{1}+w_{42}x_{2}+w_{43}x_{3}+w_{4b})</script><p>上式的<script type="math/tex">w_{4b}</script>是节点4的<strong>偏置项</strong>，图中没有画出来。而分别为节点1、2、3到节点4连接的权重，在给权重<script type="math/tex">w_{ji}</script>编号时，我们把目标节点<script type="math/tex">j</script>的编号放在前面，把源节点<script type="math/tex">i</script>的编号放在后面。</p>
<p>同样，我们可以继续计算出节点5、6、7的输出值<script type="math/tex">a_{5},a_{6},a_{7}</script>。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值<script type="math/tex">y_{1}</script>：</p>
<script type="math/tex; mode=display">
y_{1}=sigmoid(w_{84}a_{4}+w_{85}a_{5}+w_{86}a_{6}+w_{87}a_{7}+w_{8b})</script><p>同理，我们还可以计算出<script type="math/tex">y_{2}</script>的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量<script type="math/tex">\vec{x}=\begin{bmatrix}
x_{1}\\ 
x_{2}\\ 
x_{3}
\end{bmatrix}</script>时，神经网络的输出向量<script type="math/tex">\vec{y}=\begin{bmatrix}
y_{1}\\ 
y_{2}\\ 
\end{bmatrix}</script>。这里我们也看到，<strong>输出向量的维度和输出层神经元个数相同</strong>。</p>
<h3 id="神经网络的矩阵表示"><a href="#神经网络的矩阵表示" class="headerlink" title="神经网络的矩阵表示"></a>神经网络的矩阵表示</h3><p>神经网络的计算如果用矩阵来表示会很方便（当然逼格也更高），我们先来看看隐藏层的矩阵表示。</p>
<p>首先我们把隐藏层4个节点的计算依次排列出来：</p>
<script type="math/tex; mode=display">
a_{4}=sigmoid(w_{41}x_{1}+w_{42}x_{2}+w_{43}x_{3}+w_{4b})\\
a_{5}=sigmoid(w_{51}x_{1}+w_{52}x_{2}+w_{53}x_{3}+w_{5b})\\
a_{6}=sigmoid(w_{61}x_{1}+w_{62}x_{2}+w_{63}x_{3}+w_{6b})\\
a_{7}=sigmoid(w_{71}x_{1}+w_{72}x_{2}+w_{73}x_{3}+w_{7b})</script><p>接着，定义网络的输入向量<script type="math/tex">\vec{x}</script>和隐藏层每个节点的权重向量<script type="math/tex">\vec{w_{j}}</script>。令</p>
<script type="math/tex; mode=display">
\vec{x}=\begin{bmatrix}
x_{1}\\ 
x_{2}\\ 
x_{3}
\end{bmatrix}\\
\vec{w_{4}}=\begin{bmatrix}
w_{41}, &w_{42},  & w_{43}, & w_{4b}
\end{bmatrix}\\
\vec{w_{5}}=\begin{bmatrix}
w_{51}, &w_{52},  & w_{53}, & w_{5b}
\end{bmatrix}\\
\vec{w_{6}}=\begin{bmatrix}
w_{61}, &w_{62},  & w_{63}, & w_{6b}
\end{bmatrix}\\
\vec{w_{7}}=\begin{bmatrix}
w_{71}, &w_{72},  & w_{73}, & w_{7b}
\end{bmatrix}\\
f=sigmoid</script><p>代入到前面的一组式子，得到：</p>
<script type="math/tex; mode=display">
a_{4}=f(\vec{w_{4}}\cdot \vec{x})\\
a_{5}=f(\vec{w_{5}}\cdot \vec{x})\\
a_{6}=f(\vec{w_{6}}\cdot \vec{x})\\
a_{7}=f(\vec{w_{7}}\cdot \vec{x})\\</script><p>现在，我们把上述计算<script type="math/tex">a_{4},a_{5},a_{6},a_{7}</script>的四个式子写到一个矩阵里面，每个式子作为矩阵的一行，就可以利用矩阵来表示它们的计算了。令</p>
<script type="math/tex; mode=display">
\vec{a}=\begin{bmatrix}
a_{4}\\ 
a_{5}\\ 
a_{6}\\ 
a_{7}
\end{bmatrix}\\
W=\begin{bmatrix}
\vec{w_{4}}\\ 
\vec{w_{5}}\\ 
\vec{w_{6}}\\ 
\vec{w_{7}}
\end{bmatrix}=\begin{bmatrix}
w_{41} &w_{42}  & w_{43} &w_{4b} \\ 
w_{51} &w_{52}  & w_{53} &w_{5b} \\ 
w_{61} &w_{62}  & w_{63} &w_{6b} \\ 
w_{71} &w_{72}  & w_{73} &w_{7b} 
\end{bmatrix}\\
f(\begin{bmatrix}
x_{1}\\ 
x_{2}\\ 
x_{3}\\ 
.\\ 
.\\ 
.
\end{bmatrix})=\begin{bmatrix}
f(x_{1})\\ 
f(x_{2})\\ 
f(x_{3})\\ 
.\\ 
.\\ 
.
\end{bmatrix}</script><p>带入前面的一组式子，得到</p>
<script type="math/tex; mode=display">
\vec{a}=f(W\cdot\vec{x}) \tag2</script><p>在<strong>式2</strong>中，是激活函数，在本例中是sigmoid函数；W是某一层的权重矩阵；<script type="math/tex">\vec{x}</script>是某层的输入向量；<script type="math/tex">\vec{a}</script>是某层的输出向量。<strong>式2</strong>说明神经网络的每一层的作用实际上就是先将输入向量<strong>左乘</strong>一个数组进行线性变换，得到一个新的向量，然后再对这个向量<strong>逐元素</strong>应用一个激活函数。</p>
<p>每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重矩阵分别为<script type="math/tex">W_{1},W_{2},W_{3},W_{4}</script>，每个隐藏层的输出分别是<script type="math/tex">\vec{a_{1}},\vec{a_{2}},\vec{a_{3}}</script>，神经网络的输入为<script type="math/tex">\vec{x}</script>，神经网络的输出为<script type="math/tex">\vec{y}</script>，如下图所示：</p>
<p><img src="/2021/06/01/deep-learning-tutorial/深层神经网络.png" alt></p>
<p>则每一层的输出向量的计算可以表示为：</p>
<script type="math/tex; mode=display">
\vec{a_{1}}=f(W_{1}\cdot \vec{x})\\
\vec{a_{2}}=f(W_{2}\cdot \vec{a_{1}})\\
\vec{a_{3}}=f(W_{3}\cdot \vec{a_{2}})\\
\vec{y}=f(W_{4}\cdot \vec{a_{3}})</script><p>这就是神经网络输出值的计算方法。</p>
<h3 id="神经网络的训练"><a href="#神经网络的训练" class="headerlink" title="神经网络的训练"></a>神经网络的训练</h3><p>现在，我们需要知道一个神经网络的每个连接上的权值是如何得到的。我们可以说神经网络是一个<strong>模型</strong>，那么这些权值就是模型的<strong>参数</strong>，也就是模型要学习的东西。然而，一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为<strong>超参数(Hyper-Parameters)</strong>。</p>
<p>接下来，我们将要介绍神经网络的训练算法：反向传播算法。</p>
<h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p>我们首先直观的介绍反向传播算法，最后再来介绍这个算法的推导。当然读者也可以完全跳过推导部分，因为即使不知道如何推导，也不影响你写出来一个神经网络的训练代码。事实上，现在神经网络成熟的开源实现多如牛毛，除了练手之外，你可能都没有机会需要去写一个神经网络。</p>
<p>我们设神经元的激活函数f为函数sigmoid函数。</p>
<p>我们假设每个训练样本为<script type="math/tex">(\vec{x},\vec{t})</script>，其中向量<script type="math/tex">\vec{x}</script>是训练样本的特征，而<script type="math/tex">\vec{t}</script>是样本的目标值。</p>
<p><img src="/2021/06/01/deep-learning-tutorial/反向传播.png" alt></p>
<p>首先，我们根据上一节介绍的算法，用样本的特征<script type="math/tex">\vec{x}</script>，计算出神经网络中每个隐藏层节点的输出<script type="math/tex">a_{i}</script>，以及输出层每个节点的输出<script type="math/tex">y_{i}</script>。</p>
<p>我们取网络所有输出层节点的误差平方和作为目标函数：</p>
<script type="math/tex; mode=display">
E_{d}=\frac{1}{2}\sum_{i=1}^{m}(t_{i}-y_{i})^{2}</script><p>其中<script type="math/tex">m</script>表示输出节点数目，<script type="math/tex">E_{d}</script>表示样本d的误差。</p>
<p>然后，我们用<strong>随机梯度下降</strong>算法对目标函数进行优化：</p>
<script type="math/tex; mode=display">
w_{ji}\leftarrow w_{ji}-\eta\frac{\partial E_{d}}{\partial w_{ji}}</script><p>随机梯度下降算法也就是需要求出误差对于每个权重的偏导数（也就是梯度），怎么求呢？</p>
<p><img src="/2021/06/01/deep-learning-tutorial/神经网络过程图.png" alt></p>
<p>观察上图，我们发现权重<script type="math/tex">w_{ji}</script>仅能通过影响节点<script type="math/tex">j</script>的输入值影响网络的其它部分，设<script type="math/tex">net_{j}</script>是节点<script type="math/tex">j</script>的<strong>加权输入</strong>，即</p>
<script type="math/tex; mode=display">
net_{j}=\vec{w_{j}}\cdot \vec{x_{j}}=\sum_{i=1}^{n}w_{ji}x_{ji}</script><script type="math/tex; mode=display">E_{d}$$是$$net_{j}$$的函数，而$$net_{j}$$是$$w_{ji}$$的函数。根据链式求导法则，可以得到：</script><p>\frac{\partial E_{d}}{\partial w_{ji}}=\frac{\partial E_{d}}{\partial net_{j}}\frac{\partial net_{j}}{\partial w_{ji}}=\frac{\partial E_{d}}{\partial net_{j}}\frac{\partial \sum _{i}w_{ji}x_{ji}}{\partial w_{ji}}=\frac{\partial E_{d}}{\partial net_{j}}x_{ji}</p>
<script type="math/tex; mode=display">
上式中，$$x_{ji}$$是节点$$i$$传递给节点$$j$$的输入值，也就是节点$$i$$的输出值。

对于$$\frac{\partial E_{d}}{\partial net_{j}}$$的推导，需要区分**输出层**和**隐藏层**两种情况。

#### 输出层权值训练

对于**输出层**来说，$$net_{j}$$仅能通过节点$$j$$的输出值$$y_{j}$$来影响网络其它部分，也就是说$$E_{d}$$是$$y_{j}$$的函数，而$$y_{j}$$是$$net_{j}$$的函数，其中$$y_{j}=sigmoid(net_{j})$$。所以我们可以再次使用链式求导法则：</script><p>\frac{\partial E_{d}}{\partial net_{j}}=\frac{\partial E_{d}}{\partial y_{j}}\frac{\partial y_{j}}{\partial net_{j}}</p>
<script type="math/tex; mode=display">
考虑上式第一项:</script><p>\frac{\partial E_{d}}{\partial y_{j}}=\frac{\partial \frac{1}{2}\sum (t_{j}-y_{j})^{2}}{\partial y_{j}}=-(t_{j}-y_{j})</p>
<script type="math/tex; mode=display">
考虑上式第二项：</script><p>\frac{\partial y_{j}}{\partial net_{j}}=\frac{\partial sigmoid(net_{j})}{\partial net_{j}}=y_{j}(1-y_{j})</p>
<script type="math/tex; mode=display">
将第一项和第二项带入，得到：</script><p>\frac{\partial E_{d}}{\partial net_{j}}=-(t_{j}-y_{j})y_{j}(1-y_{j})</p>
<script type="math/tex; mode=display">
如果令$$\delta _{j}=-\frac{\partial E_{d}}{\partial net_{j}}$$，也就是一个节点的误差项是网络误差$$\delta$$对这个节点输入的偏导数的相反数。带入上式，得到：</script><p>\delta _{j}=(t_{j}-y_{j})y_{j}(1-y_{j})</p>
<script type="math/tex; mode=display">
将上述推导带入随机梯度下降公式，得到：</script><p>w_{ji}\leftarrow w_{ji}-\eta \frac{\partial E_{d}}{\partial w_{ji}}=w_{ji}-\eta \frac{\partial E_{d}}{\partial net_{ji}}\frac{\partial net_{ji}}{\partial w_{ji}}=w_{ji}+\eta (t_{j}-y_{j})y_{j}(1-y_{j})(1-y_{j})x_{ji}=w_{ji}+\eta \delta _{j}x_{ji}</p>
<script type="math/tex; mode=display">

#### 隐藏层权值训练

现在我们要推导出隐藏层的$$\frac{\partial E_{d}}{\partial net_{j}}$$。

首先，我们需要定义节点$$j$$的所有直接下游节点的集合$$Downstream(j)$$。例如，对于节点4来说，它的直接下游节点是节点8、节点9。可以看到$$net_{j}$$只能通过影响$$Downstream(j)$$再影响$$E_{d}$$。设$$net_{k}$$是节点$$j$$的下游节点的输入，则$$E_{d}$$是$$net_{k}$$的函数，而$$net_{k}$$是$$net_{j}$$的函数。因为$$net_{k}$$有多个，我们应用全导数公式，可以做出如下推导：</script><p>\frac{\partial E_{d}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}\frac{\partial E_{d}}{\partial net_{k}}\frac{\partial net_{k}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}-\delta _{k}\frac{\partial net_{k}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}-\delta _{k}\frac{\partial net_{k}}{\partial a_{j}}\frac{\partial a_{j}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}-\delta _{k}w_{kj}\frac{\partial a_{j}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}-\delta _{k}w_{kj}a_{j}(1-a_{j})\\<br>=-a_{j}(1-a_{j})\sum_{k\in Downstream(j)}\delta _{k}w_{kj}</p>
<script type="math/tex; mode=display">
因为$$\delta _{j}=-\frac{\partial E_{d}}{\partial net_{j}}$$带入上式得到：</script><p>\delta _{j}=a_{j}(1-a_{j})\sum_{k\in Downstream(j)}\delta _{k}w_{kj}</p>
<script type="math/tex; mode=display">
其中$$a_{j}$$为激活函数。

## 卷积神经网络

### 全连接网络VS卷积网络

全连接神经网络之所以不太适合图像识别任务，主要有以下几个方面的问题：

- **参数数量太多** 考虑一个输入$$1000*1000$$像素的图片(一百万像素，现在已经不能算大图了)，输入层有$$1000*1000=100$$万节点。假设第一个隐藏层有100个节点(这个数量并不多)，那么仅这一层就有$$(1000*1000+1)*100=1$$亿​参数，这实在是太多了！我们看到图像只扩大一点，参数数量就会多很多，因此它的扩展性很差。
- **没有利用像素之间的位置信息** 对于图像识别任务来说，每个像素和其周围像素的联系是比较紧密的，和离得很远的像素的联系可能就很小了。如果一个神经元和上一层所有神经元相连，那么就相当于对于一个像素来说，把图像的所有像素都等同看待，这不符合前面的假设。当我们完成每个连接权重的学习之后，最终可能会发现，有大量的权重，它们的值都是很小的(也就是这些连接其实无关紧要)。努力学习大量并不重要的权重，这样的学习必将是非常低效的。
- **网络层数限制** 我们知道网络层数越多其表达能力越强，但是通过梯度下降方法训练深度全连接神经网络很困难，因为全连接神经网络的梯度很难传递超过3层。因此，我们不可能得到一个很深的全连接神经网络，也就限制了它的能力。

那么，卷积神经网络又是怎样解决这个问题的呢？主要有三个思路：

- **局部连接** 这个是最容易想到的，每个神经元不再和上一层的所有神经元相连，而只和一小部分神经元相连。这样就减少了很多参数。
- **权值共享** 一组连接可以共享同一个权重，而不是每个连接有一个不同的权重，这样又减少了很多参数。
- **下采样** 可以使用Pooling来减少每层的样本数，进一步减少参数数量，同时还可以提升模型的鲁棒性。

对于图像识别任务来说，卷积神经网络通过尽可能保留重要的参数，去掉大量不重要的参数，来达到更好的学习效果。

### 卷积神经网络结构

![](./deep-learning-tutorial/卷积神经网络案例图.png)

三维的层结构

从**图中**我们可以发现**卷积神经网络**的层结构和**全连接神经网络**的层结构有很大不同。**全连接神经网络**每层的神经元是按照**一维**排列的，也就是排成一条线的样子；而**卷积神经网络**每层的神经元是按照**三维**排列的，也就是排成一个长方体的样子，有**宽度**、**高度**和**深度**。

对于**图中**展示的神经网络，我们看到输入层的宽度和高度对应于输入图像的宽度和高度，而它的深度为1。接着，第一个卷积层对这幅图像进行了卷积操作(后面我们会讲如何计算卷积)，得到了三个Feature Map。这里的"3"可能是让很多初学者迷惑的地方，实际上，就是这个卷积层包含三个Filter，也就是三套参数，每个Filter都可以把原始输入图像卷积得到一个Feature Map，三个Filter就可以得到三个Feature Map。至于一个卷积层可以有多少个Filter，那是可以自由设定的。也就是说，卷积层的Filter个数也是一个**超参数**。我们可以把Feature Map可以看做是通过卷积变换提取到的图像特征，三个Filter就对原始图像提取出三组不同的特征，也就是得到了三个Feature Map，也称做三个**通道(channel)**。

继续观察**图**，在第一个卷积层之后，Pooling层对三个Feature Map做了**下采样**(后面我们会讲如何计算下采样)，得到了三个更小的Feature Map。接着，是第二个**卷积层**，它有5个Filter。每个Fitler都把前面**下采样**之后的**3个\**Feature Map**卷积**在一起，得到一个新的Feature Map。这样，5个Filter就得到了5个Feature Map。接着，是第二个Pooling，继续对5个Feature Map进行**下采样**，得到了5个更小的Feature Map。

如图所示网络的最后两层是全连接层。第一个全连接层的每个神经元，和上一层5个Feature Map中的每个神经元相连，第二个全连接层(也就是输出层)的每个神经元，则和第一个全连接层的每个神经元相连，这样得到了整个网络的输出。

至此，我们对**卷积神经网络**有了最基本的感性认识。接下来，我们将介绍**卷积神经网络**中各种层的计算和训练。

### 卷积神经网络输出值的计算

#### 卷积层输出值的计算

我们用一个简单的例子来讲述如何计算**卷积**，然后，我们抽象出**卷积层**的一些重要概念和计算方法。

假设有一个5*5的图像，使用一个3*3的filter进行卷积，想得到一个3*3的Feature Map，如下所示：

![](./deep-learning-tutorial/卷积神经网络案例.png)

为了清楚的描述**卷积**计算过程，我们首先对图像的每个像素进行编号，用$$x_{i,j}$$表示图像的第行第列元素；对filter的每个权重进行编号，用$$w_{m,n}$$表示第$$m$$行第$$n$$列权重，用$$w_{b}$$表示filter的**偏置项**；对Feature Map的每个元素进行编号，用$$a_{i,j}$$表示Feature Map的第$$i$$行第$$j$$列元素；用$$f$$表示**激活函数**(这个例子选择**relu函数**作为激活函数)。然后，使用下列公式计算卷积：</script><p>a_{i,j}=f(\sum_{m=0}^{2}\sum_{n=0}^{2}w_{m,n}x_{i+m,j+n}+w_{b})</p>
<script type="math/tex; mode=display">
例如，对于Feature Map左上角元素来说，其卷积计算方法为：</script><p>a_{0,0}=f(\sum_{m=0}^{2}\sum_{n=0}^{2}w_{m,n}x_{m+0,n+0}+w_{b})\\<br>=relu(w_{0,0}x_{0,0}+w_{0,1}x_{0,1}+w_{0,2}x_{0,2}+w_{1,0}x_{1,0}+w_{1,1}x_{1,1}+w_{1,2}x_{1,2}+w_{2,0}x_{2,0}+w_{2,1}x_{2,1}+w_{2})\\<br>=relu(1+0+1+0+1+0+0+0+1+0)=relu(4)=4</p>
<script type="math/tex; mode=display">
计算结果如下图所示：

![](./deep-learning-tutorial/卷积过程.png)

###  权值共享

神经元的偏置部分也是同一种滤波器共享的。 比如卷积核是三层，每一层使用的偏置项都是相等的。

## 对抗生成网络(GAN)



## 优化器

### Batch Gradient Descent(BGD，批量梯度下降)

BGD训练过程中每次迭代使用所有样本来进行梯度的更新。

#### 优点

- 一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行；
- 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。

#### 缺点

- 当样本数目很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。

### Stochastic Gradient Descent(SGD,随机梯度下降)

SGD训练过程中每次迭代使用一个样本来对参数进行更新。

#### 优点

- 由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。

#### 缺点

- 准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛；
- 可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势；
- 不易于并行实现。

### Mini-Batch Gradient Descent(MBGD,小批量梯度下降)

MBGD训练过程中每次迭代使用**batch size**个样本来对参数进行更新。它是对BGD以及SGD的一个折中办法。

#### 优点

- 通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多；
- 每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果；
- 可实现并行化。

#### 缺点

- batch size的不当选择可能会带来一些问题。

  batcha size的选择带来的影响：

  - 在合理的范围内，增大batch_size的好处：

    - 内存利用率提高了，大矩阵乘法的并行化效率提高。

    - 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。

    - 在一定范围内，一般来说 batch size 越大，其确定的下降方向越准，引起训练震荡越小。

  - 盲目增大batch size的坏处

    - 内存利用率提高了，但是内存容量可能撑不住了。
    - 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。
    - batch size 增大到一定程度，其确定的下降方向已经基本不再变化



### Momentum

[]: https://blog.csdn.net/willduan1/article/details/78070086

SGD参数更新公式为：</script><p>W:=W-\alpha d_{w}\\<br>b:=b-\alpha d_{b}</p>
<script type="math/tex; mode=display">
它的梯度更新路线上下波动很大，收敛速度很慢。因此根据这些原因，有人提出了Momentum优化算法，这个是基于SGD的，简单理解就是为了防止波动，它主要是基于梯度的移动指数加权平均来更新参数。引进超参数beta(一般取0.9)。

在讲这个算法之前说一下移动指数加权平均。移动指数加权平均法加权就是根据同一个移动段内不同时间的数据对预测值的影响程度，分别给予不同的权数，然后再进行平均移动以预测未来值。假定给定一系列数据值$$x_{1}, x_{2},x_{3},……x_{n}$$。那么，我们根据这些数据来拟合一条曲线，所得的值$$v_{1}, v_{2}…..$$就是如下的公式：</script><p>v_{0} = 0\\<br>v_{1} = \beta v_{0}+(1-\beta )x_{0} \\<br>v_{2} = \beta v_{1}+(1-\beta )x_{1} \\</p>
<script type="math/tex; mode=display">
参数更新公式为：</script><p>V_{dw}=\beta V_{dw}+(1-\beta )dW\\<br>V_{db}=\beta V_{db}+(1-\beta )db\\<br>W:=W-\alpha {V_{dw}}\\<br>b:=b-\alpha {V_{db}}</p>
<script type="math/tex; mode=display">

### Nesterov Momentum

Nesterov Momentum是对Momentum的改进，可以理解为nesterov动量在标准动量方法中添加了一个**校正因子**。

### Root Mean Square Prop(RMSProp)

RMSProp思想与Momentum相似，也用到权重超参数beta（一般取0.999）。

参数更新公式为：</script><p>S_{dw}=\beta S_{dw}+(1-\beta )dW^{2}\\<br>S_{db}=\beta S_{db}+(1-\beta )db^{2}\\<br>W:=W-\alpha \frac{dW}{\sqrt{S_{dw}}}\\<br>b:=b-\alpha \frac{db}{\sqrt{S_{db}}}</p>
<script type="math/tex; mode=display">
为了防止分母为0，在分数下加上个特别小的值epsilon，通常选取10^-8。

### Adagrad

大多数优化器训练参数更新过程中都使用了相同的学习率α。Adagrad能够在训练中自动的对learning rate进行调整，对于出现频率较低的参数采用较大的α更新，相反，对于出现频率较高的参数采用较小的α更新。因此，**Adagrad非常适合处理稀疏数据**。

如果是普通的SGD，那么每一时刻梯度的更新公式为：</script><p>\Theta _{t+1}=\Theta _{t,i}-\alpha *g_{t,i}</p>
<script type="math/tex; mode=display">
g<sub>t,i</sub>为第t轮第i个参数的梯度。θ<sub>t,i</sub> 为参数值

Adagrad在每轮训练中对每个参数θ<sub>i</sub> 进行更新，参数更新公式为：</script><p>\Theta _{t+1,i}=\Theta _{t,i}-\frac{\alpha }{\sqrt{G_{t,ii}+\varepsilon }}*g_{t,i}</p>
<script type="math/tex; mode=display">
G<sub>t</sub>为对角矩阵，大小为D*D。每个对角线位置i,i为对应参数θ<sub>i</sub> 从第一轮到第t轮梯度的平方和。varepsilon 是平滑项，用于避免分母为0，一般取值为10^-8。Adagrad的缺点是在训练的中后期，分母上梯度平方的累加会越来越大，从而梯度趋近于0，使得训练提前结束。

### Adadelta

Adadelta 是 Adagrad 的一个具有更强鲁棒性的的扩展版本，它不是累积所有过去的梯度，而是根据渐变更新的移动窗口调整学习速率。 这样，即使进行了许多更新，Adadelta 仍在继续学习。   

与Adagrad 相比，就是分母的 G 换成了过去的梯度平方的衰减平均值，**指数衰减平均值**。

这个分母相当于**梯度的均方根 root mean squared (RMS)**，在数据统计分析中，将所有值平方求和，求其均值，再开平方，就得到均方根值 。

#### 优点

- 防止学习率衰减或梯度消失等问题的出现。

### Adam

该优化器相当于RMSprop+Momentum。

参数更新公式为：</script><p>V_{dw}=\beta _{1} V_{dw}+(1-\beta _{1} )dW\\<br>V_{db}=\beta _{1} V_{db}+(1-\beta _{1} )db\\<br>S_{dw}=\beta_{2} S_{dw}+(1-\beta_{2} )dW^{2}\\<br>S_{db}=\beta_{2} S_{db}+(1-\beta_{2} )db^{2}\\<br>V_{dW}^{corrected}=\frac{V_{dW}}{1-\beta _{1}^{t}}\\<br>V_{db}^{corrected}=\frac{V_{db}}{1-\beta _{1}^{t}}\\<br>S_{dW}^{corrected}=\frac{S_{dW}}{1-\beta _{2}^{t}}\\<br>S_{db}^{corrected}=\frac{S_{db}}{1-\beta _{2}^{t}}\\<br>W:=W-\alpha \frac{V_{dW}}{\sqrt{S_{dW}^{corrected}}}\\<br>b:=b-\alpha \frac{V_{db}}{\sqrt{S_{db}^{corrected}}}</p>
<script type="math/tex; mode=display">
beta1一般为0.9，beta2一般为0.9999。在实际应用中，Adam方法效果良好。与其他自适应学习率算法相比，其收敛速度更快，学习效果更为有效，而且可以纠正其他优化技术中存在的问题，如学习率消失、收敛过慢或是高方差的参数更新导致损失函数波动较大等问题。

## 激活函数

[激活函数详解](<https://zhuanlan.zhihu.com/p/22142013>)

激活函数一般用于神经网络的层与层之间，将上一层的输出转换之后输入到下一层。如果没有激活函数引入的额非线性特性，那么神经网络就只相当于原始感知机的矩阵相乘。  

### Sigmoid

![](./deep-learning-tutorial/sigmoid.jpg)

sigmoid在定义域内处处可导，且两侧导数逐渐趋近于0。

#### 缺点

- 激活函数计算量大，反向传播求误差梯度时，求导涉及除法；
- 反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练；
- Sigmoids函数饱和且kill掉梯度；
- Sigmoids函数收敛缓慢。

### tanh

![](./deep-learning-tutorial/tanh.jpg)

### Relu

![](./deep-learning-tutorial/relu.jpg)

#### 优点

- **速度快** 和sigmoid函数需要计算指数和倒数相比，relu函数其实就是一个max(0,x)，计算代价小很多。
- **减轻梯度消失问题** relu函数的导数是1，不会导致梯度变小。当然，激活函数仅仅是导致梯度减小的一个因素，但无论如何在这方面relu的表现强于sigmoid。使用relu激活函数可以让你训练更深的网络。
- **稀疏性** 通过对大脑的研究发现，大脑在工作的时候只有大约5%的神经元是激活的，而采用sigmoid激活函数的人工神经网络，其激活率大约是50%。有论文声称人工神经网络在15%-30%的激活率时是比较理想的。因为relu函数在输入小于0时是完全不激活的，因此可以获得一个更低的激活率。

### 缺点

- 训练的时候很”脆弱”，很容易就”die”了。

### PRelu

![](./deep-learning-tutorial/PRelu.jpg)

### RReLU

### Maxout

### ELU

![](./deep-learning-tutorial/elu.jpg)



## 问题及解决方法

### 梯度爆炸与梯度消失

深度神经网络训练过程中，使用了反向传播的方式更新参数，该方式基于的是链式求导。计算每层梯度的时候回设计一些连乘操作，如果网络过深，当连乘的因子大部分小于1时，最后的乘积结果可能趋于0，会导致后面的网络层参数不发生变化，不能继续进行学习（梯度消失）。当连乘的因子大部分大于1，最后的乘积结果可能趋于无穷，会导致后面的网络层参数变化过大，导致Loss值出现震荡，收敛不到最低值的情况（梯度爆炸）。

#### 梯度爆炸解决办法

- 降低学习率

- 梯度裁剪（Gradient Clipping）

  如果梯度特别大，那么将其投影到一个比较小的尺度上。

### 线性与非线性

在数学上可理解为一阶导数为常数的函数为线性函数，一阶导数不为常数的函数为非线性函数。

### 为什么RNN一般情况下为等长的

为了让多条数据合并成矩阵进行运算，能够使用并行处理。如果不等长则不能合并为矩阵。tensorflow支持同一批训练数据等长的训练接口。

### Padding 等于SAME和VALID

Padding运算作用于输入向量的每一维，每一维的操作都是一致的，所以理解Padding的操作，只需要理解一维向量的padding过程

假设一个一维向量，输入形状为input_size，经过滤波操作后的输出形状为output_size，滤波窗口为filter_size，需要padding的个数为padding_needed，滤波窗口滑动步长为stride，则之间满足关系：</script><p>output_size=(input_size+padding_needed-filter_size)/stride+1</p>
<p>$$<br>由公式可知，指定padding_needed可以确定output_size的值，反过来，如果已知输出的形状，则进而可以确定padding的数量。</p>
<p>这是两种处理padding的方案，pytorch采用的是第一种，即在卷积或池化时先确定padding数量，自动推导输出形状；tensorflow和caffe采用的是更为人熟知的第二种，即先根据Valid还是Same确定输出大小，再自动确定padding的数量</p>
<p>Valid和Same是预设的两种padding模式，Valid指不padding，same指输出大小尽可能和输入大小成比例</p>
<p>下面是tensorflow计算padding的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">作者：JamesPlur</span><br><span class="line">链接：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;73118626</span><br><span class="line">来源：知乎</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br><span class="line"></span><br><span class="line">void GetWindowedOutputSize(int64_t input_size, int32_t filter_size, int32_t dilation_rate,</span><br><span class="line">                           int32_t stride, const std::string&amp; padding_type, </span><br><span class="line">                           int64_t* output_size,int32_t* padding_before, </span><br><span class="line">                           int32_t* padding_after) &#123;</span><br><span class="line">  CHECK_GT(stride, 0);</span><br><span class="line">  CHECK_GE(dilation_rate, 1);</span><br><span class="line"></span><br><span class="line">  int32_t effective_filter_size &#x3D; (filter_size - 1) * dilation_rate + 1;</span><br><span class="line">  if (padding_type &#x3D;&#x3D; &quot;valid&quot;) &#123;</span><br><span class="line">    if (output_size) &#123; *output_size &#x3D; (input_size - effective_filter_size + stride) &#x2F; stride; &#125;</span><br><span class="line">    if (padding_before) &#123; *padding_before &#x3D; 0; &#125;</span><br><span class="line">    if (padding_after) &#123; *padding_after &#x3D; 0; &#125;</span><br><span class="line">  &#125; else if (padding_type &#x3D;&#x3D; &quot;same&quot;) &#123;</span><br><span class="line">    int64_t tmp_output_size &#x3D; (input_size + stride - 1) &#x2F; stride;</span><br><span class="line">    if (output_size) &#123; *output_size &#x3D; tmp_output_size; &#125;</span><br><span class="line">    const int32_t padding_needed &#x3D; std::max(</span><br><span class="line">        0,</span><br><span class="line">        static_cast&lt;int32_t&gt;((tmp_output_size - 1) * stride + effective_filter_size - input_size));</span><br><span class="line">    &#x2F;&#x2F; For odd values of total padding, add more padding at the &#39;right&#39;</span><br><span class="line">    &#x2F;&#x2F; side of the given dimension.</span><br><span class="line">    if (padding_before) &#123; *padding_before &#x3D; padding_needed &#x2F; 2; &#125;</span><br><span class="line">    if (padding_after) &#123; *padding_after &#x3D; padding_needed - padding_needed &#x2F; 2; &#125;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    UNIMPLEMENTED();</span><br><span class="line">  &#125;</span><br><span class="line">  if (output_size) &#123; CHECK_GE((*output_size), 0); &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h2><h2 id="AlexNet深度卷积神经网络"><a href="#AlexNet深度卷积神经网络" class="headerlink" title="AlexNet深度卷积神经网络"></a>AlexNet深度卷积神经网络</h2><h3 id="深度卷积神经网络图像集分类"><a href="#深度卷积神经网络图像集分类" class="headerlink" title="深度卷积神经网络图像集分类"></a>深度卷积神经网络图像集分类</h3><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>我们训练了一个庞大的深层卷积神经网络，将ImageNet LSVRC-2010比赛中的120万张高分辨率图像分为1000个不同的类别。在测试数据上，我们取得了37.5％和17.0％的前1和前5的错误率，这比以前的先进水平要好得多。具有6000万个参数和650,000个神经元的神经网络由五个卷积层组成，其中一些随后是最大池化层，三个全连接层以及最后的1000个softmax输出。为了加快训练速度，我们使用非饱和神经元和能高效进行卷积运算的GPU实现。为了减少全连接层中的过拟合，我们采用了最近开发的称为“dropout”的正则化方法，该方法证明是非常有效的。我们还在ILSVRC-2012比赛中使用了这种模式的一个变种，取得了15.3％的前五名测试失误率，而第二名的成绩是26.2％。</p>
<h4 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h4><p>目前，机器学习方法对物体识别非常重要。为了改善他们的表现，我们可以收集更大的数据集，训练更强大的模型，并使用更好的技术来防止过拟合。直到最近，标记好图像的数据集相对还较小——大约上万的数量级（例如，NORB [16]，Caltech-101/256 [8,9]和CIFAR-10/100 [12]）。使用这种规模的数据集可以很好地解决简单的识别任务，特别是如果他们增加了保留标签转换（label-preserving transformations）。例如，目前MNIST数字识别任务的最低错误率（&lt;0.3％）基本达到了人类的识别水平[4]。但是物体在现实环境中可能表现出相当大的变化性，所以要学会识别它们，就必须使用更大的训练集。事实上，小图像数据集的缺点已是众所周知（例如，Pinto[21]），但直到最近才可以收集到数百万的标记数据集。新的大型数据集包括LabelMe [23]，其中包含数十万个完全分割的图像，以及ImageNet [6]，其中包含超过15,000万个超过22,000个类别的高分辨率图像。<br>要从数百万图像中学习数千个类别，我们需要一个具有强大学习能力的模型。然而，物体识别任务的巨大复杂性意味着即使是像ImageNet这样大的数据集也不能完美地解决这个问题，所以我们的模型也需要使用很多先验知识来弥补我们数据集不足的问题。卷积神经网络（CNN）就构成了一类这样的模型[16,11,13,18,15,22,26]。它们的容量可以通过改变它们的深度和宽度来控制，并且它们也对图像的性质（即统计量的定态假设以及像素局部依赖性假设）做出准确而且全面的假设。因此，与具有相同大小的层的标准前馈神经网络相比，CNN具有更少的连接和参数，因此它们更容易训练，而其理论最优性能可能稍微弱一些。<br>尽管CNN具有很好的质量，并且尽管其局部结构的效率相对较高，但将它们大规模应用于高分辨率图像时仍然显得非常昂贵。幸运的是，当前的GPU可以用于高度优化的二维卷积，能够加速许多大型CNN的训练，并且最近的数据集（如ImageNet）包含足够多的标记样本来训练此类模型，而不会出现严重的过度拟合。<br>本文的具体贡献如下：我们在ILSVRC-2010和ILSVRC-2012比赛中使用的ImageNet子集上训练了迄今为止最大的卷积神经网络之一[2]，并在这些数据集上取得了迄今为止最好的结果。我们编写了一个高度优化的2D卷积的GPU实现以及其他训练卷积神经网络的固有操作，并将其公开。我们的网络包含许多新的和不同寻常的功能，这些功能可以提高网络的性能并缩短训练时间，详情请参阅第3节。我们的网络规模较大，即使有120万个带标签的训练样本，仍然存在过拟合的问题，所以我们采用了几个有效的技巧来阻止过拟合，在第4节中有详细的描述。我们最终的网络包含五个卷积层和三个全连接层，并且这个深度似乎很重要：我们发现去除任何卷积层（每个卷积层只包含不超过整个模型参数的1%的参数）都会使网络的性能变差。<br>最后，网络的规模主要受限于目前GPU上可用的内存量以及我们可接受的训练时间。我们的网络需要在两块GTX 580 3GB GPU上花费五到六天的时间来训练。我们所有的实验都表明，通过等待更快的GPU和更大的数据集出现，我们的结果可以进一步完善。</p>
<h4 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2 数据集"></a>2 数据集</h4><p>ImageNet是一个拥有超过1500万个已标记高分辨率图像的数据集，大概有22,000个类别。图像都是从网上收集，并使用Amazon-Mechanical Turk群智工具人工标记。从2010年起，作为Pascal视觉对象挑战赛的一部分，这是每年举办一次的名为ImageNet大型视觉识别挑战赛（ILSVRC）的比赛。 ILSVRC使用的是ImageNet的一个子集，每1000个类别中大约有1000个图像。总共有大约120万张训练图像，50,000张验证图像和150,000张测试图像。<br>ILSVRC-2010是ILSVRC中的唯一可以使用测试集标签的版本，因此这也正是我们进行大部分实验的版本。由于我们也在ILSVRC-2012比赛中引入了我们的模型，因此在第6部分中，我们也会给出此版本数据集的结果，尽管这个版本的测试集标签不可用。在ImageNet上，习惯上使用两种错误率：top-1和top-5，其中top-5错误率是正确标签不在被模型认为最可能的五个标签之中的测试图像的百分率。<br>ImageNet由可变分辨率的图像组成，而我们的系统需要固定的输入尺寸。因此，我们将图像下采样到256×256的固定分辨率。给定一个矩形图像，我们首先重新缩放图像，使得短边长度为256，然后从结果中裁剪出中心的256×256的图片。除了将每个像素中减去训练集的像素均值之外，我们没有以任何其他方式对图像进行预处理。所以我们在像素的（中心）原始RGB值上训练了我们的网络。</p>
<h4 id="3-结构"><a href="#3-结构" class="headerlink" title="3 结构"></a>3 结构</h4><p>图2概括了我们所提出网络的结构。它包含八个学习层——五个卷积层和三个全连接层。下面，我们将描述一些所提出网络框架中新颖或不寻常的地方。 3.1-3.4节按照我们对它们重要性的估计进行排序，其中最重要的是第一个。</p>
<h5 id="3-1-ReLU非线性单元"><a href="#3-1-ReLU非线性单元" class="headerlink" title="3.1 ReLU非线性单元"></a>3.1 ReLU非线性单元</h5><p>对一个神经元模型的输出的常规套路是，给他接上一个激活函数：<script type="math/tex">f(x)=tanh(x)</script>或者<script type="math/tex">f(x)=(1+e^{-x})^{-1}</script>。就梯度下降法的训练时间而言，这些饱和非线性函数比非饱和非线性函数如<script type="math/tex">f(x)=max(0,x)</script>慢得多。根据Nair和Hinton的说法[20]，我们将这种非线性单元称为——修正非线性单元（Rectified Linear Units (ReLUs)）。使用ReLUs做为激活函数的卷积神经网络比起使用tanh单元作为激活函数的训练起来快了好几倍。这个结果从图1中可以看出来，该图展示了对于一个特定的四层CNN，CIFAR-10数据集训练中的误差率达到25%所需要的迭代次数。从这张图的结果可以看出，如果我们使用传统的饱和神经元模型来训练CNN，那么我们将无法为这项工作训练如此大型的神经网络。</p>
<p><strong>饱和激活函数会压缩输入值</strong>：如<script type="math/tex">f(x)=max(0,x)</script>，当<script type="math/tex">x</script>趋于正无穷则<script type="math/tex">f(x)</script>也趋于正无穷，所以该函数是非饱和的，<script type="math/tex">sigmoid</script>函数的范围是<script type="math/tex">[0,1]</script>所以是饱和的，<script type="math/tex">tanh</script>函数也是饱和的，因为其取值范围为<script type="math/tex">[-1,1]</script>。</p>
<h2 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h2><p><a target="_blank" rel="noopener" href="https://github.com/DX2048/text_classification">文本分类模型实现</a></p>
<h3 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32965521">参考</a></p>
<h3 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h3><h3 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h3><h3 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h3><h3 id="Hierarchical-Attention-Network"><a href="#Hierarchical-Attention-Network" class="headerlink" title="Hierarchical Attention Network"></a>Hierarchical Attention Network</h3><h3 id="Seq2seq-With-Attention"><a href="#Seq2seq-With-Attention" class="headerlink" title="Seq2seq With Attention"></a>Seq2seq With Attention</h3><h3 id="Dynamic-Memory-Network"><a href="#Dynamic-Memory-Network" class="headerlink" title="Dynamic Memory Network"></a>Dynamic Memory Network</h3><h3 id="EntityNetwork-tracking-state-of-the-world"><a href="#EntityNetwork-tracking-state-of-the-world" class="headerlink" title="EntityNetwork:tracking state of the world"></a>EntityNetwork:tracking state of the world</h3><h3 id="Ensemble-models"><a href="#Ensemble-models" class="headerlink" title="Ensemble models"></a>Ensemble models</h3><h3 id="Transformer-“Attend-Is-All-You-Need”"><a href="#Transformer-“Attend-Is-All-You-Need”" class="headerlink" title="Transformer(“Attend Is All You Need”)"></a>Transformer(“Attend Is All You Need”)</h3><h2 id="如何选择优化算法"><a href="#如何选择优化算法" class="headerlink" title="如何选择优化算法"></a>如何选择优化算法</h2><p>如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。</p>
<p>RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。</p>
<p>Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum。</p>
<p>随着梯度变的稀疏，Adam 比 RMSprop 效果会好。</p>
<p>整体来讲，<strong>Adam 是最好的选择</strong>。</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>[零基础入门深度学习](</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/01/Reinforcement-Learning-with-Deep-Energy-Based-Policies/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/01/Reinforcement-Learning-with-Deep-Energy-Based-Policies/" class="post-title-link" itemprop="url">Reinforcement Learning with Deep Energy-Based Policies</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 22:16:57" itemprop="dateCreated datePublished" datetime="2021-06-01T22:16:57+08:00">2021-06-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-24 21:10:08" itemprop="dateModified" datetime="2021-06-24T21:10:08+08:00">2021-06-24</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    我们提出了一种用于学习连续状态和动作的基于表达能量的策略的方法，该方法之前仅在表格中可行。 我们将我们的方法应用于学习最大熵策略，从而产生一种称为soft Q-learning的新算法，该算法通过 Boltzmann 分布表达最优策略。 我们使用最近提出的amortized Stein variational gradient descent来学习随机采样网络，该网络从该分布中近似样本。 所提出算法的好处包括改进的探索和组合性，允许在任务之间转移技能，我们在游泳和步行机器人的模拟实验中证实了这一点。 我们还与actor-critic方法建立了联系，可以将其视为对相应的基于能量的模型执行近似推理。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    深度强化学习 (deep RL) 已成为自动获取复杂行为的一个有前途的方向 (Mnih et al., 2015; Silver et al., 2016)，因为它能够处理复杂的感官输入 (Jaderberg et al., 2016)并使用通用神经网络表示获得精细的行为技能 (Levine  et al., 2016)。深度强化学习方法可用于优化确定性 (Lillicrap et al., 2015)  和随机性 (Schulman et al., 2015a; Mnih et al., 2016)  策略。然而，大多数深度强化学习方法都基于传统的最优性确定性概念，其中最优解，至少在完全可观察性下，始终是确定性策略 (Sutton &amp; Barto, 1998)。虽然随机策略对于探索是可取的，但这种探索通常是通过启发式实现的，例如通过注入噪声 (Silver et al., 2014; Lillicrap et al., 2015; Mnih et al., 2015)或初始化具有高熵的随机策略 (Kakade, 2002; Schulman et al., 2015a; Mnih et al., 2016)。</p>
<p>​    在某些情况下，我们实际上可能更喜欢学习随机行为。 在本文中，我们探索了两个潜在的原因：在存在多模态目标的情况下进行探索，以及通过预训练获得的组合性。 其他好处包括面对不确定动态时的鲁棒性 (Ziebart, 2010)、模仿学习 (Ziebart et al., 2008)以及改进的收敛性和计算特性(Gu et al., 2016a)。 如  (Daniel et al., 2012) 所示，多模态也可应用于实际机器人任务。 然而，为了学习这样的策略，我们必须定义一个促进随机性的目标。</p>
<p>​    在哪些情况下，随机策略实际上是最佳解决方案？正如在之前的工作中所讨论的，当我们考虑最优控制和概率推理之间的联系时，随机策略会作为最佳答案出现(Todorov, 2008)。虽然该框架有多个实例，但它们通常包括成本或奖励函数作为因子图中的附加因子，并推断以状态为条件的动作的最佳条件分布。该解决方案可用于优化熵增强强化学习目标或对应于最大熵学习问题的解决方案 (Toussaint, 2009)。直观地说，作为推理的框架控制产生的策略不仅旨在捕获具有最低成本的单个确定性行为，而且还捕获整个范围的低成本行为，明确地最大化相应策略的熵。生成的策略不是学习执行任务的最佳方式，而是尝试学习执行任务的所有方式。现在应该很明显为什么首选此类策略：如果我们可以学习执行给定任务的所有方式，则生成的策略可以作为微调到更具体行为的良好初始化（例如，首先学习所有机器人前进的方式，然后以此作为初始化来学习单独的跑步和跳跃技能）；在多模式奖励环境中寻找最佳模式的更好的探索机制；以及面对对抗性扰动时更稳健的行为，其中以多种不同方式执行相同任务的能力可以为代理提供更多选择来从扰动中恢复。</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1F5411W7fN/">视频讲解</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/21/Deterministic-Policy-Gradient-Algorithms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/21/Deterministic-Policy-Gradient-Algorithms/" class="post-title-link" itemprop="url">Deterministic Policy Gradient Algorithms</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-21 19:31:21" itemprop="dateCreated datePublished" datetime="2021-05-21T19:31:21+08:00">2021-05-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-05-22 11:36:08" itemprop="dateModified" datetime="2021-05-22T11:36:08+08:00">2021-05-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    在本文中，我们考虑了确定性策略梯度算法，用于连续动作的强化学习。 确定性策略梯度具有特别吸引人的形式：它是动作值函数的期望梯度。 这种简单的形式意味着可以比通常的随机策略梯度更有效地估计确定性策略梯度。 为了确保进行充分的探索，我们引入了一种off-policy的行为者批评算法，该算法从探索行为策略中学习确定性目标策略。 我们证明，确定性策略梯度算法在高维操作空间中可以明显胜过其随机对应算法。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    策略梯度算法广泛应用于连续动作空间的强化学习问题。基本思想是用参数化的概率分布<script type="math/tex">\pi _{\theta }(a|s)=\mathbb{P}\left [ a|s;\theta  \right ]</script>，根据参数向量<script type="math/tex">\theta</script>随机选择状态中的动作a来表示策略。策略梯度算法通常通过对这个随机策略进行采样，并调整策略参数，以获得更大的累积奖励。</p>
<p>​    在本文中，我们将考虑确定性策略<script type="math/tex">a=\mu _{\theta }\left ( s \right )</script>。很自然会想，是否可以与随机策略遵循同样的方法：按照策略梯度的方向调整策略参数。以前认为确定性策略梯度不存在，或者只能在使用模型时获得。然而，我们证明了确定性策略梯度确实存在，而且它有一个简单的model-free形式，简单地遵循action-value函数的梯度。另外我们表明，由于策略方差趋于零，因此确定性策略梯度是随机策略梯度的极限情况。</p>
<p>​    从实际的角度来看，随机策略梯度和确定性策略梯度之间有一个关键的区别。在随机情况下，策略梯度同时在状态空间和动作空间上集成，而在确定性情况下，它只在状态空间上集成。因此，计算随机策略梯度可能需要更多的样本，特别是当动作空间有很多维数时。</p>
<p>​    为了探索完整的状态和动作空间，通常需要一个随机策略。为了确保我们的确定性策略梯度算法继续令人满意地探索，我们引入了一种off-policy学习算法。基本思想是根据随机行为策略选择行动（确保充分的探索)，但了解确定性目标策略(利用确定性策略梯度的效率）。我们使用确定性策略梯度推导出一个off-policy actor-critic算法，该算法使用可微函数近似器估计动作值函数，然后沿近似action-value梯度的方向更新策略参数。我们还引入了一个确定性策略梯度的compatiable function近似的概念，以确保该近似不会对策略梯度造成偏差。</p>
<p>我们将确定性行动者批评算法应用于以下几个基准问题：高维bandit； 低维动作空间的几个标准基准强化学习任务； 和控制章鱼手臂的高维任务。 我们的结果表明，相对于随机策略梯度，使用确定性策略梯度具有明显的性能优势，特别是在高维任务中。 此外，我们的算法比以前的方法不需要更多的计算：每次更新的计算成本在操作维度和策略参数数量上都是线性的。 最后，在许多应用程序中（例如在机器人技术中）提供了可微分的控制策略，但没有将噪声注入控制器的功能。 在这些情况下，随机策略梯度是不适用的，而我们的方法可能仍然有用。</p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/20/Continuous-Control-With-Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/20/Continuous-Control-With-Deep-Reinforcement-Learning/" class="post-title-link" itemprop="url">Continuous Control With Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-20 22:21:20" itemprop="dateCreated datePublished" datetime="2021-05-20T22:21:20+08:00">2021-05-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-18 00:44:50" itemprop="dateModified" datetime="2021-06-18T00:44:50+08:00">2021-06-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CONTINUOUS-CONTROL-WITH-DEEP-REINFORCEMENT-LEARNING"><a href="#CONTINUOUS-CONTROL-WITH-DEEP-REINFORCEMENT-LEARNING" class="headerlink" title="CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING"></a>CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们将Deep Q-Learning成功的背后思想应用到连续的行动领域。我们提出了一个actor-critic,model-free基于确定性策略梯度的算法，可以在连续动作空间上运行。使用相同的学习算法、网络架构和超参数，我们的算法稳健地解决了20多个模拟物理任务，包括卡杆摆动、灵巧操作、腿运动和汽车驾驶等经典问题。我们的算法能够找到性能与完全访问动态系统的规划算法及其衍生物相当的策略。我们进一步证明，对于许多任务，该算法可以直接通过输入原始像素学习“end-to-end”策略。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    人工智能领域的主要目标之一是从未处理的、高维的、感官的输入中解决复杂的任务。最近，通过将深度学习中的感官处理与强化学习相结合，取得了重大进展，从而形成了DQN算法 。该算法能够在许多Atari电子游戏中，使用未经处理的像素作为输入，达到了人类水平的性能。 为此，使用了深度神经网络函数逼近器来估算动作值函数。</p>
<p>​    但是，尽管DQN解决了高维观测空间的问题，但它只能处理离散和低维动作空间。 许多有趣的任务，尤其是物理控制任务，具有连续的（实际值）和高维度的动作空间。 DQN不能直接应用于连续域，因为它依赖于找到可以最大程度地提高动作值函数，在连续值情况下，每个步骤都需要迭代优化过程。</p>
<p>​    使诸如DQN之类的深度强化学习方法适应连续领域的一种明显方法是简单地离散动作空间。但是，这有很多局限性，最显着的是维度灾难：动作的数量随自由度的增加而呈指数增长。例如，对于每个关节具有最粗糙的离散<script type="math/tex">a_{i}\in\left \{ -k,0,k \right \}</script>的,关节数量为7的系统（如在人类手臂中）会导致具有以下维度的动作空间：<script type="math/tex">3^7=2187</script> 。对于需要对动作进行精细控制的任务，情况甚至更糟，因为它们需要相应地更精细的离散化，从而导致离散动作数量激增。 如此大的动作空间很难有效地探索，因此在这种情况下成功地训练类似DQN的网络可能很棘手。 此外，动作空间的单纯离散化会不必要地丢弃有关动作域结构的信息，这对于解决许多问题可能是必不可少的。</p>
<p>​    在这项工作中，我们提出了一种使用深度函数逼近器的model-free，off-policy的actor-critic算法，该算法可以学习高维，连续动作空间中的策略。我们的工作基于确定性策略梯度（DPG）算法（其自身类似于NFQCA，并且可以找到类似的想法）。 然而，正如我们在下面显示的那样，这种针对行为者的方法与神经函数近似器的单纯应用对于具有挑战性的问题是不稳定的。</p>
<p>​    在这里，我们结合了行动者批评方法和最近从Deep Q Network（DQN）成功获得的见解。 在DQN之前，通常认为使用大型非线性函数逼近器学习价值函数既困难又不稳定。 由于以下两项创新，DQN能够使用此类函数逼近器以稳定且健壮的方式学习价值函数：1.通过从重播缓冲区中抽取样本，对网络进行off-policy训练，以最大程度地减少样本之间的相关性； 2.用目标Q网络训练网络，以在时间差备份期间给出一致的目标。 在这项工作中，我们利用了相同的思想以及批处理规范化，这是深度学习的最新进展。</p>
<p>​    为了评估我们的方法，我们构造了各种具有挑战性的物理控制问题涉及复杂的多关节运动，不稳定和丰富的接触动力学以及步态行为。其中包括经典的问题，例如cartpole swing-up问题以及许多新领域。 机器人控制的长期挑战是直接从原始的感官输入（例如视频）中学习动作策略。 因此，我们将固定的视点相机放置在模拟器中，并尝试使用低维观测（例如，关节角度）以及直接从像素中进行所有任务。</p>
<p>​    我们的无模型方法，我们称为深度DPG(DDPG)，可以使用低维观测(笛卡尔坐标或关节角)来学习我们所有任务的竞争策略。使用相同的超参数和网络结构。在许多情况下，我们还能够直接从像素中学习好的策略，再次保持超参数和网络结构不变。</p>
<p>​    该方法的一个关键特征是它的简单性：它只需要简单的actor-critic体系结构和学习算法，而很少有“运动部件”，因此易于实现和扩展更困难的问题和更大的网络。 对于物理控制问题，我们将我们的结果与规划器计算的基线进行比较，该规划器可以完全访问基础模拟动力学及其派生函数（请参阅补充信息）。 有趣的是，在某些情况下，即使从像素中学习时，DDPG有时也会找到超出规划器性能的策略（规划器始终在底层的低维状态空间上进行规划）。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>​    我们考虑一个标准的强化学习设置，该设置由一个在离散的时间步中与一个环境<script type="math/tex">E</script>交互的代理组成。 代理在每个时间步长<script type="math/tex">t</script>收到观察值<script type="math/tex">x_{t}</script>，采取行动，并收到标量奖励<script type="math/tex">r_{t}</script>。 在这里考虑的所有环境中，这些动作的实值均为<script type="math/tex">a_{t}\in \mathbb{R}^{N}</script>。 通常，可能会部分观察环境，因此可能需要观察的整个历史记录，动作对<script type="math/tex">s_{t}=(x_{1},a_{1},...,a_{t-1},x_{t})</script>来描述状态。 在这里，我们假设环境是完全观察到的，因此<script type="math/tex">s_{t}=x_{t}</script>。</p>
<p>​    代理的行为由策略<script type="math/tex">\pi</script>定义，该策略将状态映射到行为<script type="math/tex">\pi</script>的概率分布：<script type="math/tex">S\rightarrow P(A)</script>。环境<script type="math/tex">E</script>也可能是随机的。 我们将其建模为马尔可夫模型状态空间为<script type="math/tex">S</script>的决策过程，动作空间<script type="math/tex">A=\mathbb{R}^{N}</script>，初始状态分布为<script type="math/tex">p(s_{1})</script>，转移概率<script type="math/tex">p(s_{t+1}|s_{t},a_{t})</script>和奖励函数<script type="math/tex">r(s_{t},a_{t})</script>。</p>
<p>​    从状态的收益被定义为具有折扣因子<script type="math/tex">\gamma \in [0,1]</script>的折扣未来奖励的总和<script type="math/tex">R_{t}=\sum_{i=t}^{T}\gamma ^{(i-t)}r(s_{i},a_{i})</script>。 请注意，返回值取决于选择的操作，因此在策略<script type="math/tex">\pi</script>上，并且可能是随机的。 强化学习的目标是学习一种策略，该策略将从初始分布最大化的预期回报。 我们表示策略<script type="math/tex">\pi</script>的折扣状态访问分布为<script type="math/tex">\rho ^{\pi }</script>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/20/Soft-Actor-Critic/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/20/Soft-Actor-Critic/" class="post-title-link" itemprop="url">Soft Actor-Critic</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-20 21:14:17" itemprop="dateCreated datePublished" datetime="2021-05-20T21:14:17+08:00">2021-05-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 23:17:10" itemprop="dateModified" datetime="2021-06-01T23:17:10+08:00">2021-06-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/17/graph-network-study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/17/graph-network-study/" class="post-title-link" itemprop="url">graph_network_study</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-17 22:50:56" itemprop="dateCreated datePublished" datetime="2021-05-17T22:50:56+08:00">2021-05-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-15 22:31:27" itemprop="dateModified" datetime="2021-06-15T22:31:27+08:00">2021-06-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/12/docker-study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/12/docker-study/" class="post-title-link" itemprop="url">docker study</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-12 21:00:59" itemprop="dateCreated datePublished" datetime="2021-05-12T21:00:59+08:00">2021-05-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 23:28:12" itemprop="dateModified" datetime="2021-06-01T23:28:12+08:00">2021-06-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/koktlzz/p/14105026.html#%E4%BB%80%E4%B9%88%E6%98%AF%E5%AE%B9%E5%99%A8%E6%95%B0%E6%8D%AE%E5%8D%B7%EF%BC%9F">狂神docker</a></p>
<p>docker run -it -v /home/pql/company_code/tmp:/root/repos/tmp hub.digi-sky.com/aid/aicloud:dante-1.8.0-cuda11.1-cudnn8-runtime /bin/bash</p>
<p>使用-v挂载文件夹时，假如docker容器里已存在挂载的文件夹，并且宿主主机也存在挂载文件夹，宿主主机的文件夹内容会覆盖docker容器里的内容。如果宿主主机不存在挂载文件夹，会新建文件夹，并让docker容器里的文件夹同步（删除之前的内容）。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">QilongPan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">44</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QilongPan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
