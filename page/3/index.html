<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="潘其龙">
<meta property="og:url" content="http://example.com/page/3/index.html">
<meta property="og:site_name" content="潘其龙">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="QilongPan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>潘其龙</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">潘其龙</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/" class="post-title-link" itemprop="url">Counterfactual Multi-Agent Policy Gradients</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-16 21:19:44" itemprop="dateCreated datePublished" datetime="2021-06-16T21:19:44+08:00">2021-06-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-08-10 22:14:55" itemprop="dateModified" datetime="2021-08-10T22:14:55+08:00">2021-08-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93/" itemprop="url" rel="index"><span itemprop="name">多智能体</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    许多现实世界中的问题，如网络包路由和自动驾驶车辆的协调，很自然地被建模为合作的多智能体系统。非常需要新的强化学习方法，可以轻松地学习此类系统的分散策略。为此，我们提出了一种新的多智能体actor-critic方法，称为counterfactual multi-agent(COMA)策略梯度。COMA使用一个集中的critic来估计Q-function，并使用分散的actors来优化代理的策略。此外，为了解决多代理credit assignent的挑战，它使用了一个counterfactual(反事实的) baseline，边缘化一个单一代理的行动，同时保持其他代理的行动不变。COMA还使用了一种critic表示，允许在一次向前传递中有效地计算counterfactual baseline。我们在StarCraft unit micromanagement测试平台中评估 COMA，使用具有显着部分可观察性的分散变体。 在这种情况下，COMA 显着提高了其他多代理 actor-critic 方法的平均性能，并且性能最好的代理与可以访问完整状态的最先进的集中控制器相比具有竞争力。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    许多复杂的强化学习(RL)问题，如自动驾驶车辆的协调 (Cao et al. 2013)，网络分组传送 (Ye, Zhang, and Yang 2015),和分布式物流系统 (Ying and Dayong 2005)很自然地被建模为合作的多智能体系统。然而，为单个代理设计的RL方法通常在此类任务上表现不佳，因为代理的联合动作空间随着代理的数量呈指数级增长。</p>
<p>​    为了应对这种复杂性，通常需要诉诸去中心化策略，其中每个代理仅根据其本地动作观察历史来选择自己的动作。 此外，执行期间的部分可观察性和通信限制可能需要使用分散的策略，即使联合行动空间不是很大。</p>
<p>​    因此，非常需要能够有效学习分散策略的新 RL 方法。 在某些情况下，学习本身也可能需要去中心化。 然而，在许多情况下，学习可以在模拟器或实验室中进行，在那里可以获得额外的状态信息并且代理可以自由交流。 这种分散策略的集中训练是多智能体规划的标准范式Oliehoek, Spaan, and Vlassis 2008; Kraemer and Banerjee 2016)，最近被深度强化学习社区采用 (Foerster et al. 2016; Jorge, Kageback, and Gustavsson 2016).。 然而，如何最好地利用集中学习的机会的问题仍然存在。</p>
<p>​    另一个关键挑战是多智能体credit assignment(信用分配)(Chang, Ho, and Kaelbling 2003)：在合作环境中，联合行动通常只产生全局奖励，这使得每个智能体很难推断出自己对团队成功的贡献。 有时可以为每个代理设计单独的奖励函数。 然而，这些奖励在合作环境中通常不可用，并且通常无法鼓励个体代理为更大的利益做出牺牲。 这通常会严重阻碍多智能体在具有挑战性的任务中学习，即使智能体数量相对较少。</p>
<p>​    在本文中，我们提出了一种新的多智能体强化学习方法，称为counterfactual multi-agent（反事实多智能体 COMA) 策略梯度，以解决这些问题。 COMA 采用actor-critic(Konda and Tsitsiklis 2000)方法，其中actor即策略，通过遵循critic估计的梯度进行训练。 COMA 基于三个主要思想。</p>
<p>​    首先，COMA 使用集中式critic。 critic只在学习期间使用，而在执行期间只需要actor。 由于学习是集中式的，因此我们可以使用集中式critic，以联合动作和所有可用状态信息为条件，而每个代理的策略仅以自己的动作观察历史为条件。</p>
<p>​    其次，COMA使用了一个counterfactual baseline。这个想法的灵感来自difference rewards(Wolpert and Tumer 2002; Tumer and Agogino 2007),其中每个智能体从一个成形的奖励中学习，该奖励将全局奖励与当该智能体的动作替换为default action时收到的奖励进行比较。虽然差异奖励是执行多智能体信用分配的强大方法，但它们需要访问模拟器或估计奖励函数，并且通常不清楚如何选择默认动作。COMA 通过使用集中式critic来计算特定于代理的advantage function来解决这个问题，该function将当前联合行动的估计回报与边缘化单个代理的行为的反事实基线进行比较，同时保持其他代理的行为固定。这类似于计算aristocrat utility(贵族效用)（Wolpert and Tumer 2002)，但避免了策略和效用函数之间递归相互依赖的问题，因为反事实基线对策略梯度的预期贡献为零。因此，COMA 不依赖于额外的模拟、近似值或关于适当默认行为的假设，而是为每个代理计算单独的基线，该基线依赖于集中式critic来推理只有该代理的行为发生变化的反事实。</p>
<p>​    第三，COMA 使用critic表示，允许有效计算反事实基线。 在单个前向传递中，它计算给定代理的所有不同动作的 Q 值，条件是所有其他代理的动作。 因为所有代理都使用一个集中的critic，所以所有代理的所有 Q 值都可以在单个分批前向传递中计算。</p>
<p>​    我们在StarCraft unit micromanagement（参见1）的测试平台中评估 COMA，它最近成为具有高随机性、大状态动作空间和延迟奖励的具有挑战性的 RL 基准任务。 以前的工作 (Usunier et al. 2016; Peng et al. 2017)利用了集中控制策略，该策略以整个状态为条件，并且可以使用强大的宏观动作，使用星际争霸的内置规划器，结合移动和攻击动作 . 为了产生一个有意义的去中心化基准，证明即使在代理相对较少的情况下也具有挑战性，我们提出了一种变体，可以大规模减少每个代理的视野并删除对这些宏观动作的访问。</p>
<p>​    我们在这个新基准上的实证结果表明，与其他多代理 actor-critic 方法以及 COMA 本身的消融版本相比，COMA 可以显着提高性能。 此外，COMA 的最佳代理可以与最先进的集中控制器竞争，后者可以访问完整的状态信息和宏操作。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>​    尽管多智能体强化学习已应用于各种设置(Busoniu, Babuska, and De Schutter 2008; Yang and Gu 2004)，但它通常仅限于表格方法和简单环境。 一个例外是最近在深度多智能体强化学习方面的工作，它可以扩展到高维输入和动作空间。 Tampuu et al. (2015)将 DQN 与独立 Q-learning相结合 (Tan 1993; Shoham and Leyton-Brown 2009)来学习如何玩两人乒乓球。 最近Leibo et al. (2017)等人使用了相同的方法研究在sequential社会困境中合作和背叛的出现。</p>
<p>​    同样相关的还有智能体之间进行通信的出现，并通过梯度下降学习(Das et al. 2017; Mordatch and Abbeel 2017; Lazaridou, Peysakhovich, and Baroni 2016; Foerster et al. 2016; Sukhbaatar, Fergus, and others 2016) 。在这一系列工作中，在训练期间在智能体之间传递梯度和共享参数是利用集中训练的两种常见方式。 然而这些方法不允许在学习期间使用额外的状态信息，也不能解决多智能体信用分配问题。</p>
<p>​    Gupta, Egorov, and Kochenderfer (2017)研究了通过集中训练实现分散执行的actor-critic方法。 然而在他们的方法中，局部、每个代理、observations和actions以及多代理信用分配的演员和评论家条件都只能通过手工制作的局部奖励来解决。</p>
<p>​    RL 以前在StarCraft management中的大多数应用都使用集中式控制器，可以访问完整状态并控制所有单元，尽管控制器的架构利用了问题的多代理性质。 Usunier et al. (2016) 使用greedy MDP，它在每个时间步按顺序选择给定之前所有动作的代理的动作，并结合 zero-order optimisation(零阶优化)，而 Peng et al. (2017)使用依赖 RNN 在代理之间交换信息的 actor-critic 方法。</p>
<p>​    最接近我们的问题设置的是Foerster et al. (2017)的问题。他们也使用多代理表示和分散策略。 然而，他们在使用 DQN 时专注于稳定经验回放，并没有充分利用集中训练制度。 由于他们不报告绝对赢率，我们不直接比较性能。 然而， Usunier et al. (2016)  解决了与我们的实验类似的场景，并在完全可观察的环境中实现了 DQN 基线。 因此，在第 6 节中，我们报告了我们针对这些最先进基线的竞争表现，同时保持分散控制。Omidshafifiei et al. (2017)还解决了多智能体环境中经验回放的稳定性问题，但假设了一个完全分散的训练制度。</p>
<p>​    (Lowe et al. 2017)同时提出了一种使用集中式critic的多代理策略梯度算法。 他们的方法没有解决多代理信用分配问题。 与我们的工作不同，它为每个代理学习一个单独的集中式critic，并应用于具有连续动作空间的竞争环境。</p>
<p>​    我们的工作直接建立在difference rewards (Wolpert and Tumer 2002)的思想之上。 COMA 与这工作的关系在第 4 节中讨论。</p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>​    我们考虑一个完全合作的多智能体任务，它可以被描述为一个随机博弈 <script type="math/tex">G</script>，由一个元组<script type="math/tex">G= \left \langle S,U,P,r,Z,O,n,\gamma  \right \rangle</script>定义，其中<script type="math/tex">n</script>个智能体由<script type="math/tex">a\in A\equiv \left \{ 1,...,n \right \}</script>选择时序动作。 环境有一个真实的状态 <script type="math/tex">s\in S</script>。在每个时间步，每个智能体同时选择一个动作<script type="math/tex">u^{a}\in U</script>，形成一个联合动作<script type="math/tex">u\in U\equiv U^{n}</script>，根据状态转换函数在环境中引起转换 <script type="math/tex">P\left ( s^{'}|s,u \right ):S\times U\times S\rightarrow \left [ 0,1 \right ]</script>。 代理都共享相同的奖励函数<script type="math/tex">r\left ( s,u \right ):S\times U\rightarrow r</script>和<script type="math/tex">\gamma \in \left [ 0,1 \right )</script>是一个折扣因子。</p>
<p>​    我们考虑一个部分可观察的设置，其中代理根据观察函数 <script type="math/tex">O(s,a):S\times A\rightarrow Z</script>绘制观察<script type="math/tex">z\in Z</script>。每个代理都有一个动作观察历史<script type="math/tex">\tau ^{a}\in T\equiv \left ( Z\times U \right )^{\ast }</script>，它以随机策略<script type="math/tex">\pi ^{a\left ( u^{a}|\tau ^{a} \right )}:T\times U\rightarrow \left [ 0,1 \right ]</script>为条件。 我们用粗体表示代理的联合数量，并用上标<script type="math/tex">-a</script>表示给定代理<script type="math/tex">a</script>以外的代理的联合数量。</p>
<p>​    折扣回报为<script type="math/tex">R_{t}=\sum_{l=0}^{\infty }\gamma ^{l}r_{t+l}</script>。代理的联合策略引入一个值函数，即期望超过<script type="math/tex">R_{t}</script>、Vπ(st)=Est+1：∞，ut：∞[Rt|st]，以及<script type="math/tex">V^{\pi }\left ( s_{t} \right )=E_{s_{t+1}:\infty,u_{t}:\infty  }\left [ R_{t}|s_{t}\right ]</script>和action-value函数<script type="math/tex">Q^{\pi }(s_{t},u_{t})=E_{s_{t+1}:\infty,u_{t+1}:\infty  }\left [ R_{t}|s_{t},u_{t} \right ]</script>。优势函数由<script type="math/tex">A^{\pi }\left ( s_{t},u_{t} \right )=Q^{\pi }\left ( s_{t},u_{t} \right )-V^{\pi }\left ( s_{t} \right )</script>给出。</p>
<p>​    继之前的工作(Oliehoek, Spaan, and Vlassis 2008; Kraemer and Banerjee 2016; Foerster et al. 2016; Jorge, Kageback, and Gustavsson 2016)之后，我们的问题设置允许集中训练，但需要分散执行。 这是大量多智能体问题的自然范例，其中使用具有附加状态信息的模拟器进行训练，但智能体在执行期间必须依赖于局部动作观察历史。 为了以完整的历史为条件，深度强化学习代理可能会使用循环神经网络(Hausknecht and Stone 2015)，通常使用门控模型，例如 LSTM (Hochreiter and Schmidhuber 1997) 或 GRU(Cho et al. 2014)。</p>
<p>​    在第 4 节中，我们开发了一种新的多智能体策略梯度方法来解决这个问题。 在本节的其余部分，我们提供了有关单代理策略梯度方法的一些背景知识 (Sutton et al. 1999)。 这些方法通过对预期折扣总奖励<script type="math/tex">J=E_{\pi }\left [ R_{0} \right ]</script>的估计执行梯度上升来优化由<script type="math/tex">\theta ^{\pi }</script>参数化的单个代理的策略。 也许最简单的策略梯度形式是 REINFORCE (Williams 1992)，其中的梯度是：</p>
<script type="math/tex; mode=display">
g=E_{s_{0}:\infty ,u_{0}:\infty }\left [ \sum_{t=0}^{T}R_{t}\bigtriangledown _{\theta ^{\pi }}log\pi \left ( u_{t}|s_{t} \right ) \right ]</script><p>​    在actor-critic方法中 (Sutton et al. 1999; Konda and Tsitsiklis 2000; Schulman et al. 2015)，actor即策略，通过遵循依赖于critic(通常估计一个值函数)的梯度进行训练。特别是<script type="math/tex">R_{t}</script>被任何等价于<script type="math/tex">Q\left ( s_{t},u_{t} \right )-b\left ( s_{t} \right )</script> 的表达式替换，其中<script type="math/tex">b\left ( s_{t} \right )</script>是设计用于减少方差的基线 (Weaver and Tao 2001)。 一个常见的选择是 <script type="math/tex">b\left ( s_{t} \right )=V\left ( s_{t} \right )</script>，在这种情况下，<script type="math/tex">R_{t}</script>被<script type="math/tex">A(s_{t},u_{t})</script>替换。 另一种选择是用temporal difference(TD) 误差<script type="math/tex">r_{t}+\gamma V\left ( s_{t+1} \right )-V\left ( s \right )</script>替换 <script type="math/tex">R_{t}</script>，这是<script type="math/tex">A(s_{t},u_{t})</script>的无偏估计。 在实践中，梯度必须根据从环境中采样的轨迹来估计，并且（动作）值函数必须用函数逼近器来估计。 因此，梯度估计的偏差和方差在很大程度上取决于估计量的确切选择 (Konda and Tsitsiklis 2000)。</p>
<p>​    在本文中，我们使用<script type="math/tex">TD(\lambda)</script>(Sutton1988)适用于深度神经网络的变体，训练critic <script type="math/tex">f^{c}\left ( .,\theta ^{c} \right )</script> on-policy来估计<script type="math/tex">Q</script>或<script type="math/tex">V</script>。<script type="math/tex">TD(\lambda)</script>混合使用<script type="math/tex">n</script>步返回<script type="math/tex">G_{t}^{(n)}=\sum_{l=1}^{n}\gamma ^{l-1}r_{t+l}+\gamma ^{n}f^{c}\left ( ._{t+n},\theta ^{c} \right )</script>。特别是，critic参数<script type="math/tex">\theta^{c}</script>通过小批梯度下降进行更新，以最小化以下loss:</p>
<script type="math/tex; mode=display">
L_{t}\left ( \theta ^{c} \right )=\left ( y^{\left ( \lambda  \right )}-f^{c}\left ( ._{t},\theta ^{c} \right ) \right )^{2}</script><p>其中<script type="math/tex">y^{\left ( \lambda  \right )}=\left ( 1-\lambda  \right )\sum_{n=1}^{\infty }\lambda ^{n-1}G_{t}^{(n)}</script>和n-step返回<script type="math/tex">G_{t}^{(n)}</script>是用目标网络(Mnih et al. 2015)估计的bootstrapped值计算的，定期从<script type="math/tex">\theta^ {c}</script>复制参数。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>在本节中，我们将描述将策略梯度扩展到我们的多代理设置的方法。</p>
<h2 id="Independent-Actor-Critic"><a href="#Independent-Actor-Critic" class="headerlink" title="Independent Actor-Critic"></a>Independent Actor-Critic</h2><p>​    将策略梯度应用于多个智能体的最简单方法是让每个智能体独立学习，有自己的actor和critic，根据自己的action-observation历史。 这本质上是independent Q-learning(Tan 1993)背后的想法，它可能是最流行的多智能体学习算法，但用 actor-critic 代替了 Q-learning。 因此，我们称这种方法为independent actor-critic (IAC)。</p>
<p>​    在我们的 IAC 实现中，我们通过在代理之间共享参数来加速学习，即我们只学习一个actor和一个critic，所有智能体都使用它们。 智能体仍然可以表现不同，因为它们接收不同的观察，包括特定于智能体的 ID，从而演化出不同的隐藏状态。 学习保持独立，因为每个智能体的critic只估计一个局部价值函数，即以<script type="math/tex">u^{a}</script>而不是<strong>u</strong>为条件的价值函数。 虽然我们不知道这个特定算法以前的应用，但我们不认为它是一个重大贡献，而只是一个基线算法。</p>
<p>​    我们考虑 IAC 的两种变体。 首先，每个智能体的critic估计<script type="math/tex">V\left ( \tau ^{a} \right )</script>并遵循基于<script type="math/tex">TD</script>误差的梯度，如第 3 节所述。第二，每个智能体的critic估计<script type="math/tex">Q\left ( \tau ^{a},u^{a} \right )</script>并遵循一个梯度基于优势:<script type="math/tex">A\left ( \tau ^{a},u^{a} \right ) = Q\left ( \tau ^{a},u^{a} \right )-V\left ( \tau ^{a} \right )</script>，其中<script type="math/tex">V\left ( \tau ^{a} \right )=\sum_{u^{a}}^{}\pi \left ( u^{a}|\tau ^{a} \right )Q\left ( \tau ^{a},u^{a} \right )</script>。独立学习很简单，但在训练时缺乏信息共享使得学习依赖于多个智能体之间交互的协调策略变得困难，或者单个智能体难以估计其行为对团队奖励的贡献。</p>
<h2 id="Counterfactual-Multi-Agent-Policy-Gradients"><a href="#Counterfactual-Multi-Agent-Policy-Gradients" class="headerlink" title="Counterfactual Multi-Agent Policy Gradients"></a>Counterfactual Multi-Agent Policy Gradients</h2><p>​    上面讨论的困难之所以出现，是因为除了参数共享之外，IAC 未能利用学习集中在我们的环境中这一事实。 在本节中，我们提出了反事实多智能体 (COMA) 策略梯度，它克服了这一限制。 COMA 背后的三个主要思想：1) critic的集中化，2) 使用反事实基线，以及 3) 使用允许对基线进行有效评估的critic表示。 本节的其余部分描述了这些想法。</p>
<p>​    首先，COMA 使用集中式critic。 请注意，在 IAC 中，每个actor <script type="math/tex">\pi \left ( u^{a}|\tau ^{a} \right )</script> 和每个critic <script type="math/tex">Q\left (\tau ^{a}, u^{a}\right )</script>或 <script type="math/tex">V\left ( \tau ^{a} \right )</script>仅以代理自己的动作观察历史<script type="math/tex">\tau ^{a}</script>为条件。 但是，critic仅在学习期间使用，在执行期间只需要actor。 由于学习是集中式的，因此我们可以使用集中式critic，该critic以真实的全局状态<script type="math/tex">s</script>为条件（如果可用），否则使用联合动作观察历史<script type="math/tex">\tau</script>。 每个actor以自己的动作观察历史<script type="math/tex">\tau ^{a}</script>为条件，参数共享，如在 IAC 中。 图 1a 说明了这种设置。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/figure1.png" alt></p>
<p>​    使用这个集中式critic的一种普通的方法是让每个actor遵循基于从这个critic估计的<script type="math/tex">TD</script>误差的梯度：</p>
<script type="math/tex; mode=display">
g=\bigtriangledown _{\theta ^{\pi }}log\pi \left ( u|\tau _{t}^{a} \right )\left ( r+\gamma V\left ( s_{t+1} \right) -V\left ( s_{t} \right )\right )</script><p>​    然而，这种方法未能解决关键的信用分配问题。 由于 TD 误差仅考虑全局奖励，因此为每个actor计算的梯度并未明确判断该特定代理的行为如何对全局奖励做出贡献。 由于其他代理可能正在探索，该代理的梯度变得非常嘈杂，特别是当有很多代理时。</p>
<p>​    因此，COMA 使用counterfactual baseline（反事实基线）。 这个想法受到difference rewards (Wolpert and Tumer 2002)的启发，其中每个智能体从一个成形的奖励<script type="math/tex">D^{a}=r\left ( s,u \right )-r\left ( s,\left ( u^{-a},c^{a} \right ) \right )</script>中学习，该奖励将全局奖励与当代理 a 的动作被默认动作<script type="math/tex">c^{a}</script>替换时收到的奖励进行比较。 代理<script type="math/tex">a</script>改进<script type="math/tex">D^{a}</script>的任何动作也会提高真实的全局奖励<script type="math/tex">r\left ( s,u \right )</script>，因为<script type="math/tex">r\left ( s,\left ( u^{-a},c^{a} \right ) \right )</script>不依赖于代理<script type="math/tex">a</script>的动作。</p>
<p>​    Difference rewards是执行多智能体信用分配的有效方式。 但是，它们通常需要访问模拟器才能估计<script type="math/tex">r\left ( s,\left ( u^{-a},c^{a} \right ) \right )</script>。 当模拟器已经用于学习时，difference rewards会增加必须进行的模拟次数，因为每个代理的difference reward需要单独的反事实模拟。  Proper and Tumer (2012) and Colby, Curran, and Tumer (2015)建议使用函数近似而不是模拟器来估计差异奖励。 但是，这仍然需要用户指定的默认操作 <script type="math/tex">c^{a}</script>，这在许多应用程序中可能难以选择。 在 actor-critic 架构中，这种方法还会引入额外的近似误差源。</p>
<p>​    COMA 背后的一个关键见解是，可以使用集中式critic以避免这些问题的方式实现difference rewards。 COMA 学习了一个集中式critic <script type="math/tex">Q(s, u)</script>，它估计了以中央状态<script type="math/tex">s</script>为条件的联合动作<script type="math/tex">u</script>的<script type="math/tex">Q</script>值。 然后，对于每个代理 <script type="math/tex">a</script>，我们可以计算一个优势函数，将当前动作<script type="math/tex">u^{a}</script>的<script type="math/tex">Q</script>值与边缘化<script type="math/tex">u^{a}</script>的反事实基线进行比较，同时保持其他代理的动作<script type="math/tex">u^{-a}</script>固定：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula4.png" alt></p>
<p>​    因此，<script type="math/tex">A^{a}\left ( s,u^{a} \right )</script>为每个代理计算单独的基线，该基线使用集中式critic来推理反事实，其中只有 a 的动作发生变化，直接从代理的经验中学习，而不是依赖额外的模拟、奖励模型或 用户设计的默认操作。</p>
<p>​    这种优势与aristocrat utility (Wolpert and Tumer 2002)具有相同的形式。 然而，使用基于价值的方法优化aristocrat utility 会产生 self-consistency（自一致性）问题，因为策略和 utility function相互依赖。 因此，先前的工作侧重于使用默认状态和操作进行差异评估。 COMA 是不同的，因为反事实基线对梯度的预期贡献，与其他策略梯度基线一样为零。 因此，虽然基线确实取决于策略，但它的期望却不是。 因此，COMA 可以使用这种形式的优势而不会产生self-consistency问题。</p>
<p>​    虽然 COMA 的优势函数用critic的评估代替了潜在的额外模拟，但如果critic是深度神经网络，这些评估本身可能很昂贵。此外，在典型的表示中，这种网络的输出节点数将等于<script type="math/tex">\left | U \right |^{n}</script>，即联合动作空间的大小，这使得训练变得不切实际。为了解决这两个问题，COMA 使用了一种critic表示，可以有效地评估基线。特别是，其他代理的动作<script type="math/tex">u_{t}^{-a}</script>是网络输入的一部分，网络为每个代理 a 的动作输出 Q 值，如图 1c 所示。因此，对于每个代理，可以通过actor和critic的单次前向传递有效地计算反事实优势。此外，输出的数量只有<script type="math/tex">\left | U \right |</script>而不是 (<script type="math/tex">\left | U \right |^{n}</script>)。虽然网络有一个很大的输入空间，可以在代理和动作的数量上线性扩展，但深度神经网络可以很好地在这些空间上泛化。 </p>
<p>​    在本文中，我们专注于具有离散动作的设置。 然而通过使用蒙特卡罗样本估计（4）中的期望或使用使其具有分析性的函数形式（例如高斯策略和critic），可以轻松地将 COMA 扩展到连续动作空间。</p>
<p>​    以下引理建立了 COMA 收敛到局部最优策略。 证明直接来自单代理actor-critic算法的收敛 (Sutton et al. 1999; Konda and Tsitsiklis 2000)，并且服从相同的假设。</p>
<h3 id="Lemma-1"><a href="#Lemma-1" class="headerlink" title="Lemma 1"></a>Lemma 1</h3><p>​    对于一个具有遵循COMA策略梯度的兼容<script type="math/tex">TD(1)</script> critic的actor-critic算法。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula5.png" alt></p>
<p>在每次迭代的k处</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula6.png" alt></p>
<p>证明:COMA梯度如下</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula7.png" alt></p>
<p>其中<script type="math/tex">\theta</script>是所有actor策略的参数，例如<script type="math/tex">\theta =\left \{ \theta ^{1},...,\theta ^{\left | A \right |} \right \}</script>和<script type="math/tex">b\left ( s,u^{-a} \right )</script>是等式4中定义的反事实基线。</p>
<p>​    首先考虑该基线<script type="math/tex">b\left ( s,u^{-a} \right )</script>的预期贡献：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula9.png" alt></p>
<p>其中期望<script type="math/tex">E_{\pi}</script>是关于由联合策略<script type="math/tex">\pi</script>引起的状态行动分布的。现在让<script type="math/tex">d^{\pi}(s)</script>是Sutton et al. (1999)定义的折现遍历状态分布：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula10.png" alt></p>
<p>显然，每代理的基线虽然降低了方差，但不会改变预期的梯度，因此不会影响COMA的收敛性。</p>
<p>​    预期策略梯度的其余部分为：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula13.png" alt></p>
<p>将联合政策作为独立actor的产物来制定：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula15.png" alt></p>
<p>生成标准的单代理actor-critic策略梯度：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula16.png" alt></p>
<p>​    Konda and Tsitsiklis (2000)证明了遵循这个梯度的actor-critic收敛于期望返回<script type="math/tex">J^{\pi }</script>的局部最大值：</p>
<ol>
<li><p>策略<script type="math/tex">\pi</script>是可微的，</p>
</li>
<li><p><script type="math/tex">Q</script>和<script type="math/tex">\pi</script>的更新时间尺度都足够慢，而且<script type="math/tex">\pi</script>的更新速度比<script type="math/tex">Q</script>足够慢。</p>
</li>
<li><p><script type="math/tex">Q</script>使用了与<script type="math/tex">\pi</script>兼容的表示法</p>
<p>在几个进一步的假设中。 策略的参数化（即单代理联合动作学习器被分解为独立的actor）对于收敛来说并不重要，只要它保持可微分即可。 但是请注意，COMA 的中心化critic对于此证明的成立至关重要。</p>
</li>
</ol>
<h1 id="实验装置"><a href="#实验装置" class="headerlink" title="实验装置"></a>实验装置</h1><p>​    在本节中，我们将描述我们应用 COMA 的星际争霸问题，以及状态特征、网络架构、训练机制和消融的细节。</p>
<h2 id="Decentralised-StarCraft-Micromanagement"><a href="#Decentralised-StarCraft-Micromanagement" class="headerlink" title="Decentralised StarCraft Micromanagement"></a>Decentralised StarCraft Micromanagement</h2><p>​    《星际争霸》是一个具有随机动态的丰富环境，无法轻易模拟。 相比之下，许多更简单的多代理设置，例如 Predator-Prey (Tan 1993) 或 Packet World (Weyns, Helleboogh, and Holvoet 2005)，具有完整的模拟器，可以控制随机性，可以自由设置为任何状态，以便完美地 重播经历。 这使得通过额外的模拟计算差异奖励成为可能，尽管计算成本很高。 在星际争霸中，就像在现实世界中一样，这是不可能的。</p>
<p>​    在本文中，我们关注的是星际争霸中的micromanagement问题，它指的是在与敌人作战时对单个单位的定位和攻击命令的低级控制。 这个任务自然表现为一个多代理系统，其中每个星际争霸单元都被一个分散的控制器所取代。 我们考虑了几种由对称团队组成的场景：3 名海军陆战队员 (3m)、5 名海军陆战队员 (5m)、5 名幽灵 (5w) 或 2 名龙骑兵和 3 名狂热者 (2d 3z)。 敌方团队由星际争霸 AI 控制，它使用合理但次优的手工启发式方法。</p>
<p>​    我们允许代理从一组离散的操作中进行选择：move[direction], attack[enemy id],stop, and noop。在星际争霸游戏中，当一个单位选择攻击动作时，它会先移动到攻击范围内再开火，利用游戏内置的寻路来选择路线。 这些强大的attack-move 宏动作使控制问题变得相当容易。</p>
<p>​    为了创建一个更具挑战性的、更有意义的去中心化基准，我们对代理施加了一个有限的视野，等于远程单位武器的射程，如图 2 所示。这偏离了集中式星际争霸控制的标准设置有三个作用。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/figure2.png" alt></p>
<p>​    首先，它引入了显着的部分可观察性。 其次，这意味着单位只能在敌人范围内进行攻击，从而无法访问星际争霸宏操作。 第三，代理无法区分已死亡的敌人和超出范围的敌人，因此可以向这些敌人发出无效的攻击命令，从而导致不采取任何行动。 这大大增加了动作空间的平均大小，从而增加了探索和控制的难度。</p>
<p>​    在这些困难的条件下，即使单元数量相对较少的场景也变得更加难以解决。 如表 1 所示，我们与一个简单的手工编码启发式进行了比较，该启发式指示代理向前跑到范围内，然后集中火力，依次攻击每个敌人直到其死亡。 这种启发式在 5m 的全视野下实现了 98% 的胜率，但在我们的设置中只有 66%。 为了在这项任务中表现出色，代理必须通过正确定位和集中火力来学习合作，同时记住哪些敌方和盟军单位还活着或不在视野中。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/table1.png" alt></p>
<p>​    所有代理在每个时间步都会收到相同的全局奖励，等于对对手单位造成的伤害总和减去所受伤害的一半。 杀死对手会产生 10 分的奖励，赢得比赛会产生等于团队剩余总生命值加 200 的奖励。 这种基于伤害的奖励信号与 Usunier et al. (2016)使用的信号相当。 与 (Peng et al. 2017)不同，我们的方法不需要估计本地奖励。</p>
<h2 id="State-Features"><a href="#State-Features" class="headerlink" title="State Features"></a>State Features</h2><p>​    actor和critic接收不同的输入特征，分别对应于局部观察和全局状态。 两者都包括盟友和敌人的特征。 单位可以是盟友或敌人，而智能体是指挥盟友单位的分散控制器。</p>
<p>​    每个代理的局部观察仅从以它控制的单位为中心的地图的圆形子集中绘制，并包括该视野内的每个单位：distance, relative x, relative y, unit type and shield（盾牌）（开火后一个单位的冷却时间被重置，它必须在再次开火前掉落。 盾牌吸收伤害直到它们破裂，之后单位开始失去健康。 龙骑兵和狂热者有盾牌，但海军陆战队没有）。 所有特征都是通过它们的最大值归一化。 我们不包括有关单位当前目标的任何信息。</p>
<p>​    全局状态表示由相似的特征组成，但对于地图上的所有单元，无论视野如何。 不包括绝对距离，并且 x-y 位置是相对于地图中心而不是特定代理给出的。 全局状态还包括所有代理的生命值和冷却时间。 提供给集中式 Q-function critic的表示是全局状态表示与正在评估其行为的代理的局部观察的连接。 我们估计 V (s) 的中心化critic，因此是代理不可知的，接收与所有代理的观察连接的全局状态。 观察结果不包含新信息，但包括相对于该代理的自我中心距离。</p>
<h2 id="Architecture-amp-Training"><a href="#Architecture-amp-Training" class="headerlink" title="Architecture &amp; Training"></a>Architecture &amp; Training</h2><p>​    参与者由128-bit门控循环单元(GRU)(Cho et al. 2014)组成。使用全连接层来处理输入和从隐藏状态产生输出值<script type="math/tex">h_{t}^{a}</script>。IAC的critic使用额外的output heads附加到actor网络的最后一层。动作概率是通过最后一层<script type="math/tex">z</script>，通过一个有界的softmax分布产生的，任何给定动作的概率下界为<script type="math/tex">\epsilon /\left | U \right |:P\left ( u \right )=\left ( 1-\epsilon  \right )softmax(z)_{u}+\epsilon /\left | U \right |</script>。我们在750个episode中线性地将<script type="math/tex">\epsilon</script>从0.5退火到0.02。集中的critic是一个前馈网络，具有多个ReLU层和全连接层。超参数在5m场景上进行了粗略调整，然后用于所有其他地图。我们发现最敏感的参数是<script type="math/tex">TD\left ( \lambda  \right )</script>，但确定了<script type="math/tex">\lambda = 0.8</script>，它对COMA和我们的baselines都最有效。我们的实现使用了TorchCraft (Synnaeve et al. 2016)和Torch 7 (Collobert, Kavukcuoglu, and Farabet 2011)。伪代码和关于训练程序的进一步细节见补充资料。</p>
<p>​    我们尝试了在代理水平考虑的critic体系结构，并进一步利用内部参数共享。然而，我们发现可伸缩性的瓶颈不是critic的集中，而是多智能体探索的困难。因此，我们推迟对COMA critic的因素的进一步调查到未来的工作中。</p>
<h2 id="Ablations"><a href="#Ablations" class="headerlink" title="Ablations"></a>Ablations</h2><p>​    我们进行了消融实验，验证了COMA的三个关键元素。首先，我们通过比较两种IAC变体，IAC-Q和IAC-V来测试集中化critic的重要性。这些critic采用与actor相同的分散输入，并与actor网络共享参数，直到最后一层。然后，IAC-Q输出<script type="math/tex">\left | U \right |</script> Q-values，每个动作输出一个，而IAC-V输出单个state-value。请注意，我们仍然在代理之间共享参数，使用以自我为中心的观察和ID作为输入的一部分，以允许出现不同的行为。合作报酬函数仍然由所有代理共享。</p>
<p>​    其次，我们测试了学习Q而不是V的重要性。该方法central-V 的critic仍然使用中心状态，但学习V(s)，并使用TD误差来估计策略梯度更新的优势。</p>
<p>​    第三，我们测试了我们的反事实基线的作用。central-QV方法同时学习Q和V，并估计其优势为Q−V，用V取代了COMA的反事实基线。所有的方法都为actor使用相同的架构和训练方案，所有的critic都使用<script type="math/tex">TD\left ( \lambda  \right )</script>进行训练。</p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>​    图3显示了每种方法和每种星际争霸场景的平均胜率。对于每种方法，我们进行了35次独立的试验，每训练100个episode存储一次模型，用于通过200 episode评估每种方法，绘制出每个episode和试验的平均水平。还显示了性能上的一个标准偏差。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/figure3.png" alt></p>
<p>​    结果表明，COMA在所有情况下都优于IAC基线。有趣的是，IAC方法最终也在5m场景学习了合理的策略，尽管它们需要更多的episode来实现。这可能似乎违反直觉，因为在IAC方法中，actor和critic网络在他们的早期层中共享参数（参见第5节），这可能会加速学习。然而，这些结果表明，通过使用全局状态而提高策略评估的准确性，超过了训练单独网络的开销。</p>
<p>​    此外，在所有设置的训练速度和最终性能方面，COMA在central-QV中具有重要地位。这是一个强有力的指标，表明当我们使用中心Q-critic来训练分散的策略时，我们的反事实基线是至关重要的。</p>
<p>​    学习状态值函数具有不以联合动作为条件的明显优势。 尽管如此，我们发现 COMA 在最终性能方面优于central-V 基线。 此外，COMA 通常更快地实现良好的策略，这是预期的，因为 COMA 提供了一个成形的训练信号。 训练也比 central-V 更稳定，这是 COMA 梯度随着策略变得贪婪而趋于零的结果。 总的来说，COMA 是性能最好和最一致的方法。</p>
<p>​    Usunier et al. (2016) 报告了他们最好的代理的性能，他们用他们最先进的集中式控制器标记为 GMEZO（具有episodic zero-order optimisation(Zero-Order Optimization Methods with Applications to RL )零阶优化的贪婪 MDP），以及集中式 DQN 控制器，两者都给出了完整的视野和访问攻击移动宏动作。 这些结果在表 1 中与针对每个地图使用 COMA 训练的最佳代理进行了比较。 显然，在大多数情况下，这些代理的性能可与公布的最佳获胜率相媲美，尽管它们受到分散策略和本地视野的限制。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/table1.png" alt></p>
<h1 id="总结和未来工作"><a href="#总结和未来工作" class="headerlink" title="总结和未来工作"></a>总结和未来工作</h1><p>​    本文介绍了 COMA 策略梯度，这是一种使用集中式critic来估计多智能体 RL 中分散式策略的反事实优势的方法。 COMA 通过使用反事实基线来解决多代理信用分配的挑战，该基线将单个代理的行为边缘化，同时保持其他代理的行为不变。 我们在分散式StarCraft unit micromanagement基准测试中的结果表明，COMA 与其他多智能体 actor-critic 方法相比显着提高了最终性能和训练速度，并在最佳性能报告下与最先进的集中控制器保持竞争力。 未来的工作将扩展 COMA 以处理具有大量代理的场景，在这些场景中，集中式critic更难训练，探索更难协调。 我们还旨在开发更多样本高效的变体，这些变体适用于自动驾驶汽车等实际应用。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/01/deep-learning-tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/01/deep-learning-tutorial/" class="post-title-link" itemprop="url">deep_learning_tutorial</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 23:02:57" itemprop="dateCreated datePublished" datetime="2021-06-01T23:02:57+08:00">2021-06-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-15 22:31:01" itemprop="dateModified" datetime="2021-06-15T22:31:01+08:00">2021-06-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="深度学习笔记"><a href="#深度学习笔记" class="headerlink" title="深度学习笔记"></a>深度学习笔记</h1><p><a target="_blank" rel="noopener" href="https://www.zybuluo.com/hanbingtao/note/433855">转载至深度学习系列教程</a></p>
<h2 id="感知器-神经元"><a href="#感知器-神经元" class="headerlink" title="感知器(神经元)"></a>感知器(神经元)</h2><p>神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是阶跃函数；而当我们说神经元时，激活函数往往选择为sigmoid函数或者tanh函数等。</p>
<p>阶跃函数：</p>
<script type="math/tex; mode=display">
f(z)=\left\{\begin{matrix}
1 &z>0 \\ 
0 & otherwise
\end{matrix}\right.</script><p>神经网络示意图：</p>
<p><img src="/2021/06/01/deep-learning-tutorial/神经网络示意图.png" alt></p>
<p>上图中每个圆圈都是一个神经元，每条线表示神经元之间的连接。我们可以看到，上面的神经元被分成了多层，层与层之间的神经元有连接，而层内之间的神经元没有连接。最左边的层叫做<strong>输入层</strong>，这层负责接收输入数据；最右边的层叫<strong>输出层</strong>，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做<strong>隐藏层</strong>。</p>
<p>隐藏层比较多（大于2）的神经网络叫做深度神经网络。而深度学习，就是使用深层架构（比如，深度神经网络）的机器学习方法。</p>
<p>那么深层网络和浅层网络相比有什么优势呢？简单来说深层网络能够表达力更强。事实上，一个仅有一个隐藏层的神经网络就能拟合任何一个函数，但是它需要很多很多的神经元。而深层网络用少得多的神经元就能拟合同样的函数。也就是为了拟合一个函数，要么使用一个浅而宽的网络，要么使用一个深而窄的网络。而后者往往更节约资源。</p>
<p>深层网络也有劣势，就是它不太容易训练。简单的说，你需要大量的数据，很多的技巧才能训练好一个深层网络。这是个手艺活。</p>
<h3 id="感知器定义"><a href="#感知器定义" class="headerlink" title="感知器定义"></a>感知器定义</h3><p><img src="/2021/06/01/deep-learning-tutorial/感知器.png" alt></p>
<p>感知器组成部分：</p>
<ul>
<li><strong>输入权值</strong> ：一个感知器可以接收多个输入{x<sub>1</sub>,x<sub>2</sub>,…,x<sub>n</sub>|x<sub>i</sub>∈R}，每个输入上有一个<strong>权值</strong>w<sub>i</sub>∈R，此外还有一个<strong>偏置项</strong>b∈R，就是上图中的w<sub>0</sub>；</li>
<li><strong>激活函数f(x)</strong>：感知器的激活函数可以有很多选择，比如sigmoid函数；</li>
<li><strong>输出</strong>：感知器的输出由下面这个公式来计算。</li>
</ul>
<script type="math/tex; mode=display">
y=f(w*x+b)</script><h3 id="感知器训练"><a href="#感知器训练" class="headerlink" title="感知器训练"></a>感知器训练</h3><p>假设损失函数为均方差函数(MSE Mean Square Error)</p>
<script type="math/tex; mode=display">
J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}(a_{i}-y_{i})^{2}\\
J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}(w_{0}x_{0}+w_{1}x_{1}+...+w_{n}x_{n}-y_{i})^{2}</script><p>a<sub>i</sub>为预测值，y<sub>i</sub>为实际值。设m=1(训练样本为1条)时</p>
<script type="math/tex; mode=display">
\frac{\partial J(w,b)}{\partial w_{1}}=x_{1}(w_{0}x_{0}+w_{1}x_{1}+...+w_{n}x_{n}-y_{i})</script><p>其余参数导数求法相同。</p>
<p>使用梯度下降更新：</p>
<script type="math/tex; mode=display">
w_{i}=w_{i}-\alpha \frac{\partial J(w_{i},b)}{\partial w_{i}}</script><p>其中α为学习率。如果损失函数内为y<sub>i</sub>-a<sub>i</sub> ，则上面的梯度下降更新负号应为正号。</p>
<h2 id="梯度下降优化算法"><a href="#梯度下降优化算法" class="headerlink" title="梯度下降优化算法"></a>梯度下降优化算法</h2><p><img src="/2021/06/01/deep-learning-tutorial/梯度下降示例图.png" alt></p>
<p>函数y=f(x)的极值点就是它的导数f<sup>‘</sup>(x)=0的那个点。因此我们可以通过解方程f<sup>‘</sup>(x)=0,求得函数的极值点。</p>
<p>对于计算机来说，随便选择一个点开始，比如上图的点x<sub>0</sub>。接下来，每次迭代修改的为x<sub>1</sub>,<sub>2</sub>,…，经过数次迭代后最终达到函数最小值点。</p>
<p>你可能要问了，为啥每次修改的值，都能往函数最小值那个方向前进呢？这里的奥秘在于，我们每次都是向函数y=f(x)的<strong>梯度</strong>的<strong>相反方向</strong>来修改。什么是<strong>梯度</strong>呢？翻开大学高数课的课本，我们会发现<strong>梯度</strong>是一个向量，它指向<strong>函数值上升最快</strong>的方向。显然，梯度的反方向当然就是函数值下降最快的方向了。我们每次沿着梯度相反方向去修改的值，当然就能走到函数的最小值附近。之所以是最小值附近而不是最小值那个点，是因为我们每次移动的步长不会那么恰到好处，有可能最后一次迭代走远了越过了最小值那个点。步长的选择是门手艺，如果选择小了，那么就会迭代很多轮才能走到最小值附近；如果选择大了，那可能就会越过最小值很远，收敛不到一个好的点上。</p>
<p>梯度下降算法的公式</p>
<script type="math/tex; mode=display">
X_{new}=X_{old}-\alpha \bigtriangledown f(x)</script><p>α为学习率（步长）</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><p>神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是<strong>阶跃函数</strong>；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数。如下图所示：</p>
<p><img src="/2021/06/01/deep-learning-tutorial/sigmoid神经元.png" alt></p>
<p>计算一个神经元的输出的方法和计算一个感知器的输出是一样的。假设神经元的输入是向量<script type="math/tex">\overrightarrow{x}</script></p>
<p> ，权重向量是<script type="math/tex">\overrightarrow{w}</script>(偏置项是w<sub>0</sub>)，激活函数是sigmoid函数，则其输出y：</p>
<script type="math/tex; mode=display">
y=sigmoid(\vec{w}^{T}*\vec{x})\tag1</script><p>sigmoid函数定义如下：</p>
<script type="math/tex; mode=display">
sigmoid(x)=\frac{1}{1+e^{-x}}</script><p>将其带入前面的式子，得到</p>
<script type="math/tex; mode=display">
y=\frac{1}{1+e^{-\vec{w}^{T}\cdot \vec{x}}}</script><p>sigmoid函数是非线性函数，值域是(0,1)。函数图像如下图所示</p>
<p><img src="/2021/06/01/deep-learning-tutorial/sigmoid.jpg" alt></p>
<p>sigmoid函数的导数是：</p>
<script type="math/tex; mode=display">
令y=sigmoid(x)\\
y=\frac{1}{1+e^{-x}} \\
u(x)=1+e^{-x} \\
g(x)=e^{-x}\\
k(x)=-x\\
\frac{\mathrm{d} y}{\mathrm{d} x} = \frac{\mathrm{d} y}{\mathrm{d} u}\cdot \frac{\mathrm{d} u}{\mathrm{d} g}\cdot \frac{\mathrm{d} g}{\mathrm{d} k}\cdot \frac{\mathrm{d} k}{\mathrm{d} x}=-1\cdot u^{-2}\cdot 1\cdot e^{-x}\cdot -1=u^{-2}\cdot e^{-x}=\frac{e^{-x}}{(1+e^{-x})^{2}}=\frac{1}{1+e^{-x}}-\frac{1}{(1+e^{-x})^{2}}=y(1-y)\\
则y^{'}=y(1-y)</script><p>可以看到，sigmoid函数的导数非常有趣，它可以用sigmoid函数自身来表示。这样，一旦计算出sigmoid函数的值，计算它的导数的值就非常方便。</p>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p><img src="/2021/06/01/deep-learning-tutorial/神经网络图2.png" alt></p>
<p>神经网络其实就是按照<strong>一定规则</strong>连接起来的多个<strong>神经元</strong>。上图展示了一个<strong>全连接(full connected, FC)</strong>神经网络，通过观察上面的图，我们可以发现它的规则包括：</p>
<ul>
<li>神经元按照层来布局。最左边的层叫做<strong>输入层</strong>，负责接收输入数据；最右边的层叫<strong>输出层</strong>，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做<strong>隐藏层</strong>，因为它们对于外部来说是不可见的。</li>
<li>同一层的神经元之间没有连接。</li>
<li>第N层的每个神经元和第N-1层的<strong>所有</strong>神经元相连(这就是full connected的含义)，第N-1层神经元的输出就是第N层神经元的输入。</li>
<li>每个连接都有一个<strong>权值</strong>。</li>
</ul>
<p>上面这些规则定义了全连接神经网络的结构。事实上还存在很多其它结构的神经网络，比如卷积神经网络(CNN)、循环神经网络(RNN)，他们都具有不同的连接规则。</p>
<h3 id="计算神经网络输出"><a href="#计算神经网络输出" class="headerlink" title="计算神经网络输出"></a>计算神经网络输出</h3><p>神经网络实际上就是一个输入向量<script type="math/tex">\vec{x}</script>到输出向量<script type="math/tex">\vec{y}</script>的函数，即：</p>
<script type="math/tex; mode=display">
\vec{y}=f_{network}(\vec{x})</script><p>根据输入计算神经网络的输出，需要首先将输入向量<script type="math/tex">\vec{x}</script>的每个元素<script type="math/tex">x_{i}</script>的值赋给神经网络的输入层的对应神经元，然后根据<strong>式1</strong>依次向前计算每一层的每个神经元的值，直到最后一层输出层的所有神经元的值计算完毕。最后，将输出层每个神经元的值串在一起就得到了输出向量<script type="math/tex">\vec{y}</script>。</p>
<p>接下来举一个例子来说明这个过程，我们先给神经网络的每个单元写上编号。</p>
<p><img src="/2021/06/01/deep-learning-tutorial/神经网络过程图.png" alt></p>
<p>如上图，输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的4个节点，编号依次为4、5、6、7；最后输出层的两个节点编号为8、9。因为我们这个神经网络是<strong>全连接</strong>网络，所以可以看到每个节点都和<strong>上一层的所有节点</strong>有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为<script type="math/tex">w_{41},w_{42},w_{43}</script>。那么，我们怎样计算节点4的输出值<script type="math/tex">a_{4}</script>呢？</p>
<p>为了计算节点4的输出值，我们必须先得到其所有上游节点（也就是节点1、2、3）的输出值。节点1、2、3是<strong>输入层</strong>的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点1、2、3的输出值分别是<script type="math/tex">x_{1},x_{2},x_{3}</script>。我们要求<strong>输入向量的维度和输入层神经元个数相同</strong>，而输入向量的某个元素对应到哪个输入节点是可以自由决定的，你偏非要把赋值给节点2也是完全没有问题的，但这样除了把自己弄晕之外，并没有什么价值。</p>
<p>一旦我们有了节点1、2、3的输出值，我们就可以根据<strong>式1</strong>计算节点4的输出值<script type="math/tex">a_{4}</script>：</p>
<script type="math/tex; mode=display">
a_{4}=sigmoid(w_{41}x_{1}+w_{42}x_{2}+w_{43}x_{3}+w_{4b})</script><p>上式的<script type="math/tex">w_{4b}</script>是节点4的<strong>偏置项</strong>，图中没有画出来。而分别为节点1、2、3到节点4连接的权重，在给权重<script type="math/tex">w_{ji}</script>编号时，我们把目标节点<script type="math/tex">j</script>的编号放在前面，把源节点<script type="math/tex">i</script>的编号放在后面。</p>
<p>同样，我们可以继续计算出节点5、6、7的输出值<script type="math/tex">a_{5},a_{6},a_{7}</script>。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值<script type="math/tex">y_{1}</script>：</p>
<script type="math/tex; mode=display">
y_{1}=sigmoid(w_{84}a_{4}+w_{85}a_{5}+w_{86}a_{6}+w_{87}a_{7}+w_{8b})</script><p>同理，我们还可以计算出<script type="math/tex">y_{2}</script>的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量<script type="math/tex">\vec{x}=\begin{bmatrix}
x_{1}\\ 
x_{2}\\ 
x_{3}
\end{bmatrix}</script>时，神经网络的输出向量<script type="math/tex">\vec{y}=\begin{bmatrix}
y_{1}\\ 
y_{2}\\ 
\end{bmatrix}</script>。这里我们也看到，<strong>输出向量的维度和输出层神经元个数相同</strong>。</p>
<h3 id="神经网络的矩阵表示"><a href="#神经网络的矩阵表示" class="headerlink" title="神经网络的矩阵表示"></a>神经网络的矩阵表示</h3><p>神经网络的计算如果用矩阵来表示会很方便（当然逼格也更高），我们先来看看隐藏层的矩阵表示。</p>
<p>首先我们把隐藏层4个节点的计算依次排列出来：</p>
<script type="math/tex; mode=display">
a_{4}=sigmoid(w_{41}x_{1}+w_{42}x_{2}+w_{43}x_{3}+w_{4b})\\
a_{5}=sigmoid(w_{51}x_{1}+w_{52}x_{2}+w_{53}x_{3}+w_{5b})\\
a_{6}=sigmoid(w_{61}x_{1}+w_{62}x_{2}+w_{63}x_{3}+w_{6b})\\
a_{7}=sigmoid(w_{71}x_{1}+w_{72}x_{2}+w_{73}x_{3}+w_{7b})</script><p>接着，定义网络的输入向量<script type="math/tex">\vec{x}</script>和隐藏层每个节点的权重向量<script type="math/tex">\vec{w_{j}}</script>。令</p>
<script type="math/tex; mode=display">
\vec{x}=\begin{bmatrix}
x_{1}\\ 
x_{2}\\ 
x_{3}
\end{bmatrix}\\
\vec{w_{4}}=\begin{bmatrix}
w_{41}, &w_{42},  & w_{43}, & w_{4b}
\end{bmatrix}\\
\vec{w_{5}}=\begin{bmatrix}
w_{51}, &w_{52},  & w_{53}, & w_{5b}
\end{bmatrix}\\
\vec{w_{6}}=\begin{bmatrix}
w_{61}, &w_{62},  & w_{63}, & w_{6b}
\end{bmatrix}\\
\vec{w_{7}}=\begin{bmatrix}
w_{71}, &w_{72},  & w_{73}, & w_{7b}
\end{bmatrix}\\
f=sigmoid</script><p>代入到前面的一组式子，得到：</p>
<script type="math/tex; mode=display">
a_{4}=f(\vec{w_{4}}\cdot \vec{x})\\
a_{5}=f(\vec{w_{5}}\cdot \vec{x})\\
a_{6}=f(\vec{w_{6}}\cdot \vec{x})\\
a_{7}=f(\vec{w_{7}}\cdot \vec{x})\\</script><p>现在，我们把上述计算<script type="math/tex">a_{4},a_{5},a_{6},a_{7}</script>的四个式子写到一个矩阵里面，每个式子作为矩阵的一行，就可以利用矩阵来表示它们的计算了。令</p>
<script type="math/tex; mode=display">
\vec{a}=\begin{bmatrix}
a_{4}\\ 
a_{5}\\ 
a_{6}\\ 
a_{7}
\end{bmatrix}\\
W=\begin{bmatrix}
\vec{w_{4}}\\ 
\vec{w_{5}}\\ 
\vec{w_{6}}\\ 
\vec{w_{7}}
\end{bmatrix}=\begin{bmatrix}
w_{41} &w_{42}  & w_{43} &w_{4b} \\ 
w_{51} &w_{52}  & w_{53} &w_{5b} \\ 
w_{61} &w_{62}  & w_{63} &w_{6b} \\ 
w_{71} &w_{72}  & w_{73} &w_{7b} 
\end{bmatrix}\\
f(\begin{bmatrix}
x_{1}\\ 
x_{2}\\ 
x_{3}\\ 
.\\ 
.\\ 
.
\end{bmatrix})=\begin{bmatrix}
f(x_{1})\\ 
f(x_{2})\\ 
f(x_{3})\\ 
.\\ 
.\\ 
.
\end{bmatrix}</script><p>带入前面的一组式子，得到</p>
<script type="math/tex; mode=display">
\vec{a}=f(W\cdot\vec{x}) \tag2</script><p>在<strong>式2</strong>中，是激活函数，在本例中是sigmoid函数；W是某一层的权重矩阵；<script type="math/tex">\vec{x}</script>是某层的输入向量；<script type="math/tex">\vec{a}</script>是某层的输出向量。<strong>式2</strong>说明神经网络的每一层的作用实际上就是先将输入向量<strong>左乘</strong>一个数组进行线性变换，得到一个新的向量，然后再对这个向量<strong>逐元素</strong>应用一个激活函数。</p>
<p>每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重矩阵分别为<script type="math/tex">W_{1},W_{2},W_{3},W_{4}</script>，每个隐藏层的输出分别是<script type="math/tex">\vec{a_{1}},\vec{a_{2}},\vec{a_{3}}</script>，神经网络的输入为<script type="math/tex">\vec{x}</script>，神经网络的输出为<script type="math/tex">\vec{y}</script>，如下图所示：</p>
<p><img src="/2021/06/01/deep-learning-tutorial/深层神经网络.png" alt></p>
<p>则每一层的输出向量的计算可以表示为：</p>
<script type="math/tex; mode=display">
\vec{a_{1}}=f(W_{1}\cdot \vec{x})\\
\vec{a_{2}}=f(W_{2}\cdot \vec{a_{1}})\\
\vec{a_{3}}=f(W_{3}\cdot \vec{a_{2}})\\
\vec{y}=f(W_{4}\cdot \vec{a_{3}})</script><p>这就是神经网络输出值的计算方法。</p>
<h3 id="神经网络的训练"><a href="#神经网络的训练" class="headerlink" title="神经网络的训练"></a>神经网络的训练</h3><p>现在，我们需要知道一个神经网络的每个连接上的权值是如何得到的。我们可以说神经网络是一个<strong>模型</strong>，那么这些权值就是模型的<strong>参数</strong>，也就是模型要学习的东西。然而，一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为<strong>超参数(Hyper-Parameters)</strong>。</p>
<p>接下来，我们将要介绍神经网络的训练算法：反向传播算法。</p>
<h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p>我们首先直观的介绍反向传播算法，最后再来介绍这个算法的推导。当然读者也可以完全跳过推导部分，因为即使不知道如何推导，也不影响你写出来一个神经网络的训练代码。事实上，现在神经网络成熟的开源实现多如牛毛，除了练手之外，你可能都没有机会需要去写一个神经网络。</p>
<p>我们设神经元的激活函数f为函数sigmoid函数。</p>
<p>我们假设每个训练样本为<script type="math/tex">(\vec{x},\vec{t})</script>，其中向量<script type="math/tex">\vec{x}</script>是训练样本的特征，而<script type="math/tex">\vec{t}</script>是样本的目标值。</p>
<p><img src="/2021/06/01/deep-learning-tutorial/反向传播.png" alt></p>
<p>首先，我们根据上一节介绍的算法，用样本的特征<script type="math/tex">\vec{x}</script>，计算出神经网络中每个隐藏层节点的输出<script type="math/tex">a_{i}</script>，以及输出层每个节点的输出<script type="math/tex">y_{i}</script>。</p>
<p>我们取网络所有输出层节点的误差平方和作为目标函数：</p>
<script type="math/tex; mode=display">
E_{d}=\frac{1}{2}\sum_{i=1}^{m}(t_{i}-y_{i})^{2}</script><p>其中<script type="math/tex">m</script>表示输出节点数目，<script type="math/tex">E_{d}</script>表示样本d的误差。</p>
<p>然后，我们用<strong>随机梯度下降</strong>算法对目标函数进行优化：</p>
<script type="math/tex; mode=display">
w_{ji}\leftarrow w_{ji}-\eta\frac{\partial E_{d}}{\partial w_{ji}}</script><p>随机梯度下降算法也就是需要求出误差对于每个权重的偏导数（也就是梯度），怎么求呢？</p>
<p><img src="/2021/06/01/deep-learning-tutorial/神经网络过程图.png" alt></p>
<p>观察上图，我们发现权重<script type="math/tex">w_{ji}</script>仅能通过影响节点<script type="math/tex">j</script>的输入值影响网络的其它部分，设<script type="math/tex">net_{j}</script>是节点<script type="math/tex">j</script>的<strong>加权输入</strong>，即</p>
<script type="math/tex; mode=display">
net_{j}=\vec{w_{j}}\cdot \vec{x_{j}}=\sum_{i=1}^{n}w_{ji}x_{ji}</script><script type="math/tex; mode=display">E_{d}$$是$$net_{j}$$的函数，而$$net_{j}$$是$$w_{ji}$$的函数。根据链式求导法则，可以得到：</script><p>\frac{\partial E_{d}}{\partial w_{ji}}=\frac{\partial E_{d}}{\partial net_{j}}\frac{\partial net_{j}}{\partial w_{ji}}=\frac{\partial E_{d}}{\partial net_{j}}\frac{\partial \sum _{i}w_{ji}x_{ji}}{\partial w_{ji}}=\frac{\partial E_{d}}{\partial net_{j}}x_{ji}</p>
<script type="math/tex; mode=display">
上式中，$$x_{ji}$$是节点$$i$$传递给节点$$j$$的输入值，也就是节点$$i$$的输出值。

对于$$\frac{\partial E_{d}}{\partial net_{j}}$$的推导，需要区分**输出层**和**隐藏层**两种情况。

#### 输出层权值训练

对于**输出层**来说，$$net_{j}$$仅能通过节点$$j$$的输出值$$y_{j}$$来影响网络其它部分，也就是说$$E_{d}$$是$$y_{j}$$的函数，而$$y_{j}$$是$$net_{j}$$的函数，其中$$y_{j}=sigmoid(net_{j})$$。所以我们可以再次使用链式求导法则：</script><p>\frac{\partial E_{d}}{\partial net_{j}}=\frac{\partial E_{d}}{\partial y_{j}}\frac{\partial y_{j}}{\partial net_{j}}</p>
<script type="math/tex; mode=display">
考虑上式第一项:</script><p>\frac{\partial E_{d}}{\partial y_{j}}=\frac{\partial \frac{1}{2}\sum (t_{j}-y_{j})^{2}}{\partial y_{j}}=-(t_{j}-y_{j})</p>
<script type="math/tex; mode=display">
考虑上式第二项：</script><p>\frac{\partial y_{j}}{\partial net_{j}}=\frac{\partial sigmoid(net_{j})}{\partial net_{j}}=y_{j}(1-y_{j})</p>
<script type="math/tex; mode=display">
将第一项和第二项带入，得到：</script><p>\frac{\partial E_{d}}{\partial net_{j}}=-(t_{j}-y_{j})y_{j}(1-y_{j})</p>
<script type="math/tex; mode=display">
如果令$$\delta _{j}=-\frac{\partial E_{d}}{\partial net_{j}}$$，也就是一个节点的误差项是网络误差$$\delta$$对这个节点输入的偏导数的相反数。带入上式，得到：</script><p>\delta _{j}=(t_{j}-y_{j})y_{j}(1-y_{j})</p>
<script type="math/tex; mode=display">
将上述推导带入随机梯度下降公式，得到：</script><p>w_{ji}\leftarrow w_{ji}-\eta \frac{\partial E_{d}}{\partial w_{ji}}=w_{ji}-\eta \frac{\partial E_{d}}{\partial net_{ji}}\frac{\partial net_{ji}}{\partial w_{ji}}=w_{ji}+\eta (t_{j}-y_{j})y_{j}(1-y_{j})(1-y_{j})x_{ji}=w_{ji}+\eta \delta _{j}x_{ji}</p>
<script type="math/tex; mode=display">

#### 隐藏层权值训练

现在我们要推导出隐藏层的$$\frac{\partial E_{d}}{\partial net_{j}}$$。

首先，我们需要定义节点$$j$$的所有直接下游节点的集合$$Downstream(j)$$。例如，对于节点4来说，它的直接下游节点是节点8、节点9。可以看到$$net_{j}$$只能通过影响$$Downstream(j)$$再影响$$E_{d}$$。设$$net_{k}$$是节点$$j$$的下游节点的输入，则$$E_{d}$$是$$net_{k}$$的函数，而$$net_{k}$$是$$net_{j}$$的函数。因为$$net_{k}$$有多个，我们应用全导数公式，可以做出如下推导：</script><p>\frac{\partial E_{d}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}\frac{\partial E_{d}}{\partial net_{k}}\frac{\partial net_{k}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}-\delta _{k}\frac{\partial net_{k}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}-\delta _{k}\frac{\partial net_{k}}{\partial a_{j}}\frac{\partial a_{j}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}-\delta _{k}w_{kj}\frac{\partial a_{j}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}-\delta _{k}w_{kj}a_{j}(1-a_{j})\\<br>=-a_{j}(1-a_{j})\sum_{k\in Downstream(j)}\delta _{k}w_{kj}</p>
<script type="math/tex; mode=display">
因为$$\delta _{j}=-\frac{\partial E_{d}}{\partial net_{j}}$$带入上式得到：</script><p>\delta _{j}=a_{j}(1-a_{j})\sum_{k\in Downstream(j)}\delta _{k}w_{kj}</p>
<script type="math/tex; mode=display">
其中$$a_{j}$$为激活函数。

## 卷积神经网络

### 全连接网络VS卷积网络

全连接神经网络之所以不太适合图像识别任务，主要有以下几个方面的问题：

- **参数数量太多** 考虑一个输入$$1000*1000$$像素的图片(一百万像素，现在已经不能算大图了)，输入层有$$1000*1000=100$$万节点。假设第一个隐藏层有100个节点(这个数量并不多)，那么仅这一层就有$$(1000*1000+1)*100=1$$亿​参数，这实在是太多了！我们看到图像只扩大一点，参数数量就会多很多，因此它的扩展性很差。
- **没有利用像素之间的位置信息** 对于图像识别任务来说，每个像素和其周围像素的联系是比较紧密的，和离得很远的像素的联系可能就很小了。如果一个神经元和上一层所有神经元相连，那么就相当于对于一个像素来说，把图像的所有像素都等同看待，这不符合前面的假设。当我们完成每个连接权重的学习之后，最终可能会发现，有大量的权重，它们的值都是很小的(也就是这些连接其实无关紧要)。努力学习大量并不重要的权重，这样的学习必将是非常低效的。
- **网络层数限制** 我们知道网络层数越多其表达能力越强，但是通过梯度下降方法训练深度全连接神经网络很困难，因为全连接神经网络的梯度很难传递超过3层。因此，我们不可能得到一个很深的全连接神经网络，也就限制了它的能力。

那么，卷积神经网络又是怎样解决这个问题的呢？主要有三个思路：

- **局部连接** 这个是最容易想到的，每个神经元不再和上一层的所有神经元相连，而只和一小部分神经元相连。这样就减少了很多参数。
- **权值共享** 一组连接可以共享同一个权重，而不是每个连接有一个不同的权重，这样又减少了很多参数。
- **下采样** 可以使用Pooling来减少每层的样本数，进一步减少参数数量，同时还可以提升模型的鲁棒性。

对于图像识别任务来说，卷积神经网络通过尽可能保留重要的参数，去掉大量不重要的参数，来达到更好的学习效果。

### 卷积神经网络结构

![](./deep-learning-tutorial/卷积神经网络案例图.png)

三维的层结构

从**图中**我们可以发现**卷积神经网络**的层结构和**全连接神经网络**的层结构有很大不同。**全连接神经网络**每层的神经元是按照**一维**排列的，也就是排成一条线的样子；而**卷积神经网络**每层的神经元是按照**三维**排列的，也就是排成一个长方体的样子，有**宽度**、**高度**和**深度**。

对于**图中**展示的神经网络，我们看到输入层的宽度和高度对应于输入图像的宽度和高度，而它的深度为1。接着，第一个卷积层对这幅图像进行了卷积操作(后面我们会讲如何计算卷积)，得到了三个Feature Map。这里的"3"可能是让很多初学者迷惑的地方，实际上，就是这个卷积层包含三个Filter，也就是三套参数，每个Filter都可以把原始输入图像卷积得到一个Feature Map，三个Filter就可以得到三个Feature Map。至于一个卷积层可以有多少个Filter，那是可以自由设定的。也就是说，卷积层的Filter个数也是一个**超参数**。我们可以把Feature Map可以看做是通过卷积变换提取到的图像特征，三个Filter就对原始图像提取出三组不同的特征，也就是得到了三个Feature Map，也称做三个**通道(channel)**。

继续观察**图**，在第一个卷积层之后，Pooling层对三个Feature Map做了**下采样**(后面我们会讲如何计算下采样)，得到了三个更小的Feature Map。接着，是第二个**卷积层**，它有5个Filter。每个Fitler都把前面**下采样**之后的**3个\**Feature Map**卷积**在一起，得到一个新的Feature Map。这样，5个Filter就得到了5个Feature Map。接着，是第二个Pooling，继续对5个Feature Map进行**下采样**，得到了5个更小的Feature Map。

如图所示网络的最后两层是全连接层。第一个全连接层的每个神经元，和上一层5个Feature Map中的每个神经元相连，第二个全连接层(也就是输出层)的每个神经元，则和第一个全连接层的每个神经元相连，这样得到了整个网络的输出。

至此，我们对**卷积神经网络**有了最基本的感性认识。接下来，我们将介绍**卷积神经网络**中各种层的计算和训练。

### 卷积神经网络输出值的计算

#### 卷积层输出值的计算

我们用一个简单的例子来讲述如何计算**卷积**，然后，我们抽象出**卷积层**的一些重要概念和计算方法。

假设有一个5*5的图像，使用一个3*3的filter进行卷积，想得到一个3*3的Feature Map，如下所示：

![](./deep-learning-tutorial/卷积神经网络案例.png)

为了清楚的描述**卷积**计算过程，我们首先对图像的每个像素进行编号，用$$x_{i,j}$$表示图像的第行第列元素；对filter的每个权重进行编号，用$$w_{m,n}$$表示第$$m$$行第$$n$$列权重，用$$w_{b}$$表示filter的**偏置项**；对Feature Map的每个元素进行编号，用$$a_{i,j}$$表示Feature Map的第$$i$$行第$$j$$列元素；用$$f$$表示**激活函数**(这个例子选择**relu函数**作为激活函数)。然后，使用下列公式计算卷积：</script><p>a_{i,j}=f(\sum_{m=0}^{2}\sum_{n=0}^{2}w_{m,n}x_{i+m,j+n}+w_{b})</p>
<script type="math/tex; mode=display">
例如，对于Feature Map左上角元素来说，其卷积计算方法为：</script><p>a_{0,0}=f(\sum_{m=0}^{2}\sum_{n=0}^{2}w_{m,n}x_{m+0,n+0}+w_{b})\\<br>=relu(w_{0,0}x_{0,0}+w_{0,1}x_{0,1}+w_{0,2}x_{0,2}+w_{1,0}x_{1,0}+w_{1,1}x_{1,1}+w_{1,2}x_{1,2}+w_{2,0}x_{2,0}+w_{2,1}x_{2,1}+w_{2})\\<br>=relu(1+0+1+0+1+0+0+0+1+0)=relu(4)=4</p>
<script type="math/tex; mode=display">
计算结果如下图所示：

![](./deep-learning-tutorial/卷积过程.png)

###  权值共享

神经元的偏置部分也是同一种滤波器共享的。 比如卷积核是三层，每一层使用的偏置项都是相等的。

## 对抗生成网络(GAN)



## 优化器

### Batch Gradient Descent(BGD，批量梯度下降)

BGD训练过程中每次迭代使用所有样本来进行梯度的更新。

#### 优点

- 一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行；
- 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。

#### 缺点

- 当样本数目很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。

### Stochastic Gradient Descent(SGD,随机梯度下降)

SGD训练过程中每次迭代使用一个样本来对参数进行更新。

#### 优点

- 由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。

#### 缺点

- 准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛；
- 可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势；
- 不易于并行实现。

### Mini-Batch Gradient Descent(MBGD,小批量梯度下降)

MBGD训练过程中每次迭代使用**batch size**个样本来对参数进行更新。它是对BGD以及SGD的一个折中办法。

#### 优点

- 通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多；
- 每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果；
- 可实现并行化。

#### 缺点

- batch size的不当选择可能会带来一些问题。

  batcha size的选择带来的影响：

  - 在合理的范围内，增大batch_size的好处：

    - 内存利用率提高了，大矩阵乘法的并行化效率提高。

    - 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。

    - 在一定范围内，一般来说 batch size 越大，其确定的下降方向越准，引起训练震荡越小。

  - 盲目增大batch size的坏处

    - 内存利用率提高了，但是内存容量可能撑不住了。
    - 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。
    - batch size 增大到一定程度，其确定的下降方向已经基本不再变化



### Momentum

[]: https://blog.csdn.net/willduan1/article/details/78070086

SGD参数更新公式为：</script><p>W:=W-\alpha d_{w}\\<br>b:=b-\alpha d_{b}</p>
<script type="math/tex; mode=display">
它的梯度更新路线上下波动很大，收敛速度很慢。因此根据这些原因，有人提出了Momentum优化算法，这个是基于SGD的，简单理解就是为了防止波动，它主要是基于梯度的移动指数加权平均来更新参数。引进超参数beta(一般取0.9)。

在讲这个算法之前说一下移动指数加权平均。移动指数加权平均法加权就是根据同一个移动段内不同时间的数据对预测值的影响程度，分别给予不同的权数，然后再进行平均移动以预测未来值。假定给定一系列数据值$$x_{1}, x_{2},x_{3},……x_{n}$$。那么，我们根据这些数据来拟合一条曲线，所得的值$$v_{1}, v_{2}…..$$就是如下的公式：</script><p>v_{0} = 0\\<br>v_{1} = \beta v_{0}+(1-\beta )x_{0} \\<br>v_{2} = \beta v_{1}+(1-\beta )x_{1} \\</p>
<script type="math/tex; mode=display">
参数更新公式为：</script><p>V_{dw}=\beta V_{dw}+(1-\beta )dW\\<br>V_{db}=\beta V_{db}+(1-\beta )db\\<br>W:=W-\alpha {V_{dw}}\\<br>b:=b-\alpha {V_{db}}</p>
<script type="math/tex; mode=display">

### Nesterov Momentum

Nesterov Momentum是对Momentum的改进，可以理解为nesterov动量在标准动量方法中添加了一个**校正因子**。

### Root Mean Square Prop(RMSProp)

RMSProp思想与Momentum相似，也用到权重超参数beta（一般取0.999）。

参数更新公式为：</script><p>S_{dw}=\beta S_{dw}+(1-\beta )dW^{2}\\<br>S_{db}=\beta S_{db}+(1-\beta )db^{2}\\<br>W:=W-\alpha \frac{dW}{\sqrt{S_{dw}}}\\<br>b:=b-\alpha \frac{db}{\sqrt{S_{db}}}</p>
<script type="math/tex; mode=display">
为了防止分母为0，在分数下加上个特别小的值epsilon，通常选取10^-8。

### Adagrad

大多数优化器训练参数更新过程中都使用了相同的学习率α。Adagrad能够在训练中自动的对learning rate进行调整，对于出现频率较低的参数采用较大的α更新，相反，对于出现频率较高的参数采用较小的α更新。因此，**Adagrad非常适合处理稀疏数据**。

如果是普通的SGD，那么每一时刻梯度的更新公式为：</script><p>\Theta _{t+1}=\Theta _{t,i}-\alpha *g_{t,i}</p>
<script type="math/tex; mode=display">
g<sub>t,i</sub>为第t轮第i个参数的梯度。θ<sub>t,i</sub> 为参数值

Adagrad在每轮训练中对每个参数θ<sub>i</sub> 进行更新，参数更新公式为：</script><p>\Theta _{t+1,i}=\Theta _{t,i}-\frac{\alpha }{\sqrt{G_{t,ii}+\varepsilon }}*g_{t,i}</p>
<script type="math/tex; mode=display">
G<sub>t</sub>为对角矩阵，大小为D*D。每个对角线位置i,i为对应参数θ<sub>i</sub> 从第一轮到第t轮梯度的平方和。varepsilon 是平滑项，用于避免分母为0，一般取值为10^-8。Adagrad的缺点是在训练的中后期，分母上梯度平方的累加会越来越大，从而梯度趋近于0，使得训练提前结束。

### Adadelta

Adadelta 是 Adagrad 的一个具有更强鲁棒性的的扩展版本，它不是累积所有过去的梯度，而是根据渐变更新的移动窗口调整学习速率。 这样，即使进行了许多更新，Adadelta 仍在继续学习。   

与Adagrad 相比，就是分母的 G 换成了过去的梯度平方的衰减平均值，**指数衰减平均值**。

这个分母相当于**梯度的均方根 root mean squared (RMS)**，在数据统计分析中，将所有值平方求和，求其均值，再开平方，就得到均方根值 。

#### 优点

- 防止学习率衰减或梯度消失等问题的出现。

### Adam

该优化器相当于RMSprop+Momentum。

参数更新公式为：</script><p>V_{dw}=\beta _{1} V_{dw}+(1-\beta _{1} )dW\\<br>V_{db}=\beta _{1} V_{db}+(1-\beta _{1} )db\\<br>S_{dw}=\beta_{2} S_{dw}+(1-\beta_{2} )dW^{2}\\<br>S_{db}=\beta_{2} S_{db}+(1-\beta_{2} )db^{2}\\<br>V_{dW}^{corrected}=\frac{V_{dW}}{1-\beta _{1}^{t}}\\<br>V_{db}^{corrected}=\frac{V_{db}}{1-\beta _{1}^{t}}\\<br>S_{dW}^{corrected}=\frac{S_{dW}}{1-\beta _{2}^{t}}\\<br>S_{db}^{corrected}=\frac{S_{db}}{1-\beta _{2}^{t}}\\<br>W:=W-\alpha \frac{V_{dW}}{\sqrt{S_{dW}^{corrected}}}\\<br>b:=b-\alpha \frac{V_{db}}{\sqrt{S_{db}^{corrected}}}</p>
<script type="math/tex; mode=display">
beta1一般为0.9，beta2一般为0.9999。在实际应用中，Adam方法效果良好。与其他自适应学习率算法相比，其收敛速度更快，学习效果更为有效，而且可以纠正其他优化技术中存在的问题，如学习率消失、收敛过慢或是高方差的参数更新导致损失函数波动较大等问题。

## 激活函数

[激活函数详解](<https://zhuanlan.zhihu.com/p/22142013>)

激活函数一般用于神经网络的层与层之间，将上一层的输出转换之后输入到下一层。如果没有激活函数引入的额非线性特性，那么神经网络就只相当于原始感知机的矩阵相乘。  

### Sigmoid

![](./deep-learning-tutorial/sigmoid.jpg)

sigmoid在定义域内处处可导，且两侧导数逐渐趋近于0。

#### 缺点

- 激活函数计算量大，反向传播求误差梯度时，求导涉及除法；
- 反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练；
- Sigmoids函数饱和且kill掉梯度；
- Sigmoids函数收敛缓慢。

### tanh

![](./deep-learning-tutorial/tanh.jpg)

### Relu

![](./deep-learning-tutorial/relu.jpg)

#### 优点

- **速度快** 和sigmoid函数需要计算指数和倒数相比，relu函数其实就是一个max(0,x)，计算代价小很多。
- **减轻梯度消失问题** relu函数的导数是1，不会导致梯度变小。当然，激活函数仅仅是导致梯度减小的一个因素，但无论如何在这方面relu的表现强于sigmoid。使用relu激活函数可以让你训练更深的网络。
- **稀疏性** 通过对大脑的研究发现，大脑在工作的时候只有大约5%的神经元是激活的，而采用sigmoid激活函数的人工神经网络，其激活率大约是50%。有论文声称人工神经网络在15%-30%的激活率时是比较理想的。因为relu函数在输入小于0时是完全不激活的，因此可以获得一个更低的激活率。

### 缺点

- 训练的时候很”脆弱”，很容易就”die”了。

### PRelu

![](./deep-learning-tutorial/PRelu.jpg)

### RReLU

### Maxout

### ELU

![](./deep-learning-tutorial/elu.jpg)



## 问题及解决方法

### 梯度爆炸与梯度消失

深度神经网络训练过程中，使用了反向传播的方式更新参数，该方式基于的是链式求导。计算每层梯度的时候回设计一些连乘操作，如果网络过深，当连乘的因子大部分小于1时，最后的乘积结果可能趋于0，会导致后面的网络层参数不发生变化，不能继续进行学习（梯度消失）。当连乘的因子大部分大于1，最后的乘积结果可能趋于无穷，会导致后面的网络层参数变化过大，导致Loss值出现震荡，收敛不到最低值的情况（梯度爆炸）。

#### 梯度爆炸解决办法

- 降低学习率

- 梯度裁剪（Gradient Clipping）

  如果梯度特别大，那么将其投影到一个比较小的尺度上。

### 线性与非线性

在数学上可理解为一阶导数为常数的函数为线性函数，一阶导数不为常数的函数为非线性函数。

### 为什么RNN一般情况下为等长的

为了让多条数据合并成矩阵进行运算，能够使用并行处理。如果不等长则不能合并为矩阵。tensorflow支持同一批训练数据等长的训练接口。

### Padding 等于SAME和VALID

Padding运算作用于输入向量的每一维，每一维的操作都是一致的，所以理解Padding的操作，只需要理解一维向量的padding过程

假设一个一维向量，输入形状为input_size，经过滤波操作后的输出形状为output_size，滤波窗口为filter_size，需要padding的个数为padding_needed，滤波窗口滑动步长为stride，则之间满足关系：</script><p>output_size=(input_size+padding_needed-filter_size)/stride+1</p>
<p>$$<br>由公式可知，指定padding_needed可以确定output_size的值，反过来，如果已知输出的形状，则进而可以确定padding的数量。</p>
<p>这是两种处理padding的方案，pytorch采用的是第一种，即在卷积或池化时先确定padding数量，自动推导输出形状；tensorflow和caffe采用的是更为人熟知的第二种，即先根据Valid还是Same确定输出大小，再自动确定padding的数量</p>
<p>Valid和Same是预设的两种padding模式，Valid指不padding，same指输出大小尽可能和输入大小成比例</p>
<p>下面是tensorflow计算padding的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">作者：JamesPlur</span><br><span class="line">链接：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;73118626</span><br><span class="line">来源：知乎</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br><span class="line"></span><br><span class="line">void GetWindowedOutputSize(int64_t input_size, int32_t filter_size, int32_t dilation_rate,</span><br><span class="line">                           int32_t stride, const std::string&amp; padding_type, </span><br><span class="line">                           int64_t* output_size,int32_t* padding_before, </span><br><span class="line">                           int32_t* padding_after) &#123;</span><br><span class="line">  CHECK_GT(stride, 0);</span><br><span class="line">  CHECK_GE(dilation_rate, 1);</span><br><span class="line"></span><br><span class="line">  int32_t effective_filter_size &#x3D; (filter_size - 1) * dilation_rate + 1;</span><br><span class="line">  if (padding_type &#x3D;&#x3D; &quot;valid&quot;) &#123;</span><br><span class="line">    if (output_size) &#123; *output_size &#x3D; (input_size - effective_filter_size + stride) &#x2F; stride; &#125;</span><br><span class="line">    if (padding_before) &#123; *padding_before &#x3D; 0; &#125;</span><br><span class="line">    if (padding_after) &#123; *padding_after &#x3D; 0; &#125;</span><br><span class="line">  &#125; else if (padding_type &#x3D;&#x3D; &quot;same&quot;) &#123;</span><br><span class="line">    int64_t tmp_output_size &#x3D; (input_size + stride - 1) &#x2F; stride;</span><br><span class="line">    if (output_size) &#123; *output_size &#x3D; tmp_output_size; &#125;</span><br><span class="line">    const int32_t padding_needed &#x3D; std::max(</span><br><span class="line">        0,</span><br><span class="line">        static_cast&lt;int32_t&gt;((tmp_output_size - 1) * stride + effective_filter_size - input_size));</span><br><span class="line">    &#x2F;&#x2F; For odd values of total padding, add more padding at the &#39;right&#39;</span><br><span class="line">    &#x2F;&#x2F; side of the given dimension.</span><br><span class="line">    if (padding_before) &#123; *padding_before &#x3D; padding_needed &#x2F; 2; &#125;</span><br><span class="line">    if (padding_after) &#123; *padding_after &#x3D; padding_needed - padding_needed &#x2F; 2; &#125;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    UNIMPLEMENTED();</span><br><span class="line">  &#125;</span><br><span class="line">  if (output_size) &#123; CHECK_GE((*output_size), 0); &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h2><h2 id="AlexNet深度卷积神经网络"><a href="#AlexNet深度卷积神经网络" class="headerlink" title="AlexNet深度卷积神经网络"></a>AlexNet深度卷积神经网络</h2><h3 id="深度卷积神经网络图像集分类"><a href="#深度卷积神经网络图像集分类" class="headerlink" title="深度卷积神经网络图像集分类"></a>深度卷积神经网络图像集分类</h3><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>我们训练了一个庞大的深层卷积神经网络，将ImageNet LSVRC-2010比赛中的120万张高分辨率图像分为1000个不同的类别。在测试数据上，我们取得了37.5％和17.0％的前1和前5的错误率，这比以前的先进水平要好得多。具有6000万个参数和650,000个神经元的神经网络由五个卷积层组成，其中一些随后是最大池化层，三个全连接层以及最后的1000个softmax输出。为了加快训练速度，我们使用非饱和神经元和能高效进行卷积运算的GPU实现。为了减少全连接层中的过拟合，我们采用了最近开发的称为“dropout”的正则化方法，该方法证明是非常有效的。我们还在ILSVRC-2012比赛中使用了这种模式的一个变种，取得了15.3％的前五名测试失误率，而第二名的成绩是26.2％。</p>
<h4 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h4><p>目前，机器学习方法对物体识别非常重要。为了改善他们的表现，我们可以收集更大的数据集，训练更强大的模型，并使用更好的技术来防止过拟合。直到最近，标记好图像的数据集相对还较小——大约上万的数量级（例如，NORB [16]，Caltech-101/256 [8,9]和CIFAR-10/100 [12]）。使用这种规模的数据集可以很好地解决简单的识别任务，特别是如果他们增加了保留标签转换（label-preserving transformations）。例如，目前MNIST数字识别任务的最低错误率（&lt;0.3％）基本达到了人类的识别水平[4]。但是物体在现实环境中可能表现出相当大的变化性，所以要学会识别它们，就必须使用更大的训练集。事实上，小图像数据集的缺点已是众所周知（例如，Pinto[21]），但直到最近才可以收集到数百万的标记数据集。新的大型数据集包括LabelMe [23]，其中包含数十万个完全分割的图像，以及ImageNet [6]，其中包含超过15,000万个超过22,000个类别的高分辨率图像。<br>要从数百万图像中学习数千个类别，我们需要一个具有强大学习能力的模型。然而，物体识别任务的巨大复杂性意味着即使是像ImageNet这样大的数据集也不能完美地解决这个问题，所以我们的模型也需要使用很多先验知识来弥补我们数据集不足的问题。卷积神经网络（CNN）就构成了一类这样的模型[16,11,13,18,15,22,26]。它们的容量可以通过改变它们的深度和宽度来控制，并且它们也对图像的性质（即统计量的定态假设以及像素局部依赖性假设）做出准确而且全面的假设。因此，与具有相同大小的层的标准前馈神经网络相比，CNN具有更少的连接和参数，因此它们更容易训练，而其理论最优性能可能稍微弱一些。<br>尽管CNN具有很好的质量，并且尽管其局部结构的效率相对较高，但将它们大规模应用于高分辨率图像时仍然显得非常昂贵。幸运的是，当前的GPU可以用于高度优化的二维卷积，能够加速许多大型CNN的训练，并且最近的数据集（如ImageNet）包含足够多的标记样本来训练此类模型，而不会出现严重的过度拟合。<br>本文的具体贡献如下：我们在ILSVRC-2010和ILSVRC-2012比赛中使用的ImageNet子集上训练了迄今为止最大的卷积神经网络之一[2]，并在这些数据集上取得了迄今为止最好的结果。我们编写了一个高度优化的2D卷积的GPU实现以及其他训练卷积神经网络的固有操作，并将其公开。我们的网络包含许多新的和不同寻常的功能，这些功能可以提高网络的性能并缩短训练时间，详情请参阅第3节。我们的网络规模较大，即使有120万个带标签的训练样本，仍然存在过拟合的问题，所以我们采用了几个有效的技巧来阻止过拟合，在第4节中有详细的描述。我们最终的网络包含五个卷积层和三个全连接层，并且这个深度似乎很重要：我们发现去除任何卷积层（每个卷积层只包含不超过整个模型参数的1%的参数）都会使网络的性能变差。<br>最后，网络的规模主要受限于目前GPU上可用的内存量以及我们可接受的训练时间。我们的网络需要在两块GTX 580 3GB GPU上花费五到六天的时间来训练。我们所有的实验都表明，通过等待更快的GPU和更大的数据集出现，我们的结果可以进一步完善。</p>
<h4 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2 数据集"></a>2 数据集</h4><p>ImageNet是一个拥有超过1500万个已标记高分辨率图像的数据集，大概有22,000个类别。图像都是从网上收集，并使用Amazon-Mechanical Turk群智工具人工标记。从2010年起，作为Pascal视觉对象挑战赛的一部分，这是每年举办一次的名为ImageNet大型视觉识别挑战赛（ILSVRC）的比赛。 ILSVRC使用的是ImageNet的一个子集，每1000个类别中大约有1000个图像。总共有大约120万张训练图像，50,000张验证图像和150,000张测试图像。<br>ILSVRC-2010是ILSVRC中的唯一可以使用测试集标签的版本，因此这也正是我们进行大部分实验的版本。由于我们也在ILSVRC-2012比赛中引入了我们的模型，因此在第6部分中，我们也会给出此版本数据集的结果，尽管这个版本的测试集标签不可用。在ImageNet上，习惯上使用两种错误率：top-1和top-5，其中top-5错误率是正确标签不在被模型认为最可能的五个标签之中的测试图像的百分率。<br>ImageNet由可变分辨率的图像组成，而我们的系统需要固定的输入尺寸。因此，我们将图像下采样到256×256的固定分辨率。给定一个矩形图像，我们首先重新缩放图像，使得短边长度为256，然后从结果中裁剪出中心的256×256的图片。除了将每个像素中减去训练集的像素均值之外，我们没有以任何其他方式对图像进行预处理。所以我们在像素的（中心）原始RGB值上训练了我们的网络。</p>
<h4 id="3-结构"><a href="#3-结构" class="headerlink" title="3 结构"></a>3 结构</h4><p>图2概括了我们所提出网络的结构。它包含八个学习层——五个卷积层和三个全连接层。下面，我们将描述一些所提出网络框架中新颖或不寻常的地方。 3.1-3.4节按照我们对它们重要性的估计进行排序，其中最重要的是第一个。</p>
<h5 id="3-1-ReLU非线性单元"><a href="#3-1-ReLU非线性单元" class="headerlink" title="3.1 ReLU非线性单元"></a>3.1 ReLU非线性单元</h5><p>对一个神经元模型的输出的常规套路是，给他接上一个激活函数：<script type="math/tex">f(x)=tanh(x)</script>或者<script type="math/tex">f(x)=(1+e^{-x})^{-1}</script>。就梯度下降法的训练时间而言，这些饱和非线性函数比非饱和非线性函数如<script type="math/tex">f(x)=max(0,x)</script>慢得多。根据Nair和Hinton的说法[20]，我们将这种非线性单元称为——修正非线性单元（Rectified Linear Units (ReLUs)）。使用ReLUs做为激活函数的卷积神经网络比起使用tanh单元作为激活函数的训练起来快了好几倍。这个结果从图1中可以看出来，该图展示了对于一个特定的四层CNN，CIFAR-10数据集训练中的误差率达到25%所需要的迭代次数。从这张图的结果可以看出，如果我们使用传统的饱和神经元模型来训练CNN，那么我们将无法为这项工作训练如此大型的神经网络。</p>
<p><strong>饱和激活函数会压缩输入值</strong>：如<script type="math/tex">f(x)=max(0,x)</script>，当<script type="math/tex">x</script>趋于正无穷则<script type="math/tex">f(x)</script>也趋于正无穷，所以该函数是非饱和的，<script type="math/tex">sigmoid</script>函数的范围是<script type="math/tex">[0,1]</script>所以是饱和的，<script type="math/tex">tanh</script>函数也是饱和的，因为其取值范围为<script type="math/tex">[-1,1]</script>。</p>
<h2 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h2><p><a target="_blank" rel="noopener" href="https://github.com/DX2048/text_classification">文本分类模型实现</a></p>
<h3 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32965521">参考</a></p>
<h3 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h3><h3 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h3><h3 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h3><h3 id="Hierarchical-Attention-Network"><a href="#Hierarchical-Attention-Network" class="headerlink" title="Hierarchical Attention Network"></a>Hierarchical Attention Network</h3><h3 id="Seq2seq-With-Attention"><a href="#Seq2seq-With-Attention" class="headerlink" title="Seq2seq With Attention"></a>Seq2seq With Attention</h3><h3 id="Dynamic-Memory-Network"><a href="#Dynamic-Memory-Network" class="headerlink" title="Dynamic Memory Network"></a>Dynamic Memory Network</h3><h3 id="EntityNetwork-tracking-state-of-the-world"><a href="#EntityNetwork-tracking-state-of-the-world" class="headerlink" title="EntityNetwork:tracking state of the world"></a>EntityNetwork:tracking state of the world</h3><h3 id="Ensemble-models"><a href="#Ensemble-models" class="headerlink" title="Ensemble models"></a>Ensemble models</h3><h3 id="Transformer-“Attend-Is-All-You-Need”"><a href="#Transformer-“Attend-Is-All-You-Need”" class="headerlink" title="Transformer(“Attend Is All You Need”)"></a>Transformer(“Attend Is All You Need”)</h3><h2 id="如何选择优化算法"><a href="#如何选择优化算法" class="headerlink" title="如何选择优化算法"></a>如何选择优化算法</h2><p>如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。</p>
<p>RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。</p>
<p>Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum。</p>
<p>随着梯度变的稀疏，Adam 比 RMSprop 效果会好。</p>
<p>整体来讲，<strong>Adam 是最好的选择</strong>。</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>[零基础入门深度学习](</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/01/Reinforcement-Learning-with-Deep-Energy-Based-Policies/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/01/Reinforcement-Learning-with-Deep-Energy-Based-Policies/" class="post-title-link" itemprop="url">Reinforcement Learning with Deep Energy-Based Policies</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 22:16:57" itemprop="dateCreated datePublished" datetime="2021-06-01T22:16:57+08:00">2021-06-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-24 21:10:08" itemprop="dateModified" datetime="2021-06-24T21:10:08+08:00">2021-06-24</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    我们提出了一种用于学习连续状态和动作的基于表达能量的策略的方法，该方法之前仅在表格中可行。 我们将我们的方法应用于学习最大熵策略，从而产生一种称为soft Q-learning的新算法，该算法通过 Boltzmann 分布表达最优策略。 我们使用最近提出的amortized Stein variational gradient descent来学习随机采样网络，该网络从该分布中近似样本。 所提出算法的好处包括改进的探索和组合性，允许在任务之间转移技能，我们在游泳和步行机器人的模拟实验中证实了这一点。 我们还与actor-critic方法建立了联系，可以将其视为对相应的基于能量的模型执行近似推理。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    深度强化学习 (deep RL) 已成为自动获取复杂行为的一个有前途的方向 (Mnih et al., 2015; Silver et al., 2016)，因为它能够处理复杂的感官输入 (Jaderberg et al., 2016)并使用通用神经网络表示获得精细的行为技能 (Levine  et al., 2016)。深度强化学习方法可用于优化确定性 (Lillicrap et al., 2015)  和随机性 (Schulman et al., 2015a; Mnih et al., 2016)  策略。然而，大多数深度强化学习方法都基于传统的最优性确定性概念，其中最优解，至少在完全可观察性下，始终是确定性策略 (Sutton &amp; Barto, 1998)。虽然随机策略对于探索是可取的，但这种探索通常是通过启发式实现的，例如通过注入噪声 (Silver et al., 2014; Lillicrap et al., 2015; Mnih et al., 2015)或初始化具有高熵的随机策略 (Kakade, 2002; Schulman et al., 2015a; Mnih et al., 2016)。</p>
<p>​    在某些情况下，我们实际上可能更喜欢学习随机行为。 在本文中，我们探索了两个潜在的原因：在存在多模态目标的情况下进行探索，以及通过预训练获得的组合性。 其他好处包括面对不确定动态时的鲁棒性 (Ziebart, 2010)、模仿学习 (Ziebart et al., 2008)以及改进的收敛性和计算特性(Gu et al., 2016a)。 如  (Daniel et al., 2012) 所示，多模态也可应用于实际机器人任务。 然而，为了学习这样的策略，我们必须定义一个促进随机性的目标。</p>
<p>​    在哪些情况下，随机策略实际上是最佳解决方案？正如在之前的工作中所讨论的，当我们考虑最优控制和概率推理之间的联系时，随机策略会作为最佳答案出现(Todorov, 2008)。虽然该框架有多个实例，但它们通常包括成本或奖励函数作为因子图中的附加因子，并推断以状态为条件的动作的最佳条件分布。该解决方案可用于优化熵增强强化学习目标或对应于最大熵学习问题的解决方案 (Toussaint, 2009)。直观地说，作为推理的框架控制产生的策略不仅旨在捕获具有最低成本的单个确定性行为，而且还捕获整个范围的低成本行为，明确地最大化相应策略的熵。生成的策略不是学习执行任务的最佳方式，而是尝试学习执行任务的所有方式。现在应该很明显为什么首选此类策略：如果我们可以学习执行给定任务的所有方式，则生成的策略可以作为微调到更具体行为的良好初始化（例如，首先学习所有机器人前进的方式，然后以此作为初始化来学习单独的跑步和跳跃技能）；在多模式奖励环境中寻找最佳模式的更好的探索机制；以及面对对抗性扰动时更稳健的行为，其中以多种不同方式执行相同任务的能力可以为代理提供更多选择来从扰动中恢复。</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1F5411W7fN/">视频讲解</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/21/Deterministic-Policy-Gradient-Algorithms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/21/Deterministic-Policy-Gradient-Algorithms/" class="post-title-link" itemprop="url">Deterministic Policy Gradient Algorithms</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-21 19:31:21" itemprop="dateCreated datePublished" datetime="2021-05-21T19:31:21+08:00">2021-05-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-05-22 11:36:08" itemprop="dateModified" datetime="2021-05-22T11:36:08+08:00">2021-05-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    在本文中，我们考虑了确定性策略梯度算法，用于连续动作的强化学习。 确定性策略梯度具有特别吸引人的形式：它是动作值函数的期望梯度。 这种简单的形式意味着可以比通常的随机策略梯度更有效地估计确定性策略梯度。 为了确保进行充分的探索，我们引入了一种off-policy的行为者批评算法，该算法从探索行为策略中学习确定性目标策略。 我们证明，确定性策略梯度算法在高维操作空间中可以明显胜过其随机对应算法。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    策略梯度算法广泛应用于连续动作空间的强化学习问题。基本思想是用参数化的概率分布<script type="math/tex">\pi _{\theta }(a|s)=\mathbb{P}\left [ a|s;\theta  \right ]</script>，根据参数向量<script type="math/tex">\theta</script>随机选择状态中的动作a来表示策略。策略梯度算法通常通过对这个随机策略进行采样，并调整策略参数，以获得更大的累积奖励。</p>
<p>​    在本文中，我们将考虑确定性策略<script type="math/tex">a=\mu _{\theta }\left ( s \right )</script>。很自然会想，是否可以与随机策略遵循同样的方法：按照策略梯度的方向调整策略参数。以前认为确定性策略梯度不存在，或者只能在使用模型时获得。然而，我们证明了确定性策略梯度确实存在，而且它有一个简单的model-free形式，简单地遵循action-value函数的梯度。另外我们表明，由于策略方差趋于零，因此确定性策略梯度是随机策略梯度的极限情况。</p>
<p>​    从实际的角度来看，随机策略梯度和确定性策略梯度之间有一个关键的区别。在随机情况下，策略梯度同时在状态空间和动作空间上集成，而在确定性情况下，它只在状态空间上集成。因此，计算随机策略梯度可能需要更多的样本，特别是当动作空间有很多维数时。</p>
<p>​    为了探索完整的状态和动作空间，通常需要一个随机策略。为了确保我们的确定性策略梯度算法继续令人满意地探索，我们引入了一种off-policy学习算法。基本思想是根据随机行为策略选择行动（确保充分的探索)，但了解确定性目标策略(利用确定性策略梯度的效率）。我们使用确定性策略梯度推导出一个off-policy actor-critic算法，该算法使用可微函数近似器估计动作值函数，然后沿近似action-value梯度的方向更新策略参数。我们还引入了一个确定性策略梯度的compatiable function近似的概念，以确保该近似不会对策略梯度造成偏差。</p>
<p>我们将确定性行动者批评算法应用于以下几个基准问题：高维bandit； 低维动作空间的几个标准基准强化学习任务； 和控制章鱼手臂的高维任务。 我们的结果表明，相对于随机策略梯度，使用确定性策略梯度具有明显的性能优势，特别是在高维任务中。 此外，我们的算法比以前的方法不需要更多的计算：每次更新的计算成本在操作维度和策略参数数量上都是线性的。 最后，在许多应用程序中（例如在机器人技术中）提供了可微分的控制策略，但没有将噪声注入控制器的功能。 在这些情况下，随机策略梯度是不适用的，而我们的方法可能仍然有用。</p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/20/Continuous-Control-With-Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/20/Continuous-Control-With-Deep-Reinforcement-Learning/" class="post-title-link" itemprop="url">Continuous Control With Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-20 22:21:20" itemprop="dateCreated datePublished" datetime="2021-05-20T22:21:20+08:00">2021-05-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-18 00:44:50" itemprop="dateModified" datetime="2021-06-18T00:44:50+08:00">2021-06-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CONTINUOUS-CONTROL-WITH-DEEP-REINFORCEMENT-LEARNING"><a href="#CONTINUOUS-CONTROL-WITH-DEEP-REINFORCEMENT-LEARNING" class="headerlink" title="CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING"></a>CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们将Deep Q-Learning成功的背后思想应用到连续的行动领域。我们提出了一个actor-critic,model-free基于确定性策略梯度的算法，可以在连续动作空间上运行。使用相同的学习算法、网络架构和超参数，我们的算法稳健地解决了20多个模拟物理任务，包括卡杆摆动、灵巧操作、腿运动和汽车驾驶等经典问题。我们的算法能够找到性能与完全访问动态系统的规划算法及其衍生物相当的策略。我们进一步证明，对于许多任务，该算法可以直接通过输入原始像素学习“end-to-end”策略。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    人工智能领域的主要目标之一是从未处理的、高维的、感官的输入中解决复杂的任务。最近，通过将深度学习中的感官处理与强化学习相结合，取得了重大进展，从而形成了DQN算法 。该算法能够在许多Atari电子游戏中，使用未经处理的像素作为输入，达到了人类水平的性能。 为此，使用了深度神经网络函数逼近器来估算动作值函数。</p>
<p>​    但是，尽管DQN解决了高维观测空间的问题，但它只能处理离散和低维动作空间。 许多有趣的任务，尤其是物理控制任务，具有连续的（实际值）和高维度的动作空间。 DQN不能直接应用于连续域，因为它依赖于找到可以最大程度地提高动作值函数，在连续值情况下，每个步骤都需要迭代优化过程。</p>
<p>​    使诸如DQN之类的深度强化学习方法适应连续领域的一种明显方法是简单地离散动作空间。但是，这有很多局限性，最显着的是维度灾难：动作的数量随自由度的增加而呈指数增长。例如，对于每个关节具有最粗糙的离散<script type="math/tex">a_{i}\in\left \{ -k,0,k \right \}</script>的,关节数量为7的系统（如在人类手臂中）会导致具有以下维度的动作空间：<script type="math/tex">3^7=2187</script> 。对于需要对动作进行精细控制的任务，情况甚至更糟，因为它们需要相应地更精细的离散化，从而导致离散动作数量激增。 如此大的动作空间很难有效地探索，因此在这种情况下成功地训练类似DQN的网络可能很棘手。 此外，动作空间的单纯离散化会不必要地丢弃有关动作域结构的信息，这对于解决许多问题可能是必不可少的。</p>
<p>​    在这项工作中，我们提出了一种使用深度函数逼近器的model-free，off-policy的actor-critic算法，该算法可以学习高维，连续动作空间中的策略。我们的工作基于确定性策略梯度（DPG）算法（其自身类似于NFQCA，并且可以找到类似的想法）。 然而，正如我们在下面显示的那样，这种针对行为者的方法与神经函数近似器的单纯应用对于具有挑战性的问题是不稳定的。</p>
<p>​    在这里，我们结合了行动者批评方法和最近从Deep Q Network（DQN）成功获得的见解。 在DQN之前，通常认为使用大型非线性函数逼近器学习价值函数既困难又不稳定。 由于以下两项创新，DQN能够使用此类函数逼近器以稳定且健壮的方式学习价值函数：1.通过从重播缓冲区中抽取样本，对网络进行off-policy训练，以最大程度地减少样本之间的相关性； 2.用目标Q网络训练网络，以在时间差备份期间给出一致的目标。 在这项工作中，我们利用了相同的思想以及批处理规范化，这是深度学习的最新进展。</p>
<p>​    为了评估我们的方法，我们构造了各种具有挑战性的物理控制问题涉及复杂的多关节运动，不稳定和丰富的接触动力学以及步态行为。其中包括经典的问题，例如cartpole swing-up问题以及许多新领域。 机器人控制的长期挑战是直接从原始的感官输入（例如视频）中学习动作策略。 因此，我们将固定的视点相机放置在模拟器中，并尝试使用低维观测（例如，关节角度）以及直接从像素中进行所有任务。</p>
<p>​    我们的无模型方法，我们称为深度DPG(DDPG)，可以使用低维观测(笛卡尔坐标或关节角)来学习我们所有任务的竞争策略。使用相同的超参数和网络结构。在许多情况下，我们还能够直接从像素中学习好的策略，再次保持超参数和网络结构不变。</p>
<p>​    该方法的一个关键特征是它的简单性：它只需要简单的actor-critic体系结构和学习算法，而很少有“运动部件”，因此易于实现和扩展更困难的问题和更大的网络。 对于物理控制问题，我们将我们的结果与规划器计算的基线进行比较，该规划器可以完全访问基础模拟动力学及其派生函数（请参阅补充信息）。 有趣的是，在某些情况下，即使从像素中学习时，DDPG有时也会找到超出规划器性能的策略（规划器始终在底层的低维状态空间上进行规划）。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>​    我们考虑一个标准的强化学习设置，该设置由一个在离散的时间步中与一个环境<script type="math/tex">E</script>交互的代理组成。 代理在每个时间步长<script type="math/tex">t</script>收到观察值<script type="math/tex">x_{t}</script>，采取行动，并收到标量奖励<script type="math/tex">r_{t}</script>。 在这里考虑的所有环境中，这些动作的实值均为<script type="math/tex">a_{t}\in \mathbb{R}^{N}</script>。 通常，可能会部分观察环境，因此可能需要观察的整个历史记录，动作对<script type="math/tex">s_{t}=(x_{1},a_{1},...,a_{t-1},x_{t})</script>来描述状态。 在这里，我们假设环境是完全观察到的，因此<script type="math/tex">s_{t}=x_{t}</script>。</p>
<p>​    代理的行为由策略<script type="math/tex">\pi</script>定义，该策略将状态映射到行为<script type="math/tex">\pi</script>的概率分布：<script type="math/tex">S\rightarrow P(A)</script>。环境<script type="math/tex">E</script>也可能是随机的。 我们将其建模为马尔可夫模型状态空间为<script type="math/tex">S</script>的决策过程，动作空间<script type="math/tex">A=\mathbb{R}^{N}</script>，初始状态分布为<script type="math/tex">p(s_{1})</script>，转移概率<script type="math/tex">p(s_{t+1}|s_{t},a_{t})</script>和奖励函数<script type="math/tex">r(s_{t},a_{t})</script>。</p>
<p>​    从状态的收益被定义为具有折扣因子<script type="math/tex">\gamma \in [0,1]</script>的折扣未来奖励的总和<script type="math/tex">R_{t}=\sum_{i=t}^{T}\gamma ^{(i-t)}r(s_{i},a_{i})</script>。 请注意，返回值取决于选择的操作，因此在策略<script type="math/tex">\pi</script>上，并且可能是随机的。 强化学习的目标是学习一种策略，该策略将从初始分布最大化的预期回报。 我们表示策略<script type="math/tex">\pi</script>的折扣状态访问分布为<script type="math/tex">\rho ^{\pi }</script>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/20/Soft-Actor-Critic/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/20/Soft-Actor-Critic/" class="post-title-link" itemprop="url">Soft Actor-Critic</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-20 21:14:17" itemprop="dateCreated datePublished" datetime="2021-05-20T21:14:17+08:00">2021-05-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 23:17:10" itemprop="dateModified" datetime="2021-06-01T23:17:10+08:00">2021-06-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/17/graph-network-study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/17/graph-network-study/" class="post-title-link" itemprop="url">graph_network_study</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-17 22:50:56" itemprop="dateCreated datePublished" datetime="2021-05-17T22:50:56+08:00">2021-05-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-15 22:31:27" itemprop="dateModified" datetime="2021-06-15T22:31:27+08:00">2021-06-15</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/12/docker-study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/12/docker-study/" class="post-title-link" itemprop="url">docker study</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-12 21:00:59" itemprop="dateCreated datePublished" datetime="2021-05-12T21:00:59+08:00">2021-05-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 23:28:12" itemprop="dateModified" datetime="2021-06-01T23:28:12+08:00">2021-06-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/koktlzz/p/14105026.html#%E4%BB%80%E4%B9%88%E6%98%AF%E5%AE%B9%E5%99%A8%E6%95%B0%E6%8D%AE%E5%8D%B7%EF%BC%9F">狂神docker</a></p>
<p>docker run -it -v /home/pql/company_code/tmp:/root/repos/tmp hub.digi-sky.com/aid/aicloud:dante-1.8.0-cuda11.1-cudnn8-runtime /bin/bash</p>
<p>使用-v挂载文件夹时，假如docker容器里已存在挂载的文件夹，并且宿主主机也存在挂载文件夹，宿主主机的文件夹内容会覆盖docker容器里的内容。如果宿主主机不存在挂载文件夹，会新建文件夹，并让docker容器里的文件夹同步（删除之前的内容）。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/" class="post-title-link" itemprop="url">IMPALA:Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-07 20:10:31" itemprop="dateCreated datePublished" datetime="2021-05-07T20:10:31+08:00">2021-05-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-07-12 23:58:43" itemprop="dateModified" datetime="2021-07-12T23:58:43+08:00">2021-07-12</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    在这项工作中，我们的目标是使用一个具有单组参数的强化学习代理来解决大量的任务集合。一个关键的挑战是如何处理不断增加的数据量和不断延长的训练时间。我们开发了一种新的分布式代理 IMPALA (Importance Weighted Actor-Learner Architecture)，它不仅在单机训练中更有效地使用资源，而且在不牺牲数据效率或资源利用率的情况下还可以扩展到数千台机器。我们通过将解耦的actor和learner与一种新的off-policy的V-trace校正方法相结合，实现了高吞吐量的稳定学习。我们在DMLab-30和Atari-57上演示了IMPALA在多任务强化学习中的有效性。我们的结果表明，IMPALA能够获得比以前的代理更好的性能，并且由于它的多任务方法，关键是在任务之间表现出积极的转移。源代码可在github.com/deepmind/scalable agent。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    我们感兴趣的是开发能够同时掌握各种任务集的新方法，以及适合评估这些方法的环境。训练单个代理同时完成多个任务的主要挑战之一是可伸缩性。由于目前最先进的方法，如A3C或 UNREAL可能需要10亿帧和多天时间来掌握单个领域，同时在数十个领域上进行训练太慢，难以实现。</p>
<p>​    我们提出了图1所示的Importance Weighted Actor-Learner Architecture (IMPALA) 。IMPALA能够扩展到数千台机器，而无需牺牲训练的稳定性或数据效率。与流行的基于a3c的代理不同，workers将策略参数的梯度传递给中心参数服务器，IMPALA actor将经验轨迹传递给集中learner（sequences of states, actions, and rewards）。由于IMPALA的learner可以获得完整的经验轨迹，因此我们使用GPU对小批量的轨迹进行更新，同时积极地并行化所有时间独立的操作。这种类型的解耦架构可以实现非常高的吞吐量。然而由于用于生成轨迹的策略在梯度计算时可能落后于learner的几次更新，学习变得off-policy。因此，我们引入了 <strong>V-trace off-policy actor-critic</strong>算法来纠正这种有害的差异。</p>
<p><img src="/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/figure1.png" alt></p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><ul>
<li>distributed asynchronous SGD (Dean et al., 2012)</li>
<li>distributed A3C (Mnih et al., 2016)</li>
<li>Gorila (Nair et al., 2015)</li>
<li>a distributed version of Deep Q-Networks (Mnih et al., 2015)</li>
<li>distributed BA3C (Adamski et al., 2018) </li>
<li>Ape-X (Horgan et al., 2018)</li>
<li>batched A2C (Clemente et al., 2017)</li>
</ul>
<p>通过利用GPU来扩大强化学习。最简单的方法之一是batched A2C(Clemente et al., 2017)。在每个步骤中，batched A2C都会生成一批操作，并将其应用于一批环境。因此，每个批处理中最慢的环境决定了执行整个批处理步骤所需的时间（见图2a和图2b）。换句话说，环境速度的高方差会严重限制性能。batched A2C在Atari环境中工作得特别好，因为与强化学习代理执行的昂贵的张量操作相比，渲染和游戏逻辑在计算上非常便宜。然而，视觉或物理上更复杂的环境模拟速度可能更慢，并且在每个步骤所需的时间上可能有很大的方差。环境也可能具有可变长度episode，在初始化episode时导致放缓。</p>
<p>​    以前关于off-policy RL的相关工作包括e (Precup et al., 2000; 2001; Wawrzynski, 2009; Geist &amp; Scherrer, 2014; O’Donoghue et al., 2017) and (Harutyunyan et al., 2016)。最接近我们的工作是<strong>Retrace</strong>算法（(Munos et al., 2016) ，它引入了针对multi-step RL的off-policy校正，并已在多个代理架构中使用 (Wang et al., 2017; Gruslys et al., 2018)。<strong>Retrace</strong>需要学习状态动作值函数<script type="math/tex">Q</script>，才能进行off-policy校正。然而许多actor-critic方法，如A3C学习状态值函数<script type="math/tex">V</script>而不是状态动作值函数<script type="math/tex">Q</script>。<strong>V-trace</strong>是基于状态值函数的。</p>
<p><img src="/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/figure2.png" alt></p>
<h1 id="IMPALA"><a href="#IMPALA" class="headerlink" title="IMPALA"></a>IMPALA</h1><p>​    <strong>IMPALA</strong>（图1）使用actor-critic来学习策略<script type="math/tex">\pi</script>和baseline函数<script type="math/tex">V^\pi</script>。生成经验的过程与学习<script type="math/tex">\pi</script>和<script type="math/tex">V^\pi</script>的参数相解耦。该架构由一组actors、重复生成经验轨迹和一个或多个learners组成，他们使用从actors发送的经验来学习off-policy的<script type="math/tex">\pi</script>。</p>
<p>​    在每个轨迹的开始时，actor将自己的本地策略<script type="math/tex">\mu</script>更新为最新的learner策略<script type="math/tex">\pi</script>，并在其环境中运行<script type="math/tex">n</script>步。经过<script type="math/tex">n</script>步后，actor将 states, actions and rewards <script type="math/tex">x_{1},a_{1},r_{1},...,x_{n},a_{n},r_{n}</script>以及相应的策略分布<script type="math/tex">\mu \left ( a_{t}|x_{t} \right )</script>和初始LSTM状态通过队列发送给learner。然后learner不断在批次轨迹上更新其策略 <script type="math/tex">\pi</script> ，每个轨迹从许多actor那里收集。这种简单的架构使learner能够使用GPU和actor轻松地分布在许多机器上进行加速。但是learner策略 <script type="math/tex">\pi</script> 可能比actor策略<script type="math/tex">\mu</script>领先几次更新，因此actor和learner之间存在<strong>policy-lag</strong>。<strong>V-trace</strong> 纠正了这种滞后，以在保持数据效率的同时实现极高的数据吞吐量。 使用 actor-learner 架构，提供类似于分布式 A3C 的容错能力，但通常具有较低的通信开销，因为 actor 发送观察而不是参数/梯度。</p>
<p>​    随着非常深入的模型架构的引入，单个GPU的速度通常是训练过程中的限制因素。<strong>IMPALA</strong>可以与分布式learner一起有效地训练大型神经网络，如图1所示。参数分布在learner中，actor并行地从所有learner中检索参数，同时只将观察结果发送给单个learner。 IMPALA 使用同步参数更新(<strong>Revisiting Distributed Synchronous SGDChen</strong> et al., 2016)，这对于在扩展到多台机器时保持数据效率至关重要。</p>
<h2 id="Efficiency-Optimisations"><a href="#Efficiency-Optimisations" class="headerlink" title="Efficiency Optimisations"></a>Efficiency Optimisations</h2><p>​    GPU和多核CPU 可以从运行少数大型、可并行化操作而不是许多小型操作中受益匪浅。 由于 IMPALA 中的learner对全部的批轨迹执行更新，因此与 A3C 等在线代理相比，它能够并行化更多的计算。 例如，典型的深度强化学习代理具有卷积网络， Long Short-Term Memory (LSTM) (Hochreiter &amp; Schmidhuber, 1997) 和 LSTM 之后的全连接输出层。 IMPALA learner通过将时间维度折叠到批次维度中，将卷积网络并行应用于所有输入。 同样，一旦计算出所有 LSTM 状态，它也会将输出层并行应用于所有时间步。 这种优化将有效批量大小增加到数千。 通过利用网络结构依赖性和操作融合，基于 LSTM 的代理还可以显着提高学习器的速度 (Appleyard et al., 2016)。</p>
<p>​    最后，我们还利用了TensorFlow中可用的一些现成的优化 (Abadi et al., 2017) ，例如在仍执行计算时为 learner准备下一批数据，使用XLA（a TensorFlow Just-In-Time compiler)编译部分计算图，以及优化数据格式以从CuDNN框架获得最大性能(Chetlur et al., 2014)。</p>
<h1 id="V-trace"><a href="#V-trace" class="headerlink" title="V-trace"></a>V-trace</h1><p>​    Off-policy在解耦的分布式actor-learner架构中很重要，因为当actor生成行为和当learner估计梯度之间存在滞后。为此，我们为learner引入了一种新的off-policy  actor-critic算法，称为<strong>V-trace</strong>。</p>
<p>​    首先，让我们介绍一下一些符号。研究了马尔可夫决策过程(MDP)中的无限视野RL问题( (Puterman, 1994; Sutton &amp; Barto, 1998)。其中目标是找到一个策略<script type="math/tex">\pi</script>，使未来折扣奖励的预期总和最大化：<script type="math/tex">V^{\pi }\left ( x \right )= E_{\pi }\left [ \sum_{t\geq 0}^{}\gamma ^{t}r_{t} \right ]</script>，其中<script type="math/tex">\gamma \in \left [ 0,1 \right )</script>是折扣因子，<script type="math/tex">r_{t}=r\left ( x_{t},a_{t} \right )</script>是时间 t 的奖励，<script type="math/tex">x_{t}</script>是时间 <script type="math/tex">t</script>的状态（ 初始化在<script type="math/tex">x_{0}=x</script>) 和<script type="math/tex">a_{t}\sim \pi \left ( \cdot |x_{t} \right )</script>是通过遵循一些策略 <script type="math/tex">\pi</script> 生成的动作。</p>
<p>​    Off-policy RL算法的目标是使用由一些策略<script type="math/tex">\mu</script>生成的轨迹，称为行为策略，来学习另一个策略<script type="math/tex">\pi</script>(可能不同于<script type="math/tex">\mu</script>)的值函数<script type="math/tex">V^{\pi}</script>，称为目标策略。</p>
<h2 id="V-trace-target"><a href="#V-trace-target" class="headerlink" title="V-trace target"></a>V-trace target</h2><p>​    考虑由actor按照某些策略<script type="math/tex">\mu</script>生成的轨迹<script type="math/tex">\left ( x_{t},a_{t},r_{t} \right )_{t=s}^{t=s+n}</script>。 我们为<script type="math/tex">V\left ( x_{s} \right )</script>定义 n-step V-trace 目标，我们在状态 <script type="math/tex">x_{s}</script>处的值近似为：</p>
<p><img src="/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/formula1.png" alt></p>
<p>其中<script type="math/tex">\delta _{t}V= \rho _{t}\left ( r_{t}+\gamma V\left ( x_{t+1} \right )-V\left ( x_{t} \right ) \right )</script>是<script type="math/tex">V</script>的时间差分，<script type="math/tex">\rho _{t}= min\left ( \bar{\rho },\frac{\pi \left ( a_{t}|x_{t} \right )}{\mu \left ( a_{t}|x_{t} \right )} \right )</script>，<script type="math/tex">c_{i}=min\left ( \bar{c},\frac{\pi \left ( a_{i}|x_{i} \right )}{\mu \left ( a_{i}|x_{i} \right )} \right )</script>是截断的重要性采样 (IS) 权重（对于<script type="math/tex">s=t</script>，我们使用符号<script type="math/tex">\prod_{i=s}^{t-1}c_{i}=1</script>）。 此外我们假设截断水平使得 <script type="math/tex">\bar{\rho }\geq \bar{c}</script>。请注意，在 on-policy 情况下（当<script type="math/tex">\pi = \mu</script>时），并且假设<script type="math/tex">\bar{c}\geq 1</script>，则所有<script type="math/tex">c_{i}= 1</script>和<script type="math/tex">\rho _{t}= 1</script>，因此（1）重写为</p>
<p><img src="/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/formula2.png" alt></p>
<p>​    这是 on-policy n-steps Bellman 目标。 因此，在 on-policy 情况下，V-trace 退化到 on-policy n-step Bellman 更新。 这一特性（Retrace (Munos et al., 2016）没有）允许人们对off-和on-policy数据使用相同的算法。</p>
<p>​    请注意，（截断的)重要性采样权重<script type="math/tex">c_{i}</script>和<script type="math/tex">\rho _{t}</script>扮演不同的角色。 权重<script type="math/tex">\rho _{t}</script>出现在时间差分<script type="math/tex">\delta _{t}V</script>的定义中，并定义了此更新规则的不动点。 在可以完美表示函数的表格情况下，此更新的不动点（即当对于所有状态当<script type="math/tex">V\left ( x_{s} \right )= v_{s}</script>时），特征为在<script type="math/tex">\mu</script>下<script type="math/tex">\delta _{t}V</script>期望值为零，是某些策略<script type="math/tex">\pi_{ \bar{\rho }}</script>的值函数<script type="math/tex">V^{\pi_{\bar{\rho }} }</script>，定义为</p>
<p><img src="/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/formula3.png" alt></p>
<p>​    （见附录 A 中的分析）。 所以当 <script type="math/tex">\bar{\rho }</script>是无限的（即没有截断<script type="math/tex">\rho_{t}</script>），那么这就是目标策略的价值函数<script type="math/tex">V^{ \pi}</script>。 然而，如果我们选择截断级别<script type="math/tex">\bar{\rho }< \infty</script>，我们的固定点是策略<script type="math/tex">\pi_{ \bar{\rho }}</script>的价值函数<script type="math/tex">V^{\pi_{\bar{\rho }} }</script>，它介于<script type="math/tex">\mu</script>和<script type="math/tex">\pi</script>之间。 在<script type="math/tex">\bar{\rho }</script>接近于零的限制下，我们得到行为策略<script type="math/tex">V^{\mu}</script>的价值函数。 在附录 A 中，我们证明了相关 V-trace 算子的收缩和相应在线 V-trace 算法的收敛性。</p>
<p>​    权重<script type="math/tex">c_{i}</script>类似于 Retrace 中的“trace cutting”系数。 他们的产品 <script type="math/tex">c_{s}...c_{t-1}</script>测量在时间<script type="math/tex">t</script>观察到的时间差异<script type="math/tex">\delta _{t}V</script>对先前时间<script type="math/tex">s</script>的价值函数更新的影响。 <script type="math/tex">\pi</script>和 <script type="math/tex">\mu</script>越不相似（我们越off-policy），这个乘积的方差就越大。 我们使用截断级别<script type="math/tex">\bar{c}</script> 作为方差减少技术。 但是请注意，这种截断不会影响我们收敛到的解决方案（仅以<script type="math/tex">\bar{\rho }</script>为特征）。</p>
<p>​    因此，我们看到截断级别<script type="math/tex">\bar{c}</script>和<script type="math/tex">\bar{\rho }</script>代表算法的不同特征：<script type="math/tex">\bar{\rho }</script>影响我们收敛到的价值函数的性质，而 <script type="math/tex">\bar{c}</script>影响我们收敛到这个函数的速度。</p>
<p><strong>Remark 1</strong> V-trace目标可以递归计算：</p>
<p><img src="/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/formula4.png" alt></p>
<p><strong>Remark 2</strong> 就像在Retrace(<script type="math/tex">\lambda</script>)中一样，我们也可以通过设置<script type="math/tex">c_{i}=\lambda min\left ( \bar{c},\frac{\pi \left ( a_{i}|x_{i} \right )}{\mu \left ( a_{i}|x_{i} \right )} \right )</script>在 V-trace 的定义中考虑额外的折扣参数<script type="math/tex">\lambda \in \left [ 0,1 \right ]</script>。在on-policy情况下，当<script type="math/tex">n= \infty</script>，V-trace然后退化为<script type="math/tex">TD(\lambda)</script>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/06/Asynchronous-Methods-for-Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/06/Asynchronous-Methods-for-Deep-Reinforcement-Learning/" class="post-title-link" itemprop="url">Asynchronous Methods for Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-06 20:26:41" itemprop="dateCreated datePublished" datetime="2021-05-06T20:26:41+08:00">2021-05-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-18 00:42:55" itemprop="dateModified" datetime="2021-06-18T00:42:55+08:00">2021-06-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Asynchronous-Methods-for-Deep-Reinforcement-Learning"><a href="#Asynchronous-Methods-for-Deep-Reinforcement-Learning" class="headerlink" title="Asynchronous Methods for Deep Reinforcement Learning"></a>Asynchronous Methods for Deep Reinforcement Learning</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​    我们提出了一个概念上简单和轻量级的深度强化学习框架，它使用异步梯度下降来优化深度神经网络控制器。我们提出了四种标准强化学习算法的异步变体，并表明并行actor-learner对训练有稳定的效果，使所有四种方法都能够成功地训练神经网络控制器。性能最好的方法是actor-critic的异步变体，它在一个多核CPU而不是GPU上训练一半的时间，超过了Atari领域上最先进的方法。此外，我们还证明了异步actor-critic在各种连续电机控制问题以及使用视觉输入导航随机三维迷宫的新任务上取得了成功。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    深度神经网络提供了丰富的表示，可以使强化学习(RL)算法能够有效地执行。然而，以前人们认为，简单的online RL算法与深度神经网络的结合从根本上是不稳定的。相反，已经提出了各种解决方案来稳定该算法（(Riedmiller, 2005; Mnih et al., 2013; 2015; Van Hasselt et al., 2015; Schulman et al., 2015a)。这些方法有一个共同的想法：online RL代理遇到的观察数据序列是非平稳的，与online RL更新有很强的相关性。通过将代理的数据存储在经验回放内存中，数据可以从不同的时间步长进行分组 (Riedmiller, 2005; Schulman et al., 2015a) 或随机采样 (Mnih et al., 2013; 2015; Van Hasselt et al., 2015) 。以这种方式聚合内存可以减少非平稳性和去相关的更新，但同时将这些方法限制为off-policy强化学习算法。</p>
<p>​    基于经验重放的深度RL算法在Atari2600等具有挑战性的领域中取得了前所未有的成功。然而，经验回放有几个缺点：它每次实际交互使用更多的内存和计算；它需要off-policy的学习算法，可以从旧策略生成的数据进行更新。</p>
<p>​    在本文中，我们提供了一个非常不同的深度强化学习范式。我们在环境的多个实例上异步地并行执行多个代理，而不是经验重放。这种并行性还将代理的数据重新关联到一个更平稳的过程中，因为在任何给定的时间步长中，并行代理都将经历各种不同的状态。这个简单的想法使基本的策略RL算法，比如Sarsa,n-step方法,actor-critic方法以及off-policy的RL算法，如Q-learning,将使用深度神经网络稳健和有效地应用。</p>
<p>​    我们的并行强化学习范式也提供了实际的好处。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">QilongPan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">42</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QilongPan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
