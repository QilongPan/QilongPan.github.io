<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="paper 介绍本文提出了将监督学习与self-play强化学习相结合，首次在围棋中击败职业选手。 将棋盘位置作为19\times19的图像传入，使用卷积层来构建位置的表示。我们使用value network和policy network来减少搜索树的有效深度和广度：使用value network评估位置，并使用policy对动作进行采样。神经网络的训练流程和架构如Figure1所示。  首先使用">
<meta property="og:type" content="article">
<meta property="og:title" content="Mastering the Game of Go with Deep Neural Networks and Tree Search">
<meta property="og:url" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/index.html">
<meta property="og:site_name" content="潘其龙">
<meta property="og:description" content="paper 介绍本文提出了将监督学习与self-play强化学习相结合，首次在围棋中击败职业选手。 将棋盘位置作为19\times19的图像传入，使用卷积层来构建位置的表示。我们使用value network和policy network来减少搜索树的有效深度和广度：使用value network评估位置，并使用policy对动作进行采样。神经网络的训练流程和架构如Figure1所示。  首先使用">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/figure1.png">
<meta property="og:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula1.png">
<meta property="og:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/extend_data_table2.png">
<meta property="og:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/extend_data_table3.png">
<meta property="og:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/extend_data_table4.png">
<meta property="og:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula2.png">
<meta property="og:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula3.png">
<meta property="og:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula4.png">
<meta property="og:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/figure3.png">
<meta property="og:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula5.png">
<meta property="og:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula6.png">
<meta property="og:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula7.png">
<meta property="og:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/figure4.png">
<meta property="og:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula9.png">
<meta property="article:published_time" content="2021-11-26T13:58:21.000Z">
<meta property="article:modified_time" content="2021-11-27T03:23:18.889Z">
<meta property="article:author" content="QilongPan">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/figure1.png">

<link rel="canonical" href="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Mastering the Game of Go with Deep Neural Networks and Tree Search | 潘其龙</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">潘其龙</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Mastering the Game of Go with Deep Neural Networks and Tree Search
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-26 21:58:21" itemprop="dateCreated datePublished" datetime="2021-11-26T21:58:21+08:00">2021-11-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-27 11:23:18" itemprop="dateModified" datetime="2021-11-27T11:23:18+08:00">2021-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/model-based/" itemprop="url" rel="index"><span itemprop="name">model based</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a target="_blank" rel="noopener" href="https://www.nature.com/articles/nature16961?MRK_CMPG_SOURCE=sm_tw_pp">paper</a></p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>本文提出了将监督学习与self-play强化学习相结合，首次在围棋中击败职业选手。</p>
<p>将棋盘位置作为<script type="math/tex">19\times19</script>的图像传入，使用卷积层来构建位置的表示。我们使用value network和policy network来减少搜索树的有效深度和广度：使用value network评估位置，并使用policy对动作进行采样。神经网络的训练流程和架构如Figure1所示。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/figure1.png" alt></p>
<p>首先使用人类专家的走步数据通过监督学习训练policy network<script type="math/tex">(p_{\sigma })</script>,同时采用<strong>Computing Elo ratings of move patterns in the game of Go</strong>和<strong>Fuego — an open-source framework for board games and Go engine based on Monte-Carlo tree search</strong>两篇论文类似的方法训练了<script type="math/tex">p_{\pi}</script>用于rollout时快速采样动作。接下来训练RL policy network <script type="math/tex">p_{\rho }</script>，它通过优化self-play的最终结果来改进监督学习策略网络。这将调整策略，以达到赢得游戏的正确目标，而不是最大化预测的准确性。最后训练价值网络<script type="math/tex">v_{\theta}</script>用于预测RL policy network对抗自己时游戏的获胜者。</p>
<h1 id="监督学习策略网络"><a href="#监督学习策略网络" class="headerlink" title="监督学习策略网络"></a>监督学习策略网络</h1><p>通过已有的专家数据进行训练，网络由卷积层和非线性激活函数组成，最后一层为softmax，输出合法动作的概率。训练数据为随机采样的state-action对<script type="math/tex">(s,a)</script>，网络的输入<script type="math/tex">s</script>表示如表extend data table2所示。使用随机梯度上升来最大限度地提高人类在状态s中移动a的可能性。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula1.png" alt></p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/extend_data_table2.png" alt></p>
<p>训练结果extend data table 3所示,Symmetrics含义可以看paper的Method章节。准确度的微小提高可以使比赛强度巨大提高，如Figure2 a所示。更大的网络可以获得更好的精度，但在搜索过程中评估速度较慢。同时还训练了一个更快但更不准确的rollout policy <script type="math/tex">p_{\pi}(a|s)</script>，使用权重为<script type="math/tex">\pi</script>的一个linear softmax of small pattern features（如extend data table 4所示），达到了24.2%的准确率，只使用<script type="math/tex">\mu s</script>来选择一个动作，而不是策略网络的<script type="math/tex">3ms</script>。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/extend_data_table3.png" alt></p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/extend_data_table4.png" alt></p>
<h1 id="强化学习策略网络"><a href="#强化学习策略网络" class="headerlink" title="强化学习策略网络"></a>强化学习策略网络</h1><p>​    使用policy gradient 强化学习方法提升策略网络。强化学习策略网络和监督学习策略网络使用相同的结构，并使用监督学习网络参数初始化强化学习网络。为了防止过拟合，训练过程中会把迭代的网络模型放入对手池中，每次从池子中随机选择一个模型与当前策略对战。使用的reward function为当游戏未结束时reward为0，如果最终胜利reward为1，最终失败为-1。在每个时间步<script type="math/tex">t</script>，通过随机梯度上升，向预期结果最大化的方向更新权重。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula2.png" alt></p>
<p>其中<script type="math/tex">z_{t}</script>表示游戏reward。最终RL policy network对战监督学习策略网络达到了80%的胜率，对战Pachi（open-source Go program）达到了85%的胜率，以前最先进的监督学习网络只有11%的胜率。</p>
<h1 id="强化学习值网络"><a href="#强化学习值网络" class="headerlink" title="强化学习值网络"></a>强化学习值网络</h1><p>该网络用于位置评估，估计一个值函数<script type="math/tex">v^p(s)</script>，该函数预测在位置<script type="math/tex">s</script>处开始使用策略<script type="math/tex">p</script>对战到游戏结束的结果。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula3.png" alt></p>
<p>理想情况下，我们想知道完美对战下的最优值函数<script type="math/tex">v^{\ast }\left ( s \right )</script>；实际上使用RL策略网络<script type="math/tex">p_{\rho}</script>来估计我们最强策略的值函数<script type="math/tex">v^{p_{\rho}}</script>。我们使用一个值网络<script type="math/tex">v_{\theta}(s)</script>来近似值函数,<script type="math/tex">v_{\theta }\left ( s \right )\approx v^{p_{\rho }}\left ( s \right )\approx v^{\ast }\left ( s \right )</script>。该神经网络具有与策略网络相似的结构，但输出一个预测值，而不是一个概率分布。我们用state-outcome (s，z)作为训练集，使用随机梯度下降来最小化预测值<script type="math/tex">v_{\theta }\left ( s \right )</script>和相应的结果<script type="math/tex">z</script>之间的均方误差(MSE)。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula4.png" alt></p>
<p>从由完整游戏组成的数据中预测游戏结果的方法会导致过拟合。问题是连续的位置是密切相关，只差一步，但回归目标是共享的。当以这种方式在数据集上进行训练时，价值网络记忆了游戏结果，而不是推广到新的位置。为了缓解这个问题，生成了一个新的self-play数据集，由3000万个不同的position组成，每个position都从一个单独的游戏中采样。每个游戏都在RL策略网络和它自身之间进行，直到游戏结束。图2b显示了值网络的位置评价精度，与MCTS使用policy <script type="math/tex">p_{\pi}</script> rollout 相比，值函数始终更准确。</p>
<h1 id="使用策略网络和价值网络搜索"><a href="#使用策略网络和价值网络搜索" class="headerlink" title="使用策略网络和价值网络搜索"></a>使用策略网络和价值网络搜索</h1><p>AlphaGo将策略和价值网络结合在一个MCTS算法中，如Figure3所示。Mcts具有select、expansion、evaluation、bacckup四部分。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/figure3.png" alt></p>
<p>从当前状态<script type="math/tex">s_{t}</script>通过公式5选择<script type="math/tex">a_{t}</script></p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula5.png" alt></p>
<p>其中的<script type="math/tex">u\left ( s,a \right )\propto \frac{P\left ( s,a \right )}{1+N\left ( s,a \right )}</script>是为了鼓励探索，最终采用的<script type="math/tex">u(s,a)</script>见Method章节，<script type="math/tex">P(s,a)</script>为先验概率（可以由策略网络输出），<script type="math/tex">N(s,a)</script>为当前<script type="math/tex">s</script>的访问次数。</p>
<p>​    当遍历到达step <script type="math/tex">L</script>的叶节点<script type="math/tex">S_{L}</script>时，可以expand该叶节点。叶节点<script type="math/tex">S_{L}</script>仅由SL策略网络<script type="math/tex">p_{\sigma }</script>处理一次输出每个合法动作的概率<script type="math/tex">P</script>,<script type="math/tex">P(s,a)=p_{\sigma}(a|s)</script>。叶节点以两种非常不同的方式进行评估：第一种通过值网络<script type="math/tex">v_{\theta}(S_{L})</script>输出；第二种使用rollout policy <script type="math/tex">p_{\pi}</script>对战到游戏终局获得结果。使用混合参数<script type="math/tex">\lambda</script>将这些评估组合形成新的评估方式<script type="math/tex">V(S_{L})</script></p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula6.png" alt></p>
<p>在每次模拟结束时，更新所有遍历边的action value (<script type="math/tex">Q(s,a)</script>)和visit count (<script type="math/tex">N(s,a)</script>)。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula7.png" alt></p>
<p><script type="math/tex">S_{L}^{i}</script>表示第<script type="math/tex">i</script>次模拟时的叶子节点，<script type="math/tex">1(s,a,i)</script>表示第<script type="math/tex">i</script>次模拟是否经过边<script type="math/tex">(s,a)</script>。当搜索完成，算法将从root position选择访问量最多的动作。在AlphaGo中使用值函数<script type="math/tex">v_{\theta }\left ( s \right )\approx v^{p_{\rho }}\left ( s \right )</script> （RL policy network）比<script type="math/tex">v_{\theta }\left ( s \right )\approx v^{p_{\sigma  }}\left ( s \right )</script> (SL policy network 监督学习)更好。</p>
<p>为了有效地将MCTS与深度神经网络结合起来，AlphaGo使用异步多线程搜索，在cpu上执行模拟，并在gpu上并行计算策略和值网络。AlphaGo的最终版本使用了40个搜索线程、48个cpu和8个gpu。还实现了一个AlphaGo的分布式版本，在Method章节会详细讲。</p>
<h1 id="AlphaGo评估"><a href="#AlphaGo评估" class="headerlink" title="AlphaGo评估"></a>AlphaGo评估</h1><p>对战结果Figure4 a</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/figure4.png" alt></p>
<p>评估了AlphaGo的变体，这些变体仅使用价值网络<script type="math/tex">\lambda=0</script>或仅使用rollout（<script type="math/tex">\lambda=1</script>）来评估位置(见图4b)。即使没有rollout，AlphaGo的性能也超过了所有其他Go程序，这表明价值网络为蒙特卡罗评估提供了一个可行的替代方案。然而，混合评价（<script type="math/tex">\lambda=0.5</script>）表现最好，与其他变体对战达到了95%以上的胜率。这说明这两种位置评估机制是互补的：价值网络近似于强者<script type="math/tex">p_{\rho}</script>所玩的游戏的结果，而rollout可以精确地评分和评估较弱但较快的rollout <script type="math/tex">p_{\pi}</script>所玩游戏的结果。</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula9.png" alt></p>
<p>具体方法的实现细节可以看paper</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/" rel="prev" title="Mastering Complex Control in MOBA Games with Deep Reinforcement Learning">
      <i class="fa fa-chevron-left"></i> Mastering Complex Control in MOBA Games with Deep Reinforcement Learning
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/11/27/Mastering-the-game-of-Go-without-human-knowledge/" rel="next" title="Mastering the game of Go without human knowledge">
      Mastering the game of Go without human knowledge <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C"><span class="nav-number">2.</span> <span class="nav-text">监督学习策略网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C"><span class="nav-number">3.</span> <span class="nav-text">强化学习策略网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%80%BC%E7%BD%91%E7%BB%9C"><span class="nav-number">4.</span> <span class="nav-text">强化学习值网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%AD%96%E7%95%A5%E7%BD%91%E7%BB%9C%E5%92%8C%E4%BB%B7%E5%80%BC%E7%BD%91%E7%BB%9C%E6%90%9C%E7%B4%A2"><span class="nav-number">5.</span> <span class="nav-text">使用策略网络和价值网络搜索</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AlphaGo%E8%AF%84%E4%BC%B0"><span class="nav-number">6.</span> <span class="nav-text">AlphaGo评估</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Method"><span class="nav-number">7.</span> <span class="nav-text">Method</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">QilongPan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">49</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QilongPan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
