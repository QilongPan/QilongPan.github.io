<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="摘要​    许多现实世界中的问题，如网络包路由和自动驾驶车辆的协调，很自然地被建模为合作的多智能体系统。非常需要新的强化学习方法，可以轻松地学习此类系统的分散策略。为此，我们提出了一种新的多智能体actor-critic方法，称为counterfactual multi-agent(COMA)策略梯度。COMA使用一个集中的critic来估计Q-function，并使用分散的actors来优化代">
<meta property="og:type" content="article">
<meta property="og:title" content="Counterfactual Multi-Agent Policy Gradients">
<meta property="og:url" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/index.html">
<meta property="og:site_name" content="潘其龙">
<meta property="og:description" content="摘要​    许多现实世界中的问题，如网络包路由和自动驾驶车辆的协调，很自然地被建模为合作的多智能体系统。非常需要新的强化学习方法，可以轻松地学习此类系统的分散策略。为此，我们提出了一种新的多智能体actor-critic方法，称为counterfactual multi-agent(COMA)策略梯度。COMA使用一个集中的critic来估计Q-function，并使用分散的actors来优化代">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/figure1.png">
<meta property="og:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula4.png">
<meta property="og:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula5.png">
<meta property="og:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula6.png">
<meta property="og:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula7.png">
<meta property="og:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula9.png">
<meta property="og:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula10.png">
<meta property="og:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula13.png">
<meta property="og:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula15.png">
<meta property="og:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula16.png">
<meta property="og:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/figure2.png">
<meta property="og:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/table1.png">
<meta property="og:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/figure3.png">
<meta property="og:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/table1.png">
<meta property="article:published_time" content="2021-06-16T13:19:44.000Z">
<meta property="article:modified_time" content="2022-04-27T14:15:45.961Z">
<meta property="article:author" content="QilongPan">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/figure1.png">

<link rel="canonical" href="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Counterfactual Multi-Agent Policy Gradients | 潘其龙</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">潘其龙</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Counterfactual Multi-Agent Policy Gradients
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-16 21:19:44" itemprop="dateCreated datePublished" datetime="2021-06-16T21:19:44+08:00">2021-06-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-04-27 22:15:45" itemprop="dateModified" datetime="2022-04-27T22:15:45+08:00">2022-04-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93/" itemprop="url" rel="index"><span itemprop="name">多智能体</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    许多现实世界中的问题，如网络包路由和自动驾驶车辆的协调，很自然地被建模为合作的多智能体系统。非常需要新的强化学习方法，可以轻松地学习此类系统的分散策略。为此，我们提出了一种新的多智能体actor-critic方法，称为counterfactual multi-agent(COMA)策略梯度。COMA使用一个集中的critic来估计Q-function，并使用分散的actors来优化代理的策略。此外，为了解决多代理credit assignent的挑战，它使用了一个counterfactual(反事实的) baseline，边缘化一个单一代理的行动，同时保持其他代理的行动不变。COMA还使用了一种critic表示，允许在一次向前传递中有效地计算counterfactual baseline。我们在StarCraft unit micromanagement测试平台中评估 COMA，使用具有显着部分可观察性的分散变体。 在这种情况下，COMA 显着提高了其他多代理 actor-critic 方法的平均性能，并且性能最好的代理与可以访问完整状态的最先进的集中控制器相比具有竞争力。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    许多复杂的强化学习(RL)问题，如自动驾驶车辆的协调 (Cao et al. 2013)，网络分组传送 (Ye, Zhang, and Yang 2015),和分布式物流系统 (Ying and Dayong 2005)很自然地被建模为合作的多智能体系统。然而，为单个代理设计的RL方法通常在此类任务上表现不佳，因为代理的联合动作空间随着代理的数量呈指数级增长。</p>
<p>​    为了应对这种复杂性，通常需要诉诸去中心化策略，其中每个代理仅根据其本地动作观察历史来选择自己的动作。 此外，执行期间的部分可观察性和通信限制可能需要使用分散的策略，即使联合行动空间不是很大。</p>
<p>​    因此，非常需要能够有效学习分散策略的新 RL 方法。 在某些情况下，学习本身也可能需要去中心化。 然而，在许多情况下，学习可以在模拟器或实验室中进行，在那里可以获得额外的状态信息并且代理可以自由交流。 这种分散策略的集中训练是多智能体规划的标准范式Oliehoek, Spaan, and Vlassis 2008; Kraemer and Banerjee 2016)，最近被深度强化学习社区采用 (Foerster et al. 2016; Jorge, Kageback, and Gustavsson 2016).。 然而，如何最好地利用集中学习的机会的问题仍然存在。</p>
<p>​    另一个关键挑战是多智能体credit assignment(信用分配)(Chang, Ho, and Kaelbling 2003)：在合作环境中，联合行动通常只产生全局奖励，这使得每个智能体很难推断出自己对团队成功的贡献。 有时可以为每个代理设计单独的奖励函数。 然而，这些奖励在合作环境中通常不可用，并且通常无法鼓励个体代理为更大的利益做出牺牲。 这通常会严重阻碍多智能体在具有挑战性的任务中学习，即使智能体数量相对较少。</p>
<p>​    在本文中，我们提出了一种新的多智能体强化学习方法，称为counterfactual multi-agent（反事实多智能体 COMA) 策略梯度，以解决这些问题。 COMA 采用actor-critic(Konda and Tsitsiklis 2000)方法，其中actor即策略，通过遵循critic估计的梯度进行训练。 COMA 基于三个主要思想。</p>
<p>​    首先，COMA 使用集中式critic。 critic只在学习期间使用，而在执行期间只需要actor。 由于学习是集中式的，因此我们可以使用集中式critic，以联合动作和所有可用状态信息为条件，而每个代理的策略仅以自己的动作观察历史为条件。</p>
<p>​    其次，COMA使用了一个counterfactual baseline。这个想法的灵感来自difference rewards(Wolpert and Tumer 2002; Tumer and Agogino 2007),其中每个智能体从一个成形的奖励中学习，该奖励将全局奖励与当该智能体的动作替换为default action时收到的奖励进行比较。虽然差异奖励是执行多智能体信用分配的强大方法，但它们需要访问模拟器或估计奖励函数，并且通常不清楚如何选择默认动作。COMA 通过使用集中式critic来计算特定于代理的advantage function来解决这个问题，该function将当前联合行动的估计回报与边缘化单个代理的行为的反事实基线进行比较，同时保持其他代理的行为固定。这类似于计算aristocrat utility(贵族效用)（Wolpert and Tumer 2002)，但避免了策略和效用函数之间递归相互依赖的问题，因为反事实基线对策略梯度的预期贡献为零。因此，COMA 不依赖于额外的模拟、近似值或关于适当默认行为的假设，而是为每个代理计算单独的基线，该基线依赖于集中式critic来推理只有该代理的行为发生变化的反事实。</p>
<p>​    第三，COMA 使用critic表示，允许有效计算反事实基线。 在单个前向传递中，它计算给定代理的所有不同动作的 Q 值，条件是所有其他代理的动作。 因为所有代理都使用一个集中的critic，所以所有代理的所有 Q 值都可以在单个分批前向传递中计算。</p>
<p>​    我们在StarCraft unit micromanagement（参见1）的测试平台中评估 COMA，它最近成为具有高随机性、大状态动作空间和延迟奖励的具有挑战性的 RL 基准任务。 以前的工作 (Usunier et al. 2016; Peng et al. 2017)利用了集中控制策略，该策略以整个状态为条件，并且可以使用强大的宏观动作，使用星际争霸的内置规划器，结合移动和攻击动作 . 为了产生一个有意义的去中心化基准，证明即使在代理相对较少的情况下也具有挑战性，我们提出了一种变体，可以大规模减少每个代理的视野并删除对这些宏观动作的访问。</p>
<p>​    我们在这个新基准上的实证结果表明，与其他多代理 actor-critic 方法以及 COMA 本身的消融版本相比，COMA 可以显着提高性能。 此外，COMA 的最佳代理可以与最先进的集中控制器竞争，后者可以访问完整的状态信息和宏操作。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>​    尽管多智能体强化学习已应用于各种设置(Busoniu, Babuska, and De Schutter 2008; Yang and Gu 2004)，但它通常仅限于表格方法和简单环境。 一个例外是最近在深度多智能体强化学习方面的工作，它可以扩展到高维输入和动作空间。 Tampuu et al. (2015)将 DQN 与独立 Q-learning相结合 (Tan 1993; Shoham and Leyton-Brown 2009)来学习如何玩两人乒乓球。 最近Leibo et al. (2017)等人使用了相同的方法研究在sequential社会困境中合作和背叛的出现。</p>
<p>​    同样相关的还有智能体之间进行通信的出现，并通过梯度下降学习(Das et al. 2017; Mordatch and Abbeel 2017; Lazaridou, Peysakhovich, and Baroni 2016; Foerster et al. 2016; Sukhbaatar, Fergus, and others 2016) 。在这一系列工作中，在训练期间在智能体之间传递梯度和共享参数是利用集中训练的两种常见方式。 然而这些方法不允许在学习期间使用额外的状态信息，也不能解决多智能体信用分配问题。</p>
<p>​    Gupta, Egorov, and Kochenderfer (2017)研究了通过集中训练实现分散执行的actor-critic方法。 然而在他们的方法中，局部、每个代理、observations和actions以及多代理信用分配的演员和评论家条件都只能通过手工制作的局部奖励来解决。</p>
<p>​    RL 以前在StarCraft management中的大多数应用都使用集中式控制器，可以访问完整状态并控制所有单元，尽管控制器的架构利用了问题的多代理性质。 Usunier et al. (2016) 使用greedy MDP，它在每个时间步按顺序选择给定之前所有动作的代理的动作，并结合 zero-order optimisation(零阶优化)，而 Peng et al. (2017)使用依赖 RNN 在代理之间交换信息的 actor-critic 方法。</p>
<p>​    最接近我们的问题设置的是Foerster et al. (2017)的问题。他们也使用多代理表示和分散策略。 然而，他们在使用 DQN 时专注于稳定经验回放，并没有充分利用集中训练制度。 由于他们不报告绝对赢率，我们不直接比较性能。 然而， Usunier et al. (2016)  解决了与我们的实验类似的场景，并在完全可观察的环境中实现了 DQN 基线。 因此，在第 6 节中，我们报告了我们针对这些最先进基线的竞争表现，同时保持分散控制。Omidshafifiei et al. (2017)还解决了多智能体环境中经验回放的稳定性问题，但假设了一个完全分散的训练制度。</p>
<p>​    (Lowe et al. 2017)同时提出了一种使用集中式critic的多代理策略梯度算法。 他们的方法没有解决多代理信用分配问题。 与我们的工作不同，它为每个代理学习一个单独的集中式critic，并应用于具有连续动作空间的竞争环境。</p>
<p>​    我们的工作直接建立在difference rewards (Wolpert and Tumer 2002)的思想之上。 COMA 与这工作的关系在第 4 节中讨论。</p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>​    我们考虑一个完全合作的多智能体任务，它可以被描述为一个随机博弈 <script type="math/tex">G</script>，由一个元组<script type="math/tex">G= \left \langle S,U,P,r,Z,O,n,\gamma  \right \rangle</script>定义，其中<script type="math/tex">n</script>个智能体由<script type="math/tex">a\in A\equiv \left \{ 1,...,n \right \}</script>选择时序动作。 环境有一个真实的状态 <script type="math/tex">s\in S</script>。在每个时间步，每个智能体同时选择一个动作<script type="math/tex">u^{a}\in U</script>，形成一个联合动作<script type="math/tex">u\in U\equiv U^{n}</script>，根据状态转换函数在环境中引起转换 <script type="math/tex">P\left ( s^{'}|s,u \right ):S\times U\times S\rightarrow \left [ 0,1 \right ]</script>。 代理都共享相同的奖励函数<script type="math/tex">r\left ( s,u \right ):S\times U\rightarrow r</script>和<script type="math/tex">\gamma \in \left [ 0,1 \right )</script>是一个折扣因子。</p>
<p>​    我们考虑一个部分可观察的设置，其中代理根据观察函数 <script type="math/tex">O(s,a):S\times A\rightarrow Z</script>绘制观察<script type="math/tex">z\in Z</script>。每个代理都有一个动作观察历史<script type="math/tex">\tau ^{a}\in T\equiv \left ( Z\times U \right )^{\ast }</script>，它以随机策略<script type="math/tex">\pi ^{a\left ( u^{a}|\tau ^{a} \right )}:T\times U\rightarrow \left [ 0,1 \right ]</script>为条件。 我们用粗体表示代理的联合数量，并用上标<script type="math/tex">-a</script>表示给定代理<script type="math/tex">a</script>以外的代理的联合数量。</p>
<p>​    折扣回报为<script type="math/tex">R_{t}=\sum_{l=0}^{\infty }\gamma ^{l}r_{t+l}</script>。代理的联合策略引入一个值函数，即期望超过<script type="math/tex">R_{t}</script>、Vπ(st)=Est+1：∞，ut：∞[Rt|st]，以及<script type="math/tex">V^{\pi }\left ( s_{t} \right )=E_{s_{t+1}:\infty,u_{t}:\infty  }\left [ R_{t}|s_{t}\right ]</script>和action-value函数<script type="math/tex">Q^{\pi }(s_{t},u_{t})=E_{s_{t+1}:\infty,u_{t+1}:\infty  }\left [ R_{t}|s_{t},u_{t} \right ]</script>。优势函数由<script type="math/tex">A^{\pi }\left ( s_{t},u_{t} \right )=Q^{\pi }\left ( s_{t},u_{t} \right )-V^{\pi }\left ( s_{t} \right )</script>给出。</p>
<p>​    继之前的工作(Oliehoek, Spaan, and Vlassis 2008; Kraemer and Banerjee 2016; Foerster et al. 2016; Jorge, Kageback, and Gustavsson 2016)之后，我们的问题设置允许集中训练，但需要分散执行。 这是大量多智能体问题的自然范例，其中使用具有附加状态信息的模拟器进行训练，但智能体在执行期间必须依赖于局部动作观察历史。 为了以完整的历史为条件，深度强化学习代理可能会使用循环神经网络(Hausknecht and Stone 2015)，通常使用门控模型，例如 LSTM (Hochreiter and Schmidhuber 1997) 或 GRU(Cho et al. 2014)。</p>
<p>​    在第 4 节中，我们开发了一种新的多智能体策略梯度方法来解决这个问题。 在本节的其余部分，我们提供了有关单代理策略梯度方法的一些背景知识 (Sutton et al. 1999)。 这些方法通过对预期折扣总奖励<script type="math/tex">J=E_{\pi }\left [ R_{0} \right ]</script>的估计执行梯度上升来优化由<script type="math/tex">\theta ^{\pi }</script>参数化的单个代理的策略。 也许最简单的策略梯度形式是 REINFORCE (Williams 1992)，其中的梯度是：</p>
<script type="math/tex; mode=display">
g=E_{s_{0}:\infty ,u_{0}:\infty }\left [ \sum_{t=0}^{T}R_{t}\bigtriangledown _{\theta ^{\pi }}log\pi \left ( u_{t}|s_{t} \right ) \right ]</script><p>​    在actor-critic方法中 (Sutton et al. 1999; Konda and Tsitsiklis 2000; Schulman et al. 2015)，actor即策略，通过遵循依赖于critic(通常估计一个值函数)的梯度进行训练。特别是<script type="math/tex">R_{t}</script>被任何等价于<script type="math/tex">Q\left ( s_{t},u_{t} \right )-b\left ( s_{t} \right )</script> 的表达式替换，其中<script type="math/tex">b\left ( s_{t} \right )</script>是设计用于减少方差的基线 (Weaver and Tao 2001)。 一个常见的选择是 <script type="math/tex">b\left ( s_{t} \right )=V\left ( s_{t} \right )</script>，在这种情况下，<script type="math/tex">R_{t}</script>被<script type="math/tex">A(s_{t},u_{t})</script>替换。 另一种选择是用temporal difference(TD) 误差<script type="math/tex">r_{t}+\gamma V\left ( s_{t+1} \right )-V\left ( s \right )</script>替换 <script type="math/tex">R_{t}</script>，这是<script type="math/tex">A(s_{t},u_{t})</script>的无偏估计。 在实践中，梯度必须根据从环境中采样的轨迹来估计，并且（动作）值函数必须用函数逼近器来估计。 因此，梯度估计的偏差和方差在很大程度上取决于估计量的确切选择 (Konda and Tsitsiklis 2000)。</p>
<p>​    在本文中，我们使用<script type="math/tex">TD(\lambda)</script>(Sutton1988)适用于深度神经网络的变体，训练critic <script type="math/tex">f^{c}\left ( .,\theta ^{c} \right )</script> on-policy来估计<script type="math/tex">Q</script>或<script type="math/tex">V</script>。<script type="math/tex">TD(\lambda)</script>混合使用<script type="math/tex">n</script>步返回<script type="math/tex">G_{t}^{(n)}=\sum_{l=1}^{n}\gamma ^{l-1}r_{t+l}+\gamma ^{n}f^{c}\left ( ._{t+n},\theta ^{c} \right )</script>。特别是，critic参数<script type="math/tex">\theta^{c}</script>通过小批梯度下降进行更新，以最小化以下loss:</p>
<script type="math/tex; mode=display">
L_{t}\left ( \theta ^{c} \right )=\left ( y^{\left ( \lambda  \right )}-f^{c}\left ( ._{t},\theta ^{c} \right ) \right )^{2}</script><p>其中<script type="math/tex">y^{\left ( \lambda  \right )}=\left ( 1-\lambda  \right )\sum_{n=1}^{\infty }\lambda ^{n-1}G_{t}^{(n)}</script>和n-step返回<script type="math/tex">G_{t}^{(n)}</script>是用目标网络(Mnih et al. 2015)估计的bootstrapped值计算的，定期从<script type="math/tex">\theta^ {c}</script>复制参数。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>在本节中，我们将描述将策略梯度扩展到我们的多代理设置的方法。</p>
<h2 id="Independent-Actor-Critic"><a href="#Independent-Actor-Critic" class="headerlink" title="Independent Actor-Critic"></a>Independent Actor-Critic</h2><p>​    将策略梯度应用于多个智能体的最简单方法是让每个智能体独立学习，有自己的actor和critic，根据自己的action-observation历史。 这本质上是independent Q-learning(Tan 1993)背后的想法，它可能是最流行的多智能体学习算法，但用 actor-critic 代替了 Q-learning。 因此，我们称这种方法为independent actor-critic (IAC)。</p>
<p>​    在我们的 IAC 实现中，我们通过在代理之间共享参数来加速学习，即我们只学习一个actor和一个critic，所有智能体都使用它们。 智能体仍然可以表现不同，因为它们接收不同的观察，包括特定于智能体的 ID，从而演化出不同的隐藏状态。 学习保持独立，因为每个智能体的critic只估计一个局部价值函数，即以<script type="math/tex">u^{a}</script>而不是<strong>u</strong>为条件的价值函数。 虽然我们不知道这个特定算法以前的应用，但我们不认为它是一个重大贡献，而只是一个基线算法。</p>
<p>​    我们考虑 IAC 的两种变体。 首先，每个智能体的critic估计<script type="math/tex">V\left ( \tau ^{a} \right )</script>并遵循基于<script type="math/tex">TD</script>误差的梯度，如第 3 节所述。第二，每个智能体的critic估计<script type="math/tex">Q\left ( \tau ^{a},u^{a} \right )</script>并遵循一个梯度基于优势:<script type="math/tex">A\left ( \tau ^{a},u^{a} \right ) = Q\left ( \tau ^{a},u^{a} \right )-V\left ( \tau ^{a} \right )</script>，其中<script type="math/tex">V\left ( \tau ^{a} \right )=\sum_{u^{a}}^{}\pi \left ( u^{a}|\tau ^{a} \right )Q\left ( \tau ^{a},u^{a} \right )</script>。独立学习很简单，但在训练时缺乏信息共享使得学习依赖于多个智能体之间交互的协调策略变得困难，或者单个智能体难以估计其行为对团队奖励的贡献。</p>
<h2 id="Counterfactual-Multi-Agent-Policy-Gradients"><a href="#Counterfactual-Multi-Agent-Policy-Gradients" class="headerlink" title="Counterfactual Multi-Agent Policy Gradients"></a>Counterfactual Multi-Agent Policy Gradients</h2><p>​    上面讨论的困难之所以出现，是因为除了参数共享之外，IAC 未能利用学习集中在我们的环境中这一事实。 在本节中，我们提出了反事实多智能体 (COMA) 策略梯度，它克服了这一限制。 COMA 背后的三个主要思想：1) critic的集中化，2) 使用反事实基线，以及 3) 使用允许对基线进行有效评估的critic表示。 本节的其余部分描述了这些想法。</p>
<p>​    首先，COMA 使用集中式critic。 请注意，在 IAC 中，每个actor <script type="math/tex">\pi \left ( u^{a}|\tau ^{a} \right )</script> 和每个critic <script type="math/tex">Q\left (\tau ^{a}, u^{a}\right )</script>或 <script type="math/tex">V\left ( \tau ^{a} \right )</script>仅以代理自己的动作观察历史<script type="math/tex">\tau ^{a}</script>为条件。 但是，critic仅在学习期间使用，在执行期间只需要actor。 由于学习是集中式的，因此我们可以使用集中式critic，该critic以真实的全局状态<script type="math/tex">s</script>为条件（如果可用），否则使用联合动作观察历史<script type="math/tex">\tau</script>。 每个actor以自己的动作观察历史<script type="math/tex">\tau ^{a}</script>为条件，参数共享，如在 IAC 中。 图 1a 说明了这种设置。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/figure1.png" alt></p>
<p>​    使用这个集中式critic的一种普通的方法是让每个actor遵循基于从这个critic估计的<script type="math/tex">TD</script>误差的梯度：</p>
<script type="math/tex; mode=display">
g=\bigtriangledown _{\theta ^{\pi }}log\pi \left ( u|\tau _{t}^{a} \right )\left ( r+\gamma V\left ( s_{t+1} \right) -V\left ( s_{t} \right )\right )</script><p>​    然而，这种方法未能解决关键的信用分配问题。 由于 TD 误差仅考虑全局奖励，因此为每个actor计算的梯度并未明确判断该特定代理的行为如何对全局奖励做出贡献。 由于其他代理可能正在探索，该代理的梯度变得非常嘈杂，特别是当有很多代理时。</p>
<p>​    因此，COMA 使用counterfactual baseline（反事实基线）。 这个想法受到difference rewards (Wolpert and Tumer 2002)的启发，其中每个智能体从一个成形的奖励<script type="math/tex">D^{a}=r\left ( s,u \right )-r\left ( s,\left ( u^{-a},c^{a} \right ) \right )</script>中学习，该奖励将全局奖励与当代理 a 的动作被默认动作<script type="math/tex">c^{a}</script>替换时收到的奖励进行比较。 代理<script type="math/tex">a</script>改进<script type="math/tex">D^{a}</script>的任何动作也会提高真实的全局奖励<script type="math/tex">r\left ( s,u \right )</script>，因为<script type="math/tex">r\left ( s,\left ( u^{-a},c^{a} \right ) \right )</script>不依赖于代理<script type="math/tex">a</script>的动作。</p>
<p>​    Difference rewards是执行多智能体信用分配的有效方式。 但是，它们通常需要访问模拟器才能估计<script type="math/tex">r\left ( s,\left ( u^{-a},c^{a} \right ) \right )</script>。 当模拟器已经用于学习时，difference rewards会增加必须进行的模拟次数，因为每个代理的difference reward需要单独的反事实模拟。  Proper and Tumer (2012) and Colby, Curran, and Tumer (2015)建议使用函数近似而不是模拟器来估计差异奖励。 但是，这仍然需要用户指定的默认操作 <script type="math/tex">c^{a}</script>，这在许多应用程序中可能难以选择。 在 actor-critic 架构中，这种方法还会引入额外的近似误差源。</p>
<p>​    COMA 背后的一个关键见解是，可以使用集中式critic以避免这些问题的方式实现difference rewards。 COMA 学习了一个集中式critic <script type="math/tex">Q(s, u)</script>，它估计了以中央状态<script type="math/tex">s</script>为条件的联合动作<script type="math/tex">u</script>的<script type="math/tex">Q</script>值。 然后，对于每个代理 <script type="math/tex">a</script>，我们可以计算一个优势函数，将当前动作<script type="math/tex">u^{a}</script>的<script type="math/tex">Q</script>值与边缘化<script type="math/tex">u^{a}</script>的反事实基线进行比较，同时保持其他代理的动作<script type="math/tex">u^{-a}</script>固定：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula4.png" alt></p>
<p>​    因此，<script type="math/tex">A^{a}\left ( s,u^{a} \right )</script>为每个代理计算单独的基线，该基线使用集中式critic来推理反事实，其中只有 a 的动作发生变化，直接从代理的经验中学习，而不是依赖额外的模拟、奖励模型或 用户设计的默认操作。</p>
<p>​    这种优势与aristocrat utility (Wolpert and Tumer 2002)具有相同的形式。 然而，使用基于价值的方法优化aristocrat utility 会产生 self-consistency（自一致性）问题，因为策略和 utility function相互依赖。 因此，先前的工作侧重于使用默认状态和操作进行差异评估。 COMA 是不同的，因为反事实基线对梯度的预期贡献，与其他策略梯度基线一样为零。 因此，虽然基线确实取决于策略，但它的期望却不是。 因此，COMA 可以使用这种形式的优势而不会产生self-consistency问题。</p>
<p>​    虽然 COMA 的优势函数用critic的评估代替了潜在的额外模拟，但如果critic是深度神经网络，这些评估本身可能很昂贵。此外，在典型的表示中，这种网络的输出节点数将等于<script type="math/tex">\left | U \right |^{n}</script>，即联合动作空间的大小，这使得训练变得不切实际。为了解决这两个问题，COMA 使用了一种critic表示，可以有效地评估基线。特别是，其他代理的动作<script type="math/tex">u_{t}^{-a}</script>是网络输入的一部分，网络为每个代理 a 的动作输出 Q 值，如图 1c 所示。因此，对于每个代理，可以通过actor和critic的单次前向传递有效地计算反事实优势。此外，输出的数量只有<script type="math/tex">\left | U \right |</script>而不是 (<script type="math/tex">\left | U \right |^{n}</script>)。虽然网络有一个很大的输入空间，可以在代理和动作的数量上线性扩展，但深度神经网络可以很好地在这些空间上泛化。 </p>
<p>​    在本文中，我们专注于具有离散动作的设置。 然而通过使用蒙特卡罗样本估计（4）中的期望或使用使其具有分析性的函数形式（例如高斯策略和critic），可以轻松地将 COMA 扩展到连续动作空间。</p>
<p>​    以下引理建立了 COMA 收敛到局部最优策略。 证明直接来自单代理actor-critic算法的收敛 (Sutton et al. 1999; Konda and Tsitsiklis 2000)，并且服从相同的假设。</p>
<h3 id="Lemma-1"><a href="#Lemma-1" class="headerlink" title="Lemma 1"></a>Lemma 1</h3><p>​    对于一个具有遵循COMA策略梯度的兼容<script type="math/tex">TD(1)</script> critic的actor-critic算法。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula5.png" alt></p>
<p>在每次迭代的k处</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula6.png" alt></p>
<p>证明:COMA梯度如下</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula7.png" alt></p>
<p>其中<script type="math/tex">\theta</script>是所有actor策略的参数，例如<script type="math/tex">\theta =\left \{ \theta ^{1},...,\theta ^{\left | A \right |} \right \}</script>和<script type="math/tex">b\left ( s,u^{-a} \right )</script>是等式4中定义的反事实基线。</p>
<p>​    首先考虑该基线<script type="math/tex">b\left ( s,u^{-a} \right )</script>的预期贡献：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula9.png" alt></p>
<p>其中期望<script type="math/tex">E_{\pi}</script>是关于由联合策略<script type="math/tex">\pi</script>引起的状态行动分布的。现在让<script type="math/tex">d^{\pi}(s)</script>是Sutton et al. (1999)定义的折现遍历状态分布：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula10.png" alt></p>
<p>显然，每代理的基线虽然降低了方差，但不会改变预期的梯度，因此不会影响COMA的收敛性。</p>
<p>​    预期策略梯度的其余部分为：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula13.png" alt></p>
<p>将联合政策作为独立actor的产物来制定：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula15.png" alt></p>
<p>生成标准的单代理actor-critic策略梯度：</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/formula16.png" alt></p>
<p>​    Konda and Tsitsiklis (2000)证明了遵循这个梯度的actor-critic收敛于期望返回<script type="math/tex">J^{\pi }</script>的局部最大值：</p>
<ol>
<li><p>策略<script type="math/tex">\pi</script>是可微的，</p>
</li>
<li><p><script type="math/tex">Q</script>和<script type="math/tex">\pi</script>的更新时间尺度都足够慢，而且<script type="math/tex">\pi</script>的更新速度比<script type="math/tex">Q</script>足够慢。</p>
</li>
<li><p><script type="math/tex">Q</script>使用了与<script type="math/tex">\pi</script>兼容的表示法</p>
<p>在几个进一步的假设中。 策略的参数化（即单代理联合动作学习器被分解为独立的actor）对于收敛来说并不重要，只要它保持可微分即可。 但是请注意，COMA 的中心化critic对于此证明的成立至关重要。</p>
</li>
</ol>
<h1 id="实验装置"><a href="#实验装置" class="headerlink" title="实验装置"></a>实验装置</h1><p>​    在本节中，我们将描述我们应用 COMA 的星际争霸问题，以及状态特征、网络架构、训练机制和消融的细节。</p>
<h2 id="Decentralised-StarCraft-Micromanagement"><a href="#Decentralised-StarCraft-Micromanagement" class="headerlink" title="Decentralised StarCraft Micromanagement"></a>Decentralised StarCraft Micromanagement</h2><p>​    《星际争霸》是一个具有随机动态的丰富环境，无法轻易模拟。 相比之下，许多更简单的多代理设置，例如 Predator-Prey (Tan 1993) 或 Packet World (Weyns, Helleboogh, and Holvoet 2005)，具有完整的模拟器，可以控制随机性，可以自由设置为任何状态，以便完美地 重播经历。 这使得通过额外的模拟计算差异奖励成为可能，尽管计算成本很高。 在星际争霸中，就像在现实世界中一样，这是不可能的。</p>
<p>​    在本文中，我们关注的是星际争霸中的micromanagement问题，它指的是在与敌人作战时对单个单位的定位和攻击命令的低级控制。 这个任务自然表现为一个多代理系统，其中每个星际争霸单元都被一个分散的控制器所取代。 我们考虑了几种由对称团队组成的场景：3 名海军陆战队员 (3m)、5 名海军陆战队员 (5m)、5 名幽灵 (5w) 或 2 名龙骑兵和 3 名狂热者 (2d 3z)。 敌方团队由星际争霸 AI 控制，它使用合理但次优的手工启发式方法。</p>
<p>​    我们允许代理从一组离散的操作中进行选择：move[direction], attack[enemy id],stop, and noop。在星际争霸游戏中，当一个单位选择攻击动作时，它会先移动到攻击范围内再开火，利用游戏内置的寻路来选择路线。 这些强大的attack-move 宏动作使控制问题变得相当容易。</p>
<p>​    为了创建一个更具挑战性的、更有意义的去中心化基准，我们对代理施加了一个有限的视野，等于远程单位武器的射程，如图 2 所示。这偏离了集中式星际争霸控制的标准设置有三个作用。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/figure2.png" alt></p>
<p>​    首先，它引入了显着的部分可观察性。 其次，这意味着单位只能在敌人范围内进行攻击，从而无法访问星际争霸宏操作。 第三，代理无法区分已死亡的敌人和超出范围的敌人，因此可以向这些敌人发出无效的攻击命令，从而导致不采取任何行动。 这大大增加了动作空间的平均大小，从而增加了探索和控制的难度。</p>
<p>​    在这些困难的条件下，即使单元数量相对较少的场景也变得更加难以解决。 如表 1 所示，我们与一个简单的手工编码启发式进行了比较，该启发式指示代理向前跑到范围内，然后集中火力，依次攻击每个敌人直到其死亡。 这种启发式在 5m 的全视野下实现了 98% 的胜率，但在我们的设置中只有 66%。 为了在这项任务中表现出色，代理必须通过正确定位和集中火力来学习合作，同时记住哪些敌方和盟军单位还活着或不在视野中。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/table1.png" alt></p>
<p>​    所有代理在每个时间步都会收到相同的全局奖励，等于对对手单位造成的伤害总和减去所受伤害的一半。 杀死对手会产生 10 分的奖励，赢得比赛会产生等于团队剩余总生命值加 200 的奖励。 这种基于伤害的奖励信号与 Usunier et al. (2016)使用的信号相当。 与 (Peng et al. 2017)不同，我们的方法不需要估计本地奖励。</p>
<h2 id="State-Features"><a href="#State-Features" class="headerlink" title="State Features"></a>State Features</h2><p>​    actor和critic接收不同的输入特征，分别对应于局部观察和全局状态。 两者都包括盟友和敌人的特征。 单位可以是盟友或敌人，而智能体是指挥盟友单位的分散控制器。</p>
<p>​    每个代理的局部观察仅从以它控制的单位为中心的地图的圆形子集中绘制，并包括该视野内的每个单位：distance, relative x, relative y, unit type and shield（盾牌）（开火后一个单位的冷却时间被重置，它必须在再次开火前掉落。 盾牌吸收伤害直到它们破裂，之后单位开始失去健康。 龙骑兵和狂热者有盾牌，但海军陆战队没有）。 所有特征都是通过它们的最大值归一化。 我们不包括有关单位当前目标的任何信息。</p>
<p>​    全局状态表示由相似的特征组成，但对于地图上的所有单元，无论视野如何。 不包括绝对距离，并且 x-y 位置是相对于地图中心而不是特定代理给出的。 全局状态还包括所有代理的生命值和冷却时间。 提供给集中式 Q-function critic的表示是全局状态表示与正在评估其行为的代理的局部观察的连接。 我们估计 V (s) 的中心化critic，因此是代理不可知的，接收与所有代理的观察连接的全局状态。 观察结果不包含新信息，但包括相对于该代理的自我中心距离。</p>
<h2 id="Architecture-amp-Training"><a href="#Architecture-amp-Training" class="headerlink" title="Architecture &amp; Training"></a>Architecture &amp; Training</h2><p>​    参与者由128-bit门控循环单元(GRU)(Cho et al. 2014)组成。使用全连接层来处理输入和从隐藏状态产生输出值<script type="math/tex">h_{t}^{a}</script>。IAC的critic使用额外的output heads附加到actor网络的最后一层。动作概率是通过最后一层<script type="math/tex">z</script>，通过一个有界的softmax分布产生的，任何给定动作的概率下界为<script type="math/tex">\epsilon /\left | U \right |:P\left ( u \right )=\left ( 1-\epsilon  \right )softmax(z)_{u}+\epsilon /\left | U \right |</script>。我们在750个episode中线性地将<script type="math/tex">\epsilon</script>从0.5退火到0.02。集中的critic是一个前馈网络，具有多个ReLU层和全连接层。超参数在5m场景上进行了粗略调整，然后用于所有其他地图。我们发现最敏感的参数是<script type="math/tex">TD\left ( \lambda  \right )</script>，但确定了<script type="math/tex">\lambda = 0.8</script>，它对COMA和我们的baselines都最有效。我们的实现使用了TorchCraft (Synnaeve et al. 2016)和Torch 7 (Collobert, Kavukcuoglu, and Farabet 2011)。伪代码和关于训练程序的进一步细节见补充资料。</p>
<p>​    我们尝试了在代理水平考虑的critic体系结构，并进一步利用内部参数共享。然而，我们发现可伸缩性的瓶颈不是critic的集中，而是多智能体探索的困难。因此，我们推迟对COMA critic的因素的进一步调查到未来的工作中。</p>
<h2 id="Ablations"><a href="#Ablations" class="headerlink" title="Ablations"></a>Ablations</h2><p>​    我们进行了消融实验，验证了COMA的三个关键元素。首先，我们通过比较两种IAC变体，IAC-Q和IAC-V来测试集中化critic的重要性。这些critic采用与actor相同的分散输入，并与actor网络共享参数，直到最后一层。然后，IAC-Q输出<script type="math/tex">\left | U \right |</script> Q-values，每个动作输出一个，而IAC-V输出单个state-value。请注意，我们仍然在代理之间共享参数，使用以自我为中心的观察和ID作为输入的一部分，以允许出现不同的行为。合作报酬函数仍然由所有代理共享。</p>
<p>​    其次，我们测试了学习Q而不是V的重要性。该方法central-V 的critic仍然使用中心状态，但学习V(s)，并使用TD误差来估计策略梯度更新的优势。</p>
<p>​    第三，我们测试了我们的反事实基线的作用。central-QV方法同时学习Q和V，并估计其优势为Q−V，用V取代了COMA的反事实基线。所有的方法都为actor使用相同的架构和训练方案，所有的critic都使用<script type="math/tex">TD\left ( \lambda  \right )</script>进行训练。</p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>​    图3显示了每种方法和每种星际争霸场景的平均胜率。对于每种方法，我们进行了35次独立的试验，每训练100个episode存储一次模型，用于通过200 episode评估每种方法，绘制出每个episode和试验的平均水平。还显示了性能上的一个标准偏差。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/figure3.png" alt></p>
<p>​    结果表明，COMA在所有情况下都优于IAC基线。有趣的是，IAC方法最终也在5m场景学习了合理的策略，尽管它们需要更多的episode来实现。这可能似乎违反直觉，因为在IAC方法中，actor和critic网络在他们的早期层中共享参数（参见第5节），这可能会加速学习。然而，这些结果表明，通过使用全局状态而提高策略评估的准确性，超过了训练单独网络的开销。</p>
<p>​    此外，在所有设置的训练速度和最终性能方面，COMA在central-QV中具有重要地位。这是一个强有力的指标，表明当我们使用中心Q-critic来训练分散的策略时，我们的反事实基线是至关重要的。</p>
<p>​    学习状态值函数具有不以联合动作为条件的明显优势。 尽管如此，我们发现 COMA 在最终性能方面优于central-V 基线。 此外，COMA 通常更快地实现良好的策略，这是预期的，因为 COMA 提供了一个成形的训练信号。 训练也比 central-V 更稳定，这是 COMA 梯度随着策略变得贪婪而趋于零的结果。 总的来说，COMA 是性能最好和最一致的方法。</p>
<p>​    Usunier et al. (2016) 报告了他们最好的代理的性能，他们用他们最先进的集中式控制器标记为 GMEZO（具有episodic zero-order optimisation(Zero-Order Optimization Methods with Applications to RL )零阶优化的贪婪 MDP），以及集中式 DQN 控制器，两者都给出了完整的视野和访问攻击移动宏动作。 这些结果在表 1 中与针对每个地图使用 COMA 训练的最佳代理进行了比较。 显然，在大多数情况下，这些代理的性能可与公布的最佳获胜率相媲美，尽管它们受到分散策略和本地视野的限制。</p>
<p><img src="/2021/06/16/Counterfactual-Multi-Agent-Policy-Gradients/table1.png" alt></p>
<h1 id="总结和未来工作"><a href="#总结和未来工作" class="headerlink" title="总结和未来工作"></a>总结和未来工作</h1><p>​    本文介绍了 COMA 策略梯度，这是一种使用集中式critic来估计多智能体 RL 中分散式策略的反事实优势的方法。 COMA 通过使用反事实基线来解决多代理信用分配的挑战，该基线将单个代理的行为边缘化，同时保持其他代理的行为不变。 我们在分散式StarCraft unit micromanagement基准测试中的结果表明，COMA 与其他多智能体 actor-critic 方法相比显着提高了最终性能和训练速度，并在最佳性能报告下与最先进的集中控制器保持竞争力。 未来的工作将扩展 COMA 以处理具有大量代理的场景，在这些场景中，集中式critic更难训练，探索更难协调。 我们还旨在开发更多样本高效的变体，这些变体适用于自动驾驶汽车等实际应用。</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/06/01/deep-learning-tutorial/" rel="prev" title="deep_learning_tutorial">
      <i class="fa fa-chevron-left"></i> deep_learning_tutorial
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/06/21/Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks/" rel="next" title="Bag of Tricks for Image Classification with Convolutional Neural Networks">
      Bag of Tricks for Image Classification with Convolutional Neural Networks <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D"><span class="nav-number">2.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="nav-number">3.</span> <span class="nav-text">相关工作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">4.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-number">5.</span> <span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Independent-Actor-Critic"><span class="nav-number">5.1.</span> <span class="nav-text">Independent Actor-Critic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Counterfactual-Multi-Agent-Policy-Gradients"><span class="nav-number">5.2.</span> <span class="nav-text">Counterfactual Multi-Agent Policy Gradients</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Lemma-1"><span class="nav-number">5.2.1.</span> <span class="nav-text">Lemma 1</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E8%A3%85%E7%BD%AE"><span class="nav-number">6.</span> <span class="nav-text">实验装置</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Decentralised-StarCraft-Micromanagement"><span class="nav-number">6.1.</span> <span class="nav-text">Decentralised StarCraft Micromanagement</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#State-Features"><span class="nav-number">6.2.</span> <span class="nav-text">State Features</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Architecture-amp-Training"><span class="nav-number">6.3.</span> <span class="nav-text">Architecture &amp; Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ablations"><span class="nav-number">6.4.</span> <span class="nav-text">Ablations</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C"><span class="nav-number">7.</span> <span class="nav-text">结果</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E5%92%8C%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C"><span class="nav-number">8.</span> <span class="nav-text">总结和未来工作</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">QilongPan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">54</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QilongPan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
