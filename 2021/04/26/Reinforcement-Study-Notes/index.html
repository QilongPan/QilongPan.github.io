<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="论文下载sci-hub 根据 DOI全称数字对象唯一标识符(Digital Object Unique Identifier-DOI) 查询下载 https:&#x2F;&#x2F;www.douban.com&#x2F;group&#x2F;topic&#x2F;144179135&#x2F; https:&#x2F;&#x2F;gfsoso.99lb.net&#x2F;sci-hub.html 李宏毅视频笔记视频地址 lesson1强化学习难点 Reward delay Agent">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Study Notes">
<meta property="og:url" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/index.html">
<meta property="og:site_name" content="潘其龙">
<meta property="og:description" content="论文下载sci-hub 根据 DOI全称数字对象唯一标识符(Digital Object Unique Identifier-DOI) 查询下载 https:&#x2F;&#x2F;www.douban.com&#x2F;group&#x2F;topic&#x2F;144179135&#x2F; https:&#x2F;&#x2F;gfsoso.99lb.net&#x2F;sci-hub.html 李宏毅视频笔记视频地址 lesson1强化学习难点 Reward delay Agent">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic1.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic2.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic3.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic6.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic7.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/add_baseline.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic24.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic25.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic39.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic40.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic41.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic42.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic43.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic44.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic45.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic46.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic47.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic48.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic49.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic50.png">
<meta property="og:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic51.png">
<meta property="article:published_time" content="2021-04-26T12:53:20.000Z">
<meta property="article:modified_time" content="2021-06-21T13:42:18.673Z">
<meta property="article:author" content="QilongPan">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/04/26/Reinforcement-Study-Notes/pic1.png">

<link rel="canonical" href="http://example.com/2021/04/26/Reinforcement-Study-Notes/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Reinforcement Study Notes | 潘其龙</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">潘其龙</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/04/26/Reinforcement-Study-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Reinforcement Study Notes
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-04-26 20:53:20" itemprop="dateCreated datePublished" datetime="2021-04-26T20:53:20+08:00">2021-04-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-21 21:42:18" itemprop="dateModified" datetime="2021-06-21T21:42:18+08:00">2021-06-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="论文下载"><a href="#论文下载" class="headerlink" title="论文下载"></a>论文下载</h1><p>sci-hub</p>
<p>根据 DOI全称数字对象唯一标识符(Digital Object Unique Identifier-DOI) 查询下载</p>
<p><a target="_blank" rel="noopener" href="https://www.douban.com/group/topic/144179135/">https://www.douban.com/group/topic/144179135/</a></p>
<p><a target="_blank" rel="noopener" href="https://gfsoso.99lb.net/sci-hub.html">https://gfsoso.99lb.net/sci-hub.html</a></p>
<h1 id="李宏毅视频笔记"><a href="#李宏毅视频笔记" class="headerlink" title="李宏毅视频笔记"></a>李宏毅视频笔记</h1><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1UE411G78S?from=search&amp;seid=9027506742489609557">视频地址</a></p>
<h2 id="lesson1"><a href="#lesson1" class="headerlink" title="lesson1"></a>lesson1</h2><h3 id="强化学习难点"><a href="#强化学习难点" class="headerlink" title="强化学习难点"></a>强化学习难点</h3><ul>
<li>Reward delay</li>
<li>Agent’s actions affect the subsequent data it receives</li>
</ul>
<h3 id="强化学习方法"><a href="#强化学习方法" class="headerlink" title="强化学习方法"></a>强化学习方法</h3><p>policy-based :   Learning an Actor</p>
<p>value-based:    Learning a Critic</p>
<p>model-based</p>
<p>Three Steps for Deep Learning</p>
<ul>
<li>Neural Network as Actor</li>
</ul>
<p>Input of neural network:the observation of machine represented as a vector or a matrix</p>
<p>Output neural network:each action corresponds to a neuron in output layer</p>
<ul>
<li>goodness of function</li>
</ul>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic1.png" alt></p>
<p>Actor的好坏。通过actor玩游戏算reward期望</p>
<p>An episode is considered as a trajectory </p>
<p>$\tau = \left \{ s_{1},a_{1},r_{1},s_{2},a_{2},r_{2},…,s_{T},a_{T},r_{T} \right \}$</p>
<p>$R_{\tau } = \sum_{n=1}^{N}r_{n}$</p>
<p>If you use an actor to play the game,each <script type="math/tex">\tau</script> has a probability to be sampled. The probability depends on actor parameter <script type="math/tex">\theta</script>:$P(\tau|\theta)$</p>
<p>Use <script type="math/tex">\pi_{\theta}</script> to play the game N times,obtain <script type="math/tex">\left \{ \tau^{1},\tau^{2},...,\tau^{N} \right \}</script> </p>
<p>Sampling <script type="math/tex">\tau</script> from <script type="math/tex">P(\tau|\theta)</script> N times.</p>
<script type="math/tex; mode=display">\bar{R_\theta}=\sum_{\tau}R(\tau)P(\tau|\theta)\approx \frac{1}{N}\sum_{n=1}^{N}R(\tau^{n})</script><p>Gradient Ascent</p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic2.png" alt></p>
<p>Problem statement</p>
<script type="math/tex; mode=display">\theta ^{*} = argmax \bar{R_{\theta}}</script><script type="math/tex; mode=display">\bar{R_{\theta}}=\sum_{\tau}R(\tau)P(\tau|\theta)$$\

Start with $$\theta^{0}</script><script type="math/tex; mode=display">\theta ^{1}\leftarrow \theta ^{0}+\eta \bigtriangledown \bar{R_{\theta ^{0}}}</script><script type="math/tex; mode=display">\theta ^{2}\leftarrow \theta ^{1}+\eta \bigtriangledown \bar{R_{\theta ^{1}}}</script><script type="math/tex; mode=display">...</script><p><img src="/2021/04/26/Reinforcement-Study-Notes/pic3.png" alt></p>
<script type="math/tex; mode=display">\bigtriangledown \bar{R_{\theta }}=\sum_{\tau}R(\tau)\bigtriangledown P(\tau|\theta) =\sum_{\tau}R(\tau)P(\tau|\theta)\frac{\bigtriangledown P(\tau|\theta)}{P(\tau|\theta)} =\sum_{\tau}R(\tau)P(\tau|\theta)\bigtriangledown log P(\tau|\theta)</script><script type="math/tex; mode=display">\frac{dlog(f(x))}{dx}=\frac{1}{f(x)}\frac{df(x)}{dx}</script><script type="math/tex; mode=display">\sum_{\tau}R(\tau)P(\tau|\theta)\bigtriangledown log P(\tau|\theta) \approx \frac{1}{N}\sum_{n=1}^{N}R(\tau ^{n})\bigtriangledown logP(\tau^{n}|\theta)$$,Because $$\bar{R_\theta}=\sum_{\tau}R(\tau)P(\tau|\theta)\approx \frac{1}{N}\sum_{n=1}^{N}R(\tau^{n})$$.

![](./Reinforcement-Study-Notes/pic4.png)

Use $$\pi_{\theta}$$ to play the game N times,obtain $$\left \{ \tau^{1},\tau^{2},...,\tau^{N} \right \}</script><script type="math/tex; mode=display">R(\tau)$$ do not have to be differentiable It can even be a black box.

$$P(\tau|\theta) = p(s_{1})p(a_{1}|s_{1},\theta)p(r_{1},s_{2}|s_{1},a_{1})p(a_{2}|s_{2},\theta)p(r_{2},s_{3}|s_{2},a_{2})... = p(s_{1})\prod_{t=1}^{T}p(a_{t}|s_{t},\theta)p(r_{t},s_{t+1}|s_{t},a_{t})$$ ![](./Reinforcement-Study-Notes/pic5.png)

$$p(a_{1}|s_{1},\theta)$$取决于策略，$$p(r_{1},s_{2}|s_{1},a_{1})$$取决于游戏。$$p(s_{1})$$ and $$p(r_{t},s_{t+1}|s_{t},a_{t})$$ not releated to your actor

$$logP(\tau|\theta) = logp(s_{1}) + \sum_{t=1}^{T}logp(a_{t}|s_{t},\theta) + logp(r_{t},s_{t+1}|s_{t},a_{t})</script><p>Ignore the terms not related to <script type="math/tex">\theta</script></p>
<script type="math/tex; mode=display">\bigtriangledown logP(\tau|\theta) = \sum_{t=1}^{T}\bigtriangledown logp(a_{t}|s_{t},\theta)</script><p><img src="/2021/04/26/Reinforcement-Study-Notes/pic6.png" alt></p>
<p>finally:</p>
<script type="math/tex; mode=display">\bigtriangledown \bar{R_{\theta }} \approx \frac{1}{N}\sum_{n=1}^{N}R(\tau ^{n})\bigtriangledown logP(\tau^{n}|\theta)= \frac{1}{N}\sum_{n=1}^{N}R(\tau ^{n})\sum_{t=1}^{T}\bigtriangledown logp(a_{t}|s_{t},\theta) = \frac{1}{N}\sum_{n=1}^{N}R(\tau ^{n})\sum_{t=1}^{T}\bigtriangledown logp(a_{t}|s_{t},\theta) =\frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_{n}}R(\tau ^{n})\bigtriangledown logp(a_{t}|s_{t},\theta)</script><script type="math/tex; mode=display">\theta^{new}\leftarrow \theta^{old} + \eta \bigtriangledown \bar{R_{\theta ^{old}}}</script><p>If in <script type="math/tex">\tau^{n}</script> machine takes  <script type="math/tex">a_{t}^{n}</script> when seeing <script type="math/tex">s_{t}^{n}</script> in </p>
<script type="math/tex; mode=display">R(\tau^{n})$$ is positive  -------> Tuning  $$\theta$$ to increase  $$p(a_{t}^{n})|s_{t}^{n})</script><script type="math/tex; mode=display">R(\tau^{n})$$ is negative  -------> Tuning  $$\theta$$ to decrease $$p(a_{t}^{n})|s_{t}^{n})</script><p>It is very important to consider the cumulative reward <script type="math/tex">R(\tau^{n})</script> of the whole trajectory <script type="math/tex">\tau^{n}</script> instead of immediate reward <script type="math/tex">r_{t}^{n}</script>(是用整个trajectory的累加reward而不是用某一时刻的reward去乘)</p>
<p>如果使用的是某一时刻的reward去乘，只会让那一个动作的概率增加，而不会让其它动作概率增加</p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic7.png" alt></p>
<p><strong>为什么要使用<script type="math/tex">\bigtriangledown logP(\tau^{n}|\theta)</script>,而不用<script type="math/tex">\frac{\bigtriangledown P(a_{t}^{n}|s_{t}^{n},\theta)}{ P(a_{t}^{n}|s_{t}^{n},\theta)}</script></strong></p>
<p>e.g. in the sampling data… s has been seen in <script type="math/tex">\tau^{13},\tau^{15},\tau^{17},\tau^{33}</script></p>
<p>In <script type="math/tex">\tau^{13}</script> ,take action a <script type="math/tex">R(\tau^{13})=2</script></p>
<p>In <script type="math/tex">\tau^{15}</script> ,take action b <script type="math/tex">R(\tau^{15})=1</script></p>
<p>In <script type="math/tex">\tau^{17}</script> ,take action b <script type="math/tex">R(\tau^{17})=1</script></p>
<p>In <script type="math/tex">\tau^{33}</script> ,take action b <script type="math/tex">R(\tau^{33})=1</script></p>
<p>因为求期望是rewad乘以概率的累加，所以因为b出现次数较多，所以会导致虽然b的单次reward不如a高，但是由于其出现次数多，会使b的概率大于a。所以需要divided by <script type="math/tex">P(a_{t}^{n}|s_{t}^{n},\theta)</script>,相当于进行normalize</p>
<p><strong>Add a Baseline</strong></p>
<p>It is possible that <script type="math/tex">R(\tau^{n})</script> is always positive.</p>
<p>假如有动作a没有被sample，其它动作<script type="math/tex">R(\tau^{n})</script>为正，所以概率会增加，相对的动作a的概率就是减小</p>
<p>The probability of the actions not sampled will decrease.</p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/add_baseline.png" alt></p>
<script type="math/tex; mode=display">\bigtriangledown \bar{R_{\theta }} \approx \frac{1}{N}\sum_{n=1}^{N}\sum_{t=1}^{T_{n}}(R(\tau ^{n}) - b)\bigtriangledown logp(a_{t}|s_{t},\theta)</script><p>大于baseline才增加概率，小于baseline减小概率</p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic24.png" alt></p>
<p>假设游戏只有三步，从图中可以看出第一个episode得了3分，但是每个action的贡献都不一样，<script type="math/tex">a_{1}</script>贡献了5分，<script type="math/tex">a_{3}</script>贡献了-3分，但是训练的时候反而会增加<script type="math/tex">a_{3}</script>的概率。如果sample的episode够多，能够得到一个期望分数，或许能够忽略这个问题，比如第二个episode的情况下，<script type="math/tex">a_{3}</script>的reward为负的。为了解决这个问题，可以讲公式中的<script type="math/tex">R(\tau^{n})</script>(整个episode的reward和)换成只从当前action开始的reward和，因为之前的reward和当前action无关，公式如图所示。</p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic25.png" alt></p>
<p>由于未来的reward不确定性大，所以在计算reward和时会加入衰减，公式如图所示。</p>
<script type="math/tex; mode=display">R(\tau^{n}) - b$$是状态独立的，为Advantage Function,表示该状态采取该动作有多好，这个是相对的。Advantage Function有多种形式，这里只是其中一种。

**Critic**

- A critic does not determine the action.
- Given an actor,it evaluates the how good the actor is

- pick the best function  

## lesson2

该部分通俗解释policy gradient的公式

![](./Reinforcement-Study-Notes/pic8.png)

![](./Reinforcement-Study-Notes/pic9.png)

![](./Reinforcement-Study-Notes/pic10.png)

$$R(\tau^{1})$$可以看成是增加其对应数据的样本

## lesson3

![](./Reinforcement-Study-Notes/pic11.png)

![](./Reinforcement-Study-Notes/pic12.png)

![](./Reinforcement-Study-Notes/pic13.png)

![](./Reinforcement-Study-Notes/pic14.png)

TD approach 不用等到游戏结束就可以train，只需要一段时间就可以。

![](./Reinforcement-Study-Notes/pic15.png)

dqn、dueling dqn、rainbow

![](./Reinforcement-Study-Notes/pic16.png)

![](./Reinforcement-Study-Notes/pic17.png)

每个actor复制global parameters，然后和环境进行交互，计算gradients，然后传回global network

![](./Reinforcement-Study-Notes/pic18.png)

![](./Reinforcement-Study-Notes/pic19.png)

![](./Reinforcement-Study-Notes/pic20.png)

很多情况下是不知道reward function的，所以需要根据专家的历史trajectory来推出reward function，然后可以进行强化学习。

![](./Reinforcement-Study-Notes/pic21.png)

![](./Reinforcement-Study-Notes/pic22.png)

![](./Reinforcement-Study-Notes/pic23.png)

## lesson4

### Proximal Policy Optimization(PPO)

**From on-policy to off-policy**

![](./Reinforcement-Study-Notes/pic26.png)

![](./Reinforcement-Study-Notes/pic27.png)

![](./Reinforcement-Study-Notes/pic28.png)如如果$$p(x)$$与$$q(x)$$相差太大，会导致方差大

![](./Reinforcement-Study-Notes/pic29.png)

![](./Reinforcement-Study-Notes/pic30.png)

![](./Reinforcement-Study-Notes/pic31.png)

![](./Reinforcement-Study-Notes/pic32.png)

![](./Reinforcement-Study-Notes/pic33.png)

![](./Reinforcement-Study-Notes/pic34.png)

## lesson5

### Q-Learning

![](./Reinforcement-Study-Notes/pic35.png)

![](./Reinforcement-Study-Notes/pic36.png)

这种方法每次需要计算cumulated reward，需要游戏结束

![](./Reinforcement-Study-Notes/pic38.png)

不是预测$$V^{\pi}$$,而是预测$$V^{\pi}(s_{t})-V^{\pi}(s_{t+1})=r_{t}</script><p><img src="/2021/04/26/Reinforcement-Study-Notes/pic39.png" alt></p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic40.png" alt></p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic41.png" alt></p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic42.png" alt></p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic43.png" alt></p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic44.png" alt></p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic45.png" alt></p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic46.png" alt></p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic47.png" alt></p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic48.png" alt></p>
<h2 id="lesson6"><a href="#lesson6" class="headerlink" title="lesson6"></a>lesson6</h2><h3 id="Tips-of-Q-Learning"><a href="#Tips-of-Q-Learning" class="headerlink" title="Tips of Q-Learning"></a>Tips of Q-Learning</h3><ul>
<li><p>Nature DQN</p>
<p>Nature DQN做了一个改进，就是增加Target Q网络。也就是我们在计算目标Q值时使用专门的一个目标Q网络来计算，而不是直接使用预更新的Q网络。这样做的目的是为了减少目标计算与当前值的相关性。  原来NIPS版本的DQN目标Q网络是动态变化的，跟着Q网络的更新而变化，这样不利于计算目标Q值，导致目标Q值和当前的Q值相关性较大。因此提出单独使用一个目标Q网络。那么目标Q网络的参数如何来呢？还是从Q网络中来，只不过是延迟更新。也就是每次等训练了一段时间再将当前Q网络的参数值复制给目标Q网络。 </p>
</li>
<li><p>Double DQN</p>
</li>
</ul>
<p>Q value is usually over-estimated</p>
<ul>
<li>Dueling DQN</li>
</ul>
<p>修改network架构</p>
<ul>
<li>Prioritized Reply</li>
<li>Multi-step</li>
</ul>
<p>Balance between MC and TD</p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic49.png" alt></p>
<ul>
<li><p>Noisy Net</p>
</li>
<li><p>Distributional Q-function</p>
</li>
<li><p>Rainbow</p>
</li>
</ul>
<h2 id="lesson7"><a href="#lesson7" class="headerlink" title="lesson7"></a>lesson7</h2><h3 id="Q-Learning-for-Continuous-Actions"><a href="#Q-Learning-for-Continuous-Actions" class="headerlink" title="Q-Learning for Continuous Actions"></a>Q-Learning for Continuous Actions</h3><p><img src="/2021/04/26/Reinforcement-Study-Notes/pic50.png" alt></p>
<p><img src="/2021/04/26/Reinforcement-Study-Notes/pic51.png" alt></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/03/17/TLeague/" rel="prev" title="TLeague">
      <i class="fa fa-chevron-left"></i> TLeague
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/04/26/Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning/" rel="next" title="Genetic State-Grouping Algorithm for Deep Reinforcement Learning">
      Genetic State-Grouping Algorithm for Deep Reinforcement Learning <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%E4%B8%8B%E8%BD%BD"><span class="nav-number">1.</span> <span class="nav-text">论文下载</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9D%8E%E5%AE%8F%E6%AF%85%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0"><span class="nav-number">2.</span> <span class="nav-text">李宏毅视频笔记</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#lesson1"><span class="nav-number">2.1.</span> <span class="nav-text">lesson1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%9A%BE%E7%82%B9"><span class="nav-number">2.1.1.</span> <span class="nav-text">强化学习难点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="nav-number">2.1.2.</span> <span class="nav-text">强化学习方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lesson6"><span class="nav-number">2.2.</span> <span class="nav-text">lesson6</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tips-of-Q-Learning"><span class="nav-number">2.2.1.</span> <span class="nav-text">Tips of Q-Learning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lesson7"><span class="nav-number">2.3.</span> <span class="nav-text">lesson7</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Q-Learning-for-Continuous-Actions"><span class="nav-number">2.3.1.</span> <span class="nav-text">Q-Learning for Continuous Actions</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">QilongPan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">46</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QilongPan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
