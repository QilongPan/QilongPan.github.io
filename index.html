<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="潘其龙">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="潘其龙">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="QilongPan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>潘其龙</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">潘其龙</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/" class="post-title-link" itemprop="url">Curiosity-driven Exploration by Self-supervised Prediction</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-27 20:55:03" itemprop="dateCreated datePublished" datetime="2021-11-27T20:55:03+08:00">2021-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-29 23:30:00" itemprop="dateModified" datetime="2021-11-29T23:30:00+08:00">2021-11-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    在许多现实世界中，代理外部的奖励是非常稀疏的，或者完全没有。在这种情况下，好奇心可以作为一种内在的奖励信号，使智能体能够探索其环境，并学习可能在以后的生活中有用的技能。我们将好奇心表示为一个代理在self-supervised逆动力学模型学习的视觉特征空间中预测其自身行为的结果的能力的误差。我们的公式可以扩展到图像等高维连续状态空间，绕过了直接预测像素的困难，而且忽略了环境中不能影响代理的方面。该方法在两种环境下进行了评估：VizDoom和Super Mario Bros。研究了三种广泛的情况：1)稀疏的外部奖励，好奇心允许与环境的互动更少，以达到目标；2)没有外部奖励的探索，好奇心推动智能体更有效地探索；3)推广到看不见的场景（例如，同一游戏的新关卡），从早期经验中获得的知识有助于主体探索。</p>
<h1 id="Curiosity-Driven-Exploration"><a href="#Curiosity-Driven-Exploration" class="headerlink" title="Curiosity-Driven Exploration"></a>Curiosity-Driven Exploration</h1><p>​    我们的代理由两个子系统组成：一个奖励生成器，它输出一个好奇心驱动的内在奖励信号和一个输出一系列行动来最大化奖励信号的策略。除了内在奖励外，主体还从可以选择性的从环境中获得一些外在奖励。智能体在时间<script type="math/tex">t</script>产生的内在好奇心奖励为<script type="math/tex">r_{t}^{i}</script>，外在奖励<script type="math/tex">r_{t}^{e}</script>。策略子系统被训练，使这两种奖励的总和<script type="math/tex">r_{t}= r_{t}^{i}+r_{t}^{e}</script>最大化，大多数情况<script type="math/tex">r_{t}^{e}</script>为零。</p>
<p>​    我们用一个参数为<script type="math/tex">\theta_{P}</script>的深度神经网络来表示策略<script type="math/tex">\pi \left ( s_{t}:\theta _{P} \right )</script>。给定智能体状态为<script type="math/tex">s_{t}</script>，它执行从策略中采样的动作<script type="math/tex">a_{t}\sim \pi \left ( s_{t}:\theta _{P} \right )</script>。<script type="math/tex">\theta_{P}</script>使用最大化预期的奖励总和进行优化。</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula1.png" alt></p>
<p>​    在本文讨论的实验中，我们使用asynchronous advantage actor critic策略梯度(A3C)进行策略学习。</p>
<h2 id="Prediction-error-as-curiosity-reward"><a href="#Prediction-error-as-curiosity-reward" class="headerlink" title="Prediction error as curiosity reward"></a>Prediction error as curiosity reward</h2><p>​    在原始感觉空间(例如，当<script type="math/tex">s_{t}</script>对应于图像时)进行预测是不可取的，不仅因为很难直接预测像素，而且因为不清楚预测像素是否是优化的正确目标。假如使用像素空间中的预测误差作为好奇心的奖励，想象一下这样一个场景，代理在微风中观察树叶的运动。由于微风天生很难建模，因此甚至很难预测每个叶子的像素位置。这意味着像素预测误差将保持很高，并且代理将始终对叶子保持好奇。但是叶子的运动对代理是无关紧要的，因此它对它们的持续好奇心是不可取的。潜在的问题是，代理没有意识到状态空间的某些部分根本无法被建模，因此代理可能会陷入一个人为的好奇心陷阱，阻碍其探索。以表格形式记录被访问状态的数量（或它们对连续状态空间的扩展）的寻求新奇的探索方案也存在这个问题。在过去，测量学习进步而不是预测误差已经被提出作为一种解决方案(Schmidhuber, 1991)。不幸的是，目前还没有已知的计算上可行的机制来衡量学习进展。</p>
<p>​    如果不是原始的观测空间，那么什么是正确的特征空间来进行预测，从而使预测误差提供了一个很好的好奇心测量方法呢？为了回答这个问题，让我们将所有可以修改代理观察结果的源分为三种情况：（1）可以由代理控制的东西；（2）代理无法控制但会影响代理（例如由另一个代理驾驶的车辆）（3）超出代理控制而不影响代理的东西（例如正在移动的树叶）。一个很好的好奇心特性空间应该建模（1）和（2），并且不受（3）的影响。后者是因为，如果有一个对代理来说无关紧要的变异源，那么代理就没有动机去知道它。</p>
<h2 id="Self-supervised-prediction-for-exploration"><a href="#Self-supervised-prediction-for-exploration" class="headerlink" title="Self-supervised prediction for exploration"></a>Self-supervised prediction for exploration</h2><p>​    我们的目标是不需要手工设计每个环境的特征表示，而是提出一种学习特征表示的一般机制，使学习特征空间中的预测误差提供一个很好的内在奖励信号。我们提出可以通过具有两个子模块的深度神经网络训练的特征空间：第一个子模块编码原始状态<script type="math/tex">s_{t}</script>成一个特征向量<script type="math/tex">\phi \left ( s_{t} \right )</script>和第二个子模块作为输入时间<script type="math/tex">t</script>和<script type="math/tex">t+1</script>的特征编码<script type="math/tex">\phi \left ( s_{t} \right )</script>，<script type="math/tex">\phi \left ( s_{t+1} \right )</script>，预测使状态从<script type="math/tex">s_{t}</script>到<script type="math/tex">s_{t+1}</script>的动作。训练这个神经网络相当于学习函数<script type="math/tex">g</script>，定义为：</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula2.png" alt></p>
<p>其中<script type="math/tex">\hat{a}_{t}</script>为对动作的预测估计，并对神经网络参数<script type="math/tex">\theta _{I}</script>进行优化</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula3.png" alt></p>
<p>其中<script type="math/tex">L_{I}</script>是衡量预测行动和实际行动之间差异的损失函数。在<script type="math/tex">a_{t}</script>是离散的情况下，<script type="math/tex">g</script>的输出是所有可能动作的soft-max分布，最小化<script type="math/tex">L_{I}</script>相当于多项分布下<script type="math/tex">\theta _{I}</script>的最大似然估计。学习到的函数<script type="math/tex">g</script>也被称为逆动力学模型，当代理使用其当前的策略<script type="math/tex">\pi(s)</script>与环境交互时，可以获得学习<script type="math/tex">g</script>所需的元组<script type="math/tex">(s_{t},a_{t},s_{t+1})</script>。</p>
<p>​    除了逆动力学模型，我们还训练了另一个神经网络，以<script type="math/tex">a_{t}</script>和<script type="math/tex">\phi \left ( s_{t} \right )</script>作为输入，并预测时间步<script type="math/tex">t+1</script>状态的特征编码。</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula4.png" alt></p>
<p>其中，<script type="math/tex">\hat\phi \left ( s_{t+1} \right )</script>)为<script type="math/tex">\phi \left ( s_{t+1} \right )</script>的预测估计值，通过最小化损失函数<script type="math/tex">L_{F}</script>来优化神经网络参数<script type="math/tex">\theta_{F}</script>：</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula5.png" alt></p>
<p>学习到的函数<script type="math/tex">f</script>也被称为前向动力学模型。内在奖励信号<script type="math/tex">r_{t}^{i}</script>的计算方法为</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula6.png" alt></p>
<p>其中，<script type="math/tex">\eta > 0</script>是一个缩放因子。为了生成基于好奇心的内在奖励信号，我们分别共同优化了公式3和公式5中所描述的正向动力学损失和逆动力学损失。逆模型学习一个特征空间，该特征空间只编码与预测代理行为相关的信息，而前向模型在该特征空间中进行预测。我们将好奇心公式定义为Intrinsic Curiosity Module (ICM)。由于该特征空间没有动机对任何不受代理行为影响的环境特征进行编码，因此我们的代理将不会因达到本质上不可预测的环境状态而获得奖励，其探索策略具有鲁棒性。</p>
<p>训练结构如Figure2所示</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/figure2.png" alt></p>
<p>学习智能体的总体优化可以写成</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula7.png" alt></p>
<p>其中<script type="math/tex">0\leq \beta \leq 1</script>是一个衡量逆模型损失与正向模型损失的标量，而<script type="math/tex">\lambda > 0</script>是一个衡量策略梯度损失的重要性与学习内在奖励信号的重要性的标量。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/27/Mastering-the-game-of-Go-without-human-knowledge/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/27/Mastering-the-game-of-Go-without-human-knowledge/" class="post-title-link" itemprop="url">Mastering the game of Go without human knowledge</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-27 11:25:26" itemprop="dateCreated datePublished" datetime="2021-11-27T11:25:26+08:00">2021-11-27</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/" class="post-title-link" itemprop="url">Mastering the Game of Go with Deep Neural Networks and Tree Search</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-26 21:58:21" itemprop="dateCreated datePublished" datetime="2021-11-26T21:58:21+08:00">2021-11-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-27 11:23:18" itemprop="dateModified" datetime="2021-11-27T11:23:18+08:00">2021-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/model-based/" itemprop="url" rel="index"><span itemprop="name">model based</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.nature.com/articles/nature16961?MRK_CMPG_SOURCE=sm_tw_pp">paper</a></p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>本文提出了将监督学习与self-play强化学习相结合，首次在围棋中击败职业选手。</p>
<p>将棋盘位置作为<script type="math/tex">19\times19</script>的图像传入，使用卷积层来构建位置的表示。我们使用value network和policy network来减少搜索树的有效深度和广度：使用value network评估位置，并使用policy对动作进行采样。神经网络的训练流程和架构如Figure1所示。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/figure1.png" alt></p>
<p>首先使用人类专家的走步数据通过监督学习训练policy network<script type="math/tex">(p_{\sigma })</script>,同时采用<strong>Computing Elo ratings of move patterns in the game of Go</strong>和<strong>Fuego — an open-source framework for board games and Go engine based on Monte-Carlo tree search</strong>两篇论文类似的方法训练了<script type="math/tex">p_{\pi}</script>用于rollout时快速采样动作。接下来训练RL policy network <script type="math/tex">p_{\rho }</script>，它通过优化self-play的最终结果来改进监督学习策略网络。这将调整策略，以达到赢得游戏的正确目标，而不是最大化预测的准确性。最后训练价值网络<script type="math/tex">v_{\theta}</script>用于预测RL policy network对抗自己时游戏的获胜者。</p>
<h1 id="监督学习策略网络"><a href="#监督学习策略网络" class="headerlink" title="监督学习策略网络"></a>监督学习策略网络</h1><p>通过已有的专家数据进行训练，网络由卷积层和非线性激活函数组成，最后一层为softmax，输出合法动作的概率。训练数据为随机采样的state-action对<script type="math/tex">(s,a)</script>，网络的输入<script type="math/tex">s</script>表示如表extend data table2所示。使用随机梯度上升来最大限度地提高人类在状态s中移动a的可能性。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula1.png" alt></p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/extend_data_table2.png" alt></p>
<p>训练结果extend data table 3所示,Symmetrics含义可以看paper的Method章节。准确度的微小提高可以使比赛强度巨大提高，如Figure2 a所示。更大的网络可以获得更好的精度，但在搜索过程中评估速度较慢。同时还训练了一个更快但更不准确的rollout policy <script type="math/tex">p_{\pi}(a|s)</script>，使用权重为<script type="math/tex">\pi</script>的一个linear softmax of small pattern features（如extend data table 4所示），达到了24.2%的准确率，只使用<script type="math/tex">\mu s</script>来选择一个动作，而不是策略网络的<script type="math/tex">3ms</script>。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/extend_data_table3.png" alt></p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/extend_data_table4.png" alt></p>
<h1 id="强化学习策略网络"><a href="#强化学习策略网络" class="headerlink" title="强化学习策略网络"></a>强化学习策略网络</h1><p>​    使用policy gradient 强化学习方法提升策略网络。强化学习策略网络和监督学习策略网络使用相同的结构，并使用监督学习网络参数初始化强化学习网络。为了防止过拟合，训练过程中会把迭代的网络模型放入对手池中，每次从池子中随机选择一个模型与当前策略对战。使用的reward function为当游戏未结束时reward为0，如果最终胜利reward为1，最终失败为-1。在每个时间步<script type="math/tex">t</script>，通过随机梯度上升，向预期结果最大化的方向更新权重。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula2.png" alt></p>
<p>其中<script type="math/tex">z_{t}</script>表示游戏reward。最终RL policy network对战监督学习策略网络达到了80%的胜率，对战Pachi（open-source Go program）达到了85%的胜率，以前最先进的监督学习网络只有11%的胜率。</p>
<h1 id="强化学习值网络"><a href="#强化学习值网络" class="headerlink" title="强化学习值网络"></a>强化学习值网络</h1><p>该网络用于位置评估，估计一个值函数<script type="math/tex">v^p(s)</script>，该函数预测在位置<script type="math/tex">s</script>处开始使用策略<script type="math/tex">p</script>对战到游戏结束的结果。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula3.png" alt></p>
<p>理想情况下，我们想知道完美对战下的最优值函数<script type="math/tex">v^{\ast }\left ( s \right )</script>；实际上使用RL策略网络<script type="math/tex">p_{\rho}</script>来估计我们最强策略的值函数<script type="math/tex">v^{p_{\rho}}</script>。我们使用一个值网络<script type="math/tex">v_{\theta}(s)</script>来近似值函数,<script type="math/tex">v_{\theta }\left ( s \right )\approx v^{p_{\rho }}\left ( s \right )\approx v^{\ast }\left ( s \right )</script>。该神经网络具有与策略网络相似的结构，但输出一个预测值，而不是一个概率分布。我们用state-outcome (s，z)作为训练集，使用随机梯度下降来最小化预测值<script type="math/tex">v_{\theta }\left ( s \right )</script>和相应的结果<script type="math/tex">z</script>之间的均方误差(MSE)。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula4.png" alt></p>
<p>从由完整游戏组成的数据中预测游戏结果的方法会导致过拟合。问题是连续的位置是密切相关，只差一步，但回归目标是共享的。当以这种方式在数据集上进行训练时，价值网络记忆了游戏结果，而不是推广到新的位置。为了缓解这个问题，生成了一个新的self-play数据集，由3000万个不同的position组成，每个position都从一个单独的游戏中采样。每个游戏都在RL策略网络和它自身之间进行，直到游戏结束。图2b显示了值网络的位置评价精度，与MCTS使用policy <script type="math/tex">p_{\pi}</script> rollout 相比，值函数始终更准确。</p>
<h1 id="使用策略网络和价值网络搜索"><a href="#使用策略网络和价值网络搜索" class="headerlink" title="使用策略网络和价值网络搜索"></a>使用策略网络和价值网络搜索</h1><p>AlphaGo将策略和价值网络结合在一个MCTS算法中，如Figure3所示。Mcts具有select、expansion、evaluation、bacckup四部分。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/figure3.png" alt></p>
<p>从当前状态<script type="math/tex">s_{t}</script>通过公式5选择<script type="math/tex">a_{t}</script></p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula5.png" alt></p>
<p>其中的<script type="math/tex">u\left ( s,a \right )\propto \frac{P\left ( s,a \right )}{1+N\left ( s,a \right )}</script>是为了鼓励探索，最终采用的<script type="math/tex">u(s,a)</script>见Method章节，<script type="math/tex">P(s,a)</script>为先验概率（可以由策略网络输出），<script type="math/tex">N(s,a)</script>为当前<script type="math/tex">s</script>的访问次数。</p>
<p>​    当遍历到达step <script type="math/tex">L</script>的叶节点<script type="math/tex">S_{L}</script>时，可以expand该叶节点。叶节点<script type="math/tex">S_{L}</script>仅由SL策略网络<script type="math/tex">p_{\sigma }</script>处理一次输出每个合法动作的概率<script type="math/tex">P</script>,<script type="math/tex">P(s,a)=p_{\sigma}(a|s)</script>。叶节点以两种非常不同的方式进行评估：第一种通过值网络<script type="math/tex">v_{\theta}(S_{L})</script>输出；第二种使用rollout policy <script type="math/tex">p_{\pi}</script>对战到游戏终局获得结果。使用混合参数<script type="math/tex">\lambda</script>将这些评估组合形成新的评估方式<script type="math/tex">V(S_{L})</script></p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula6.png" alt></p>
<p>在每次模拟结束时，更新所有遍历边的action value (<script type="math/tex">Q(s,a)</script>)和visit count (<script type="math/tex">N(s,a)</script>)。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula7.png" alt></p>
<p><script type="math/tex">S_{L}^{i}</script>表示第<script type="math/tex">i</script>次模拟时的叶子节点，<script type="math/tex">1(s,a,i)</script>表示第<script type="math/tex">i</script>次模拟是否经过边<script type="math/tex">(s,a)</script>。当搜索完成，算法将从root position选择访问量最多的动作。在AlphaGo中使用值函数<script type="math/tex">v_{\theta }\left ( s \right )\approx v^{p_{\rho }}\left ( s \right )</script> （RL policy network）比<script type="math/tex">v_{\theta }\left ( s \right )\approx v^{p_{\sigma  }}\left ( s \right )</script> (SL policy network 监督学习)更好。</p>
<p>为了有效地将MCTS与深度神经网络结合起来，AlphaGo使用异步多线程搜索，在cpu上执行模拟，并在gpu上并行计算策略和值网络。AlphaGo的最终版本使用了40个搜索线程、48个cpu和8个gpu。还实现了一个AlphaGo的分布式版本，在Method章节会详细讲。</p>
<h1 id="AlphaGo评估"><a href="#AlphaGo评估" class="headerlink" title="AlphaGo评估"></a>AlphaGo评估</h1><p>对战结果Figure4 a</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/figure4.png" alt></p>
<p>评估了AlphaGo的变体，这些变体仅使用价值网络<script type="math/tex">\lambda=0</script>或仅使用rollout（<script type="math/tex">\lambda=1</script>）来评估位置(见图4b)。即使没有rollout，AlphaGo的性能也超过了所有其他Go程序，这表明价值网络为蒙特卡罗评估提供了一个可行的替代方案。然而，混合评价（<script type="math/tex">\lambda=0.5</script>）表现最好，与其他变体对战达到了95%以上的胜率。这说明这两种位置评估机制是互补的：价值网络近似于强者<script type="math/tex">p_{\rho}</script>所玩的游戏的结果，而rollout可以精确地评分和评估较弱但较快的rollout <script type="math/tex">p_{\pi}</script>所玩游戏的结果。</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula9.png" alt></p>
<p>具体方法的实现细节可以看paper</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/" class="post-title-link" itemprop="url">Mastering Complex Control in MOBA Games with Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-24 21:34:37" itemprop="dateCreated datePublished" datetime="2021-11-24T21:34:37+08:00">2021-11-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-25 22:28:14" itemprop="dateModified" datetime="2021-11-25T22:28:14+08:00">2021-11-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E6%88%98%E6%96%97%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">战斗类</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    本文从系统和算法角度提出了一个深度强化学习框架用于解决1v1战斗场景中复杂动作控制的问题。系统具有低耦合和高可伸缩性，这使大规模的高效探索成为可能。算法包括几种新的策略，包括 control dependency decoupling, action mask, target attention, and dual-clip PPO，提出的actor-critic网络可以在系统中得到有效的训练。在王者荣耀中测试，训练的智能体可以在1v1游戏中击败顶级职业人类玩家。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    本文对构建MOBA 1v1游戏AI(需要高度复杂的动作控制)进行了系统而深入的研究。在系统方面，开发了一个深度强化学习框架，提供可扩展的off-policy训练。在算法方面，actor-critic神经网络用于动作控制。我们的网络采用多标签近端策略算法(PPO)目标进行优化，并具有控制依赖的解耦、注意机制用于目标选择、action mask用于有效探索、用于学习技能组合的LSTM，以及一个改进版本的PPO，称为dual-clip PPO，以确保训练收敛。</p>
<p>​    在王者荣耀游戏中进行大量的测试，表明训练过的AI可以使用不同的英雄类型击败顶级职业人类玩家。</p>
<h1 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h1><p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/figure1.png" alt></p>
<p>系统包含如下四个模块：</p>
<ul>
<li><p>AI Server</p>
<p>AI Server涵盖了游戏环境与AI模型之间的交互逻辑。AI Server通过使用镜像策略self-play生成游戏片段(Silver et al. 2017)。对手的策略采样类似于 (Bansal et al. 2017)。基于从游戏状态中提取的特征，利用玻尔兹曼探索法预测英雄动作(Cesa-Bianchi et al. 2017)，即基于softmax分布的抽样。然后将采样的动作转发到游戏核心进行执行。执行后，游戏核心连续返回相应的奖励值和下一个状态。在使用中，一台AI Server将绑定一个CPU核心。由于游戏逻辑是在cpu上运行的，所以我们也在cpu上运行模型推理，以节省IO成本。为了有效地生成片段，我们构建了一个快速推理库<a target="_blank" rel="noopener" href="https://github.com/Tencent/FeatherCNN">FeatherCNN</a>的CPU版本。FeatherCNN可以自动将从Tensorflow和Caffe等主流工具训练出来的AI模型转换为定制的推理格式。</p>
</li>
<li><p>Dispatch Module</p>
<p> 每个Dispatch Module都与同一台机器上的几个AI服务器绑定。它从AI Server中收集数据样本，包括奖励、特征、行动概率等。这些样本首先被压缩和打包，然后发送到Memory Pool。</p>
</li>
<li><p>Memory Pool</p>
<p>Memory Pool也是一个服务器。它的内部被实现为数据存储高效的循环队列。它支持不同长度的样本，并基于生成的时间进行数据采样。</p>
</li>
<li><p>RL Learner</p>
<p>RL Learner 是一个分布式训练环境。 为了加速策略更新使用large batch size，集成多个 RL Learner 从相同数量的内存池中并行获取数据。 RL Learner 中的梯度通过 ring allreduce 算法 (Sergeev and Balso 2018)进行平均。 为了降低 IO 成本，RL Learner 使用共享内存而不是 socket 与 Memory Pools 通信，这可以提供 2-3 倍的速度提升。 来自 RL Learner的训练模型以点对点的方式快速同步到AI Servers。</p>
</li>
</ul>
<p>在我们的系统中，经验的生成与参数的学习相解耦。这种灵活的机制使AI Server和RL Learner具有高吞吐量的可扩展性。为了避免learners和actors之间的通信成本瓶颈，我们训练过的模型通过我们的主RL Learner的点对点与AI Server同步。为了平滑数据的存储和传输，我们设计了两个中介服务器，即Dispatch服务器和Memory Pool服务器。在实践中，我们可以毫不费力地扩展到数百万个CPU内核和数千个gpu。请注意，这种设计不同于现有的系统设计，如IMPALA (Espeholt et al. 2018)。在IMPALA中，参数分布在整个学习者中，参与者同时从所有学习者中检索参数。</p>
<h1 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h1><p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/figure2.png" alt></p>
<p>creeps表示野怪，turrets表示防御塔</p>
<p>网络将<script type="math/tex">f_{i}</script>(本地图像信息，比如2D障碍信息),<script type="math/tex">f_{8}</script>（可观测的单元属性，比如英雄类型、血量等）,<script type="math/tex">f_{g}</script>（游戏状态信息，比如游戏时长等）分别编码为<script type="math/tex">h_{i}</script>,<script type="math/tex">h_{u}</script>,<script type="math/tex">h_{g}</script>，使用的网络层如Figure2所示。<script type="math/tex">f_{u}</script>经过几层后会被分为单元的表示和target的attention keys两部分。为了处理不同数量的单元，通过max-pooling将相同类型的单元映射到固定长度的特征向量。然后将所有类型的<script type="math/tex">h_i</script>、<script type="math/tex">h_{u}</script>和 <script type="math/tex">h_{g}</script> 连接表示为一个可观察状态的编码向量。然后通过一个LSTM层将状态编码映射到最终的表示<script type="math/tex">h_{LSTM}</script>上，从而进一步考虑了时间信息。<script type="math/tex">h_{LSTM}</script>被发送到一个FC层来预测动作<script type="math/tex">a</script>。动作<script type="math/tex">a</script>的目标单元<script type="math/tex">t</script>可由每个单元上的目标注意机制进行预测。该机制将<script type="math/tex">h_{LSTM}</script>的FC输出视为query，将所有单元编码的stack视为keys <script type="math/tex">h_{keys}</script>，并计算目标注意力:</p>
<p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/formula1.png" alt></p>
<p><script type="math/tex">p(t|a)</script>是当前状态下所有单元的注意力分布，<script type="math/tex">p(t|a)</script>的维度是当前状态下的单元数。</p>
<p>​    其次，在multi-label policy network中，很难明确地建模不同标签之间的相互关联，例如技能方向(Offset X and Offset Y)与技能类型（Button）之间的相关性。为了解决这个问题，我们在一个操作中独立地处理每个标签，以解耦它们的相互关联，即控制依赖关系的解耦。在解耦相互互关联之前，without clipping PPO目标是：</p>
<p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/formula2.png" alt></p>
<p>假设动作被解耦为<script type="math/tex">a=(a^{0},...,a^{N_{a}-1})</script>,目标函数变为</p>
<p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/formula3.png" alt></p>
<p>​    这个解耦的目标带来了两个优点。首先，它简化了策略结构。具体来说，策略网络可以不考虑相互的相关性，因为这种依赖关系可以进行后处理。第二，它增加了行动的多样性。由于每个组件都有自己独立的价值输出通道，因此行动可以显著多样化，从而在训练过程中诱导更多的探索。此外，<strong>为了探索的多样性，我们在游戏开始的训练中随机两个代理的位置</strong>。</p>
<p>​    为了提高训练效率，提出了action mask，基于经验丰富的人类玩家的先验知识，在策略的最终输出层中mask不合理的动作。</p>
<p><strong>Dual-clip PPO</strong>  设<script type="math/tex">r_{t}(\theta)</script>表示概率比<script type="math/tex">\frac{\pi _{\theta }\left ( a_{t}|s_{t} \right )}{\pi _{\theta _{old}}\left ( a_{t}|s_{t} \right )}</script>。由于<script type="math/tex">r_{t}(\theta)</script>的比值可能非常大，因此RL目标的最大化可能会导致过大的策略偏差。为了缓解这个问题，标准的PPO算法 (Schulman et al. 2017) 包含如下ratio clip来惩罚policy的极端变化</p>
<p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/formula4.png" alt></p>
<p>然而，在大规模off-policy训练环境中，轨迹从各种policy来源中采样，这可能与当前的policy <script type="math/tex">\pi_{\theta}</script>有很大的不同。在这种情况下，标准PPO将无法处理这些偏差，因为它最初是针对on-policy提出的 (Schulman et al. 2017)。例如，当<script type="math/tex">\pi _{\theta }\left ( a_{t}^{\left ( i \right )}|s_{t} \right )> >\pi  _{\theta _{old}}\left ( a_{t}^{\left ( i \right )} |s_{t}\right )</script>时，比率<script type="math/tex">r_{t}(\theta)</script>是一个巨大的数字。当<script type="math/tex">\hat{A}_{t}<0</script>时，如此大的比率<script type="math/tex">r_{t}(\theta)</script>将引入一个大的无界方差,因为<script type="math/tex">r_{t}\left ( \theta  \right )\hat{A}_{t}< < 0</script>。因此，即使采用PPO的目标，新policy也与旧policy有了明显的偏差，这很难确保策略可以收敛。因此我们提出了一种 dual-clipped PPO算法来支持大规模分布训练，进一步剪<script type="math/tex">r_{t}\left ( \theta  \right )\hat{A}_{t}</script>下界，如图3所示。当<script type="math/tex">\hat{A}_{t}<0</script>时，dual-clipped PPO的新目标是：</p>
<p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/formula5.png" alt></p>
<p>其中c&gt;1是一个常数，表示下界。</p>
<p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/figure3.png" alt></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/11/Attention-Is-All-You-Need/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/11/Attention-Is-All-You-Need/" class="post-title-link" itemprop="url">Attention Is All You Need</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-08-11 21:04:01" itemprop="dateCreated datePublished" datetime="2021-08-11T21:04:01+08:00">2021-08-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-08-21 10:21:56" itemprop="dateModified" datetime="2021-08-21T10:21:56+08:00">2021-08-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​     主要的序列转换模型基于复杂的循环或卷积神经网络，包括编码器和解码器。 性能最好的模型还通过注意力机制连接编码器和解码器。 我们提出了一种新的简单网络架构，即 Transformer，它完全基于注意力机制，完全消除了递归和卷积。 在两个机器翻译任务上的实验表明，这些模型在质量上更胜一筹，同时更可并行化并且需要更少的训练时间。 我们的模型在 WMT 2014 英德翻译任务上达到了 28.4 BLEU，比现有的最佳结果（包括集成）提高了 2 BLEU。 在 WMT 2014 英语到法语翻译任务中，我们的模型在 8 个 GPU 上训练 3.5 天后建立了一个新的单模型最先进的 BLEU 分数 41.0，这是最好的训练成本的一小部分文献中的模型。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    RNN输入需要上一个隐藏状态，这种固有的顺序特性导致在训练中不能使用并行化。在除少数情况下，注意机制与循环网络一起使用。在这项工作中，我们提出了Transformer，一个避免递归的模型架构，而是完全依赖于一个注意机制来绘制输入和输出之间的全局依赖关系。Transformer允许更多的并行化，在8个P100 gpu上训练了短短12个小时后，可以达到翻译质量的新水平。</p>
<h1 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h1><p>​    大多数具有竞争力的神经序列转换模型都具有encoder-decoder结构。 在这里，encoder将符号表示的输入序列 <script type="math/tex">\left ( x_{1},...,x_{n} \right )</script> 映射到连续表示的序列<script type="math/tex">z= \left ( z_{1},...,z_{n} \right )</script>。 给定<script type="math/tex">z</script>，decoder然后生成一个符号的输出序列<script type="math/tex">\left ( y_{1},...,y_{n} \right )</script>，一次一个元素。 在每一步，模型都是自回归的，在生成下一个时，将先前生成的符号作为附加输入使用。</p>
<p>​    Transformer 遵循这种整体架构，使用堆叠的self-attention和point-wise、完全连接的encoder和decoder层，分别如图 1 的左半部分和右半部分所示。</p>
<p><img src="/2021/08/11/Attention-Is-All-You-Need/figure1.png" alt></p>
<h2 id="Encoder-and-Decoder-Stacks"><a href="#Encoder-and-Decoder-Stacks" class="headerlink" title="Encoder and Decoder Stacks"></a>Encoder and Decoder Stacks</h2><ul>
<li><p>Encoder:编码器由<script type="math/tex">N=6</script>个相同层组成。每一层都有两个子层。第一个是multi-head self-attention mechanism，第二个是一个简单的position-wise全连接的前馈网络。我们在两个子层周围使用一个残差连接，然后是层归一化。即每个子层的输出都是<script type="math/tex">LayerNorm(x+Sublayer(x))</script>，其中<script type="math/tex">Sublayer(x)</script>是由子层本身实现的函数。为了方便这些残差连接，模型中的所有子层以及嵌入层都会产生了维度<script type="math/tex">d_{model}=512</script>的输出。</p>
</li>
<li><p>Decoder:该解码器还由<script type="math/tex">N=6</script>个相同层的堆栈组成。除了每个编码器层中的两个子层外，解码器还插入第三子层，该子层对编码器堆栈的输出执行multi-head attention。与编码器类似，我们在每个子层周围使用残差连接，然后进行层归一化。我们还修改了解码器堆栈中的self-attention 子层，以防止位置关注后续位置。这种掩蔽，再加上输出嵌入被一个位置偏移的事实，确保了对位置<script type="math/tex">i</script>的预测只能依赖于小于<script type="math/tex">i</script>的位置的已知输出。</p>
</li>
</ul>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>​    注意函数可以描述为将query和一组key-value对映射到输出，其中query、keys、values和output都是向量。输出计算为values的加权和，其中分配给每个value的权重由query与相应key的兼容性函数计算。</p>
<h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>​    我们将我们的特别attention称为“Scaled Dot-Product Attention”（图 2）。 输入包括维度<script type="math/tex">d_{k}</script>的queries和keys，以及维度<script type="math/tex">d_{v}</script>的values。 我们使用query和所有keys的点积，将每个除以<script type="math/tex">\sqrt{d_{k}}</script>，然后应用 softmax 函数来获得value上的权重。</p>
<p><img src="/2021/08/11/Attention-Is-All-You-Need/figure2.png" alt></p>
<p>​    在实践中，我们同时计算一组queries上的attention function，并打包到一个矩阵<script type="math/tex">Q</script>中。keys和values也被打包在矩阵<script type="math/tex">k</script>和<script type="math/tex">V</script>中。我们计算的输出矩阵如下：</p>
<p><img src="/2021/08/11/Attention-Is-All-You-Need/formula1.png" alt></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/10/Deep-Reinforcement-Learning-with-Double-Q-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/10/Deep-Reinforcement-Learning-with-Double-Q-learning/" class="post-title-link" itemprop="url">Deep Reinforcement Learning with Double Q-learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-08-10 21:58:54 / Modified: 22:10:11" itemprop="dateCreated datePublished" datetime="2021-08-10T21:58:54+08:00">2021-08-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/dqn%E7%B3%BB%E5%88%97/" itemprop="url" rel="index"><span itemprop="name">dqn系列</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="/2021/08/10/Deep-Reinforcement-Learning-with-Double-Q-learning/formula4.png" alt></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/10/Prioritized-Experience-Replay/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/10/Prioritized-Experience-Replay/" class="post-title-link" itemprop="url">Prioritized Experience Replay</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-08-10 21:48:34 / Modified: 22:16:43" itemprop="dateCreated datePublished" datetime="2021-08-10T21:48:34+08:00">2021-08-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/dqn%E7%B3%BB%E5%88%97/" itemprop="url" rel="index"><span itemprop="name">dqn系列</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="/2021/08/10/Prioritized-Experience-Replay/algorithm1.png" alt></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/10/Dueling-Network-Architectures-for-Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/10/Dueling-Network-Architectures-for-Deep-Reinforcement-Learning/" class="post-title-link" itemprop="url">Dueling Network Architectures for Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-08-10 21:29:19 / Modified: 22:17:04" itemprop="dateCreated datePublished" datetime="2021-08-10T21:29:19+08:00">2021-08-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/dqn%E7%B3%BB%E5%88%97/" itemprop="url" rel="index"><span itemprop="name">dqn系列</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="/2021/08/10/Dueling-Network-Architectures-for-Deep-Reinforcement-Learning/architecture.png" alt></p>
<p><img src="/2021/08/10/Dueling-Network-Architectures-for-Deep-Reinforcement-Learning/formula7.png" alt></p>
<p><img src="/2021/08/10/Dueling-Network-Architectures-for-Deep-Reinforcement-Learning/formula8.png" alt></p>
<p><img src="/2021/08/10/Dueling-Network-Architectures-for-Deep-Reinforcement-Learning/formula9.png" alt></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/10/Nature-DQN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/10/Nature-DQN/" class="post-title-link" itemprop="url">Nature DQN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-08-10 21:12:29 / Modified: 22:17:20" itemprop="dateCreated datePublished" datetime="2021-08-10T21:12:29+08:00">2021-08-10</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/dqn%E7%B3%BB%E5%88%97/" itemprop="url" rel="index"><span itemprop="name">dqn系列</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="/2021/08/10/Nature-DQN/algorithm1.png" alt></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/09/Revisiting-Rainbow-Promoting-more-Insightful-and-Inclusive-Deep-Reinforcement-Learning-Research/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/09/Revisiting-Rainbow-Promoting-more-Insightful-and-Inclusive-Deep-Reinforcement-Learning-Research/" class="post-title-link" itemprop="url">Revisiting Rainbow:Promoting more Insightful and Inclusive Deep Reinforcement Learning Research</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-08-09 19:53:36" itemprop="dateCreated datePublished" datetime="2021-08-09T19:53:36+08:00">2021-08-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-08-10 22:17:36" itemprop="dateModified" datetime="2021-08-10T22:17:36+08:00">2021-08-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/dqn%E7%B3%BB%E5%88%97/" itemprop="url" rel="index"><span itemprop="name">dqn系列</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://github.com/JohanSamir/revisiting rainbow">code</a></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    自引入DQN以来，绝大多数的强化学习研究都集中在以深度神经网络作为函数近似器的强化学习上。新方法通常在一组现在已经成为标准的环境上进行评估，比如Atari 2600游戏。虽然这些基准有助于标准化评估，但它们的计算成本有一个不幸的副作用，即扩大了那些有充足机会获取计算资源和那些没有计算资源的人之间的差距。在这项工作中，我们认为，尽管社区强调大规模环境，但传统的小规模环境仍然可以产生有价值的科学见解，并可以帮助减少底层社区进入的障碍。为了证实我们的主张，我们通过经验回顾了介绍Rainbow算法的论文 (Hessel et al., 2018)，并对Rainbow使用的算法提出了一些新的见解。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><ul>
<li>Double Q-learning</li>
<li>Prioritized experience replay</li>
<li>Dueling networks</li>
<li>Multi-step learning</li>
<li>Distributional RL</li>
<li>Noisy nets</li>
</ul>
<h1 id="Revisiting-Rainbow"><a href="#Revisiting-Rainbow" class="headerlink" title="Revisiting Rainbow"></a>Revisiting Rainbow</h1><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>​    我们遵循了与Hessel等人（2018）类似的过程。在评估上述各种算法变体时：我们研究了在原始DQN代理上添加每个算法的影响，以及从最终的Rainbow代理中删除每个算法的影响，横扫每个代理的学习率。</p>
<p>​    实验环境为CartPole, Acrobot, LunarLander, and MountainCar。在这些实验中，我们使用了多层感知器(MLP)，两个多层感知器每层512个单元。这些代理都在CPU上接受了训练；值得注意的是，在这些环境中，训练时间最长的环境（LunarLander）仍然能在不到两个小时内完成训练。</p>
<p>​    为了加强Rainbow Connection，我们还在MinAtar环境(Young and Tian, 2019)上进行了一组实验，这是五款ALE游戏(Asterix, Breakout, Freeway, Seaquest, and SpaceInvaders)的小型化版本。这些环境比以前探索过的四种经典控制环境要大得多，但它们的训练速度要比常规的ALE环境要快得多。具体来说，训练其中一种智能体，使用P100GPU大约需要12-14小时。在这些实验中，我们遵循了Young and Tian (2019)使用的网络架构，由一个单个卷积层和一个线性层组成。</p>
<h2 id="实证评价"><a href="#实证评价" class="headerlink" title="实证评价"></a>实证评价</h2><p>​    在恒定的超参数设置下（详见附录），我们在 classic control environments（图1）和Minatar（图2）上评估了每个算法组件添加到DQN中和从整个Rainbow代理中删除。</p>
<p>​    我们在第3节中提出的前两个问题的背景下分析我们的结果：如果Hessel等人在一组小规模实验上进行实验，他们会得出同样的定性结论吗？Hessel等人的结果是否能很好地推广到非ALE环境，或者他们的结果是否过度特定于所选基准。</p>
<p>​    我们发现，不同组件的性能在所有环境中都不是一致的；这一发现与Hessel等人观察到的结果一致。然而，如果我们建议使用一个单一的代理来平衡不同算法组件的权衡，我们的分析将与Hessel等人的分析相一致：组合所有组件可以产生更好的整体代理。</p>
<p>​    然而，在不同算法组件的变化中有一些重要的细节，值得进行更彻底的研究。我们工作的一个重要发现是，当将distributional RL单独添加到DQN中时，实际上可能会损害性能（如Acrobot and Freeway)；类似地，当distributional RL从Rainbow中移除时，性能有时会提高（如MountainCar和 Seaquest）。这与Hessel等人在ALE实验中的发现形成了对比，值得进一步研究。正如Lyle et al. (2019) 注意到的一样，在非线性函数近似器下(正如我们在这些实验中使用的那样），使用 distributional RL通常会产生与non-distributional变体不同的结果，但这些差异并不总是正的。</p>
<p><img src="/2021/08/09/Revisiting-Rainbow-Promoting-more-Insightful-and-Inclusive-Deep-Reinforcement-Learning-Research/figure1.png" alt></p>
<p><img src="/2021/08/09/Revisiting-Rainbow-Promoting-more-Insightful-and-Inclusive-Deep-Reinforcement-Learning-Research/figure2.png" alt></p>
<h1 id="Beyond-the-Rainbow"><a href="#Beyond-the-Rainbow" class="headerlink" title="Beyond the Rainbow"></a>Beyond the Rainbow</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">QilongPan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">46</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QilongPan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
