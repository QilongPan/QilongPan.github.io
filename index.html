<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="潘其龙">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="潘其龙">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="QilongPan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>潘其龙</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">潘其龙</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/01/deep-learning-tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/01/deep-learning-tutorial/" class="post-title-link" itemprop="url">deep_learning_tutorial</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-01 23:02:57" itemprop="dateCreated datePublished" datetime="2021-06-01T23:02:57+08:00">2021-06-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-02 23:58:10" itemprop="dateModified" datetime="2021-06-02T23:58:10+08:00">2021-06-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="深度学习笔记"><a href="#深度学习笔记" class="headerlink" title="深度学习笔记"></a>深度学习笔记</h1><p><a target="_blank" rel="noopener" href="https://www.zybuluo.com/hanbingtao/note/433855">转载至深度学习系列教程</a></p>
<h2 id="感知器-神经元"><a href="#感知器-神经元" class="headerlink" title="感知器(神经元)"></a>感知器(神经元)</h2><p>神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是阶跃函数；而当我们说神经元时，激活函数往往选择为sigmoid函数或者tanh函数等。</p>
<p>阶跃函数：</p>
<script type="math/tex; mode=display">
f(z)=\left\{\begin{matrix}
1 &z>0 \\ 
0 & otherwise
\end{matrix}\right.</script><p>神经网络示意图：</p>
<p><img src="/2021/06/01/deep-learning-tutorial/神经网络示意图.png" alt></p>
<p>上图中每个圆圈都是一个神经元，每条线表示神经元之间的连接。我们可以看到，上面的神经元被分成了多层，层与层之间的神经元有连接，而层内之间的神经元没有连接。最左边的层叫做<strong>输入层</strong>，这层负责接收输入数据；最右边的层叫<strong>输出层</strong>，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做<strong>隐藏层</strong>。</p>
<p>隐藏层比较多（大于2）的神经网络叫做深度神经网络。而深度学习，就是使用深层架构（比如，深度神经网络）的机器学习方法。</p>
<p>那么深层网络和浅层网络相比有什么优势呢？简单来说深层网络能够表达力更强。事实上，一个仅有一个隐藏层的神经网络就能拟合任何一个函数，但是它需要很多很多的神经元。而深层网络用少得多的神经元就能拟合同样的函数。也就是为了拟合一个函数，要么使用一个浅而宽的网络，要么使用一个深而窄的网络。而后者往往更节约资源。</p>
<p>深层网络也有劣势，就是它不太容易训练。简单的说，你需要大量的数据，很多的技巧才能训练好一个深层网络。这是个手艺活。</p>
<h3 id="感知器定义"><a href="#感知器定义" class="headerlink" title="感知器定义"></a>感知器定义</h3><p><img src="/2021/06/01/deep-learning-tutorial/感知器.png" alt></p>
<p>感知器组成部分：</p>
<ul>
<li><strong>输入权值</strong> ：一个感知器可以接收多个输入{x<sub>1</sub>,x<sub>2</sub>,…,x<sub>n</sub>|x<sub>i</sub>∈R}，每个输入上有一个<strong>权值</strong>w<sub>i</sub>∈R，此外还有一个<strong>偏置项</strong>b∈R，就是上图中的w<sub>0</sub>；</li>
<li><strong>激活函数f(x)</strong>：感知器的激活函数可以有很多选择，比如sigmoid函数；</li>
<li><strong>输出</strong>：感知器的输出由下面这个公式来计算。</li>
</ul>
<script type="math/tex; mode=display">
y=f(w*x+b)</script><h3 id="感知器训练"><a href="#感知器训练" class="headerlink" title="感知器训练"></a>感知器训练</h3><p>假设损失函数为均方差函数(MSE Mean Square Error)</p>
<script type="math/tex; mode=display">
J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}(a_{i}-y_{i})^{2}\\
J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}(w_{0}x_{0}+w_{1}x_{1}+...+w_{n}x_{n}-y_{i})^{2}</script><p>a<sub>i</sub>为预测值，y<sub>i</sub>为实际值。设m=1(训练样本为1条)时</p>
<script type="math/tex; mode=display">
\frac{\partial J(w,b)}{\partial w_{1}}=x_{1}(w_{0}x_{0}+w_{1}x_{1}+...+w_{n}x_{n}-y_{i})</script><p>其余参数导数求法相同。</p>
<p>使用梯度下降更新：</p>
<script type="math/tex; mode=display">
w_{i}=w_{i}-\alpha \frac{\partial J(w_{i},b)}{\partial w_{i}}</script><p>其中α为学习率。如果损失函数内为y<sub>i</sub>-a<sub>i</sub> ，则上面的梯度下降更新负号应为正号。</p>
<h2 id="梯度下降优化算法"><a href="#梯度下降优化算法" class="headerlink" title="梯度下降优化算法"></a>梯度下降优化算法</h2><p><img src="/2021/06/01/deep-learning-tutorial/梯度下降示例图.png" alt></p>
<p>函数y=f(x)的极值点就是它的导数f<sup>‘</sup>(x)=0的那个点。因此我们可以通过解方程f<sup>‘</sup>(x)=0,求得函数的极值点。</p>
<p>对于计算机来说，随便选择一个点开始，比如上图的点x<sub>0</sub>。接下来，每次迭代修改的为x<sub>1</sub>,<sub>2</sub>,…，经过数次迭代后最终达到函数最小值点。</p>
<p>你可能要问了，为啥每次修改的值，都能往函数最小值那个方向前进呢？这里的奥秘在于，我们每次都是向函数y=f(x)的<strong>梯度</strong>的<strong>相反方向</strong>来修改。什么是<strong>梯度</strong>呢？翻开大学高数课的课本，我们会发现<strong>梯度</strong>是一个向量，它指向<strong>函数值上升最快</strong>的方向。显然，梯度的反方向当然就是函数值下降最快的方向了。我们每次沿着梯度相反方向去修改的值，当然就能走到函数的最小值附近。之所以是最小值附近而不是最小值那个点，是因为我们每次移动的步长不会那么恰到好处，有可能最后一次迭代走远了越过了最小值那个点。步长的选择是门手艺，如果选择小了，那么就会迭代很多轮才能走到最小值附近；如果选择大了，那可能就会越过最小值很远，收敛不到一个好的点上。</p>
<p>梯度下降算法的公式</p>
<script type="math/tex; mode=display">
X_{new}=X_{old}-\alpha \bigtriangledown f(x)</script><p>α为学习率（步长）</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><h3 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h3><p>神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是<strong>阶跃函数</strong>；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数。如下图所示：</p>
<p><img src="/2021/06/01/deep-learning-tutorial/sigmoid神经元.png" alt></p>
<p>计算一个神经元的输出的方法和计算一个感知器的输出是一样的。假设神经元的输入是向量<script type="math/tex">\overrightarrow{x}</script></p>
<p> ，权重向量是<script type="math/tex">\overrightarrow{w}</script>(偏置项是w<sub>0</sub>)，激活函数是sigmoid函数，则其输出y：</p>
<script type="math/tex; mode=display">
y=sigmoid(\vec{w}^{T}*\vec{x})\tag1</script><p>sigmoid函数定义如下：</p>
<script type="math/tex; mode=display">
sigmoid(x)=\frac{1}{1+e^{-x}}</script><p>将其带入前面的式子，得到</p>
<script type="math/tex; mode=display">
y=\frac{1}{1+e^{-\vec{w}^{T}\cdot \vec{x}}}</script><p>sigmoid函数是非线性函数，值域是(0,1)。函数图像如下图所示</p>
<p><img src="/2021/06/01/deep-learning-tutorial/sigmoid.jpg" alt></p>
<p>sigmoid函数的导数是：</p>
<script type="math/tex; mode=display">
令y=sigmoid(x)\\
y=\frac{1}{1+e^{-x}} \\
u(x)=1+e^{-x} \\
g(x)=e^{-x}\\
k(x)=-x\\
\frac{\mathrm{d} y}{\mathrm{d} x} = \frac{\mathrm{d} y}{\mathrm{d} u}\cdot \frac{\mathrm{d} u}{\mathrm{d} g}\cdot \frac{\mathrm{d} g}{\mathrm{d} k}\cdot \frac{\mathrm{d} k}{\mathrm{d} x}=-1\cdot u^{-2}\cdot 1\cdot e^{-x}\cdot -1=u^{-2}\cdot e^{-x}=\frac{e^{-x}}{(1+e^{-x})^{2}}=\frac{1}{1+e^{-x}}-\frac{1}{(1+e^{-x})^{2}}=y(1-y)\\
则y^{'}=y(1-y)</script><p>可以看到，sigmoid函数的导数非常有趣，它可以用sigmoid函数自身来表示。这样，一旦计算出sigmoid函数的值，计算它的导数的值就非常方便。</p>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p><img src="/2021/06/01/deep-learning-tutorial/神经网络图2.png" alt></p>
<p>神经网络其实就是按照<strong>一定规则</strong>连接起来的多个<strong>神经元</strong>。上图展示了一个<strong>全连接(full connected, FC)</strong>神经网络，通过观察上面的图，我们可以发现它的规则包括：</p>
<ul>
<li>神经元按照层来布局。最左边的层叫做<strong>输入层</strong>，负责接收输入数据；最右边的层叫<strong>输出层</strong>，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做<strong>隐藏层</strong>，因为它们对于外部来说是不可见的。</li>
<li>同一层的神经元之间没有连接。</li>
<li>第N层的每个神经元和第N-1层的<strong>所有</strong>神经元相连(这就是full connected的含义)，第N-1层神经元的输出就是第N层神经元的输入。</li>
<li>每个连接都有一个<strong>权值</strong>。</li>
</ul>
<p>上面这些规则定义了全连接神经网络的结构。事实上还存在很多其它结构的神经网络，比如卷积神经网络(CNN)、循环神经网络(RNN)，他们都具有不同的连接规则。</p>
<h3 id="计算神经网络输出"><a href="#计算神经网络输出" class="headerlink" title="计算神经网络输出"></a>计算神经网络输出</h3><p>神经网络实际上就是一个输入向量<script type="math/tex">\vec{x}</script>到输出向量<script type="math/tex">\vec{y}</script>的函数，即：</p>
<script type="math/tex; mode=display">
\vec{y}=f_{network}(\vec{x})</script><p>根据输入计算神经网络的输出，需要首先将输入向量<script type="math/tex">\vec{x}</script>的每个元素<script type="math/tex">x_{i}</script>的值赋给神经网络的输入层的对应神经元，然后根据<strong>式1</strong>依次向前计算每一层的每个神经元的值，直到最后一层输出层的所有神经元的值计算完毕。最后，将输出层每个神经元的值串在一起就得到了输出向量<script type="math/tex">\vec{y}</script>。</p>
<p>接下来举一个例子来说明这个过程，我们先给神经网络的每个单元写上编号。</p>
<p><img src="/2021/06/01/deep-learning-tutorial/神经网络过程图.png" alt></p>
<p>如上图，输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的4个节点，编号依次为4、5、6、7；最后输出层的两个节点编号为8、9。因为我们这个神经网络是<strong>全连接</strong>网络，所以可以看到每个节点都和<strong>上一层的所有节点</strong>有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为<script type="math/tex">w_{41},w_{42},w_{43}</script>。那么，我们怎样计算节点4的输出值<script type="math/tex">a_{4}</script>呢？</p>
<p>为了计算节点4的输出值，我们必须先得到其所有上游节点（也就是节点1、2、3）的输出值。节点1、2、3是<strong>输入层</strong>的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点1、2、3的输出值分别是<script type="math/tex">x_{1},x_{2},x_{3}</script>。我们要求<strong>输入向量的维度和输入层神经元个数相同</strong>，而输入向量的某个元素对应到哪个输入节点是可以自由决定的，你偏非要把赋值给节点2也是完全没有问题的，但这样除了把自己弄晕之外，并没有什么价值。</p>
<p>一旦我们有了节点1、2、3的输出值，我们就可以根据<strong>式1</strong>计算节点4的输出值<script type="math/tex">a_{4}</script>：</p>
<script type="math/tex; mode=display">
a_{4}=sigmoid(w_{41}x_{1}+w_{42}x_{2}+w_{43}x_{3}+w_{4b})</script><p>上式的<script type="math/tex">w_{4b}</script>是节点4的<strong>偏置项</strong>，图中没有画出来。而分别为节点1、2、3到节点4连接的权重，在给权重<script type="math/tex">w_{ji}</script>编号时，我们把目标节点<script type="math/tex">j</script>的编号放在前面，把源节点<script type="math/tex">i</script>的编号放在后面。</p>
<p>同样，我们可以继续计算出节点5、6、7的输出值<script type="math/tex">a_{5},a_{6},a_{7}</script>。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值<script type="math/tex">y_{1}</script>：</p>
<script type="math/tex; mode=display">
y_{1}=sigmoid(w_{84}a_{4}+w_{85}a_{5}+w_{86}a_{6}+w_{87}a_{7}+w_{8b})</script><p>同理，我们还可以计算出<script type="math/tex">y_{2}</script>的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量<script type="math/tex">\vec{x}=\begin{bmatrix}
x_{1}\\ 
x_{2}\\ 
x_{3}
\end{bmatrix}</script>时，神经网络的输出向量<script type="math/tex">\vec{y}=\begin{bmatrix}
y_{1}\\ 
y_{2}\\ 
\end{bmatrix}</script>。这里我们也看到，<strong>输出向量的维度和输出层神经元个数相同</strong>。</p>
<h3 id="神经网络的矩阵表示"><a href="#神经网络的矩阵表示" class="headerlink" title="神经网络的矩阵表示"></a>神经网络的矩阵表示</h3><p>神经网络的计算如果用矩阵来表示会很方便（当然逼格也更高），我们先来看看隐藏层的矩阵表示。</p>
<p>首先我们把隐藏层4个节点的计算依次排列出来：</p>
<script type="math/tex; mode=display">
a_{4}=sigmoid(w_{41}x_{1}+w_{42}x_{2}+w_{43}x_{3}+w_{4b})\\
a_{5}=sigmoid(w_{51}x_{1}+w_{52}x_{2}+w_{53}x_{3}+w_{5b})\\
a_{6}=sigmoid(w_{61}x_{1}+w_{62}x_{2}+w_{63}x_{3}+w_{6b})\\
a_{7}=sigmoid(w_{71}x_{1}+w_{72}x_{2}+w_{73}x_{3}+w_{7b})</script><p>接着，定义网络的输入向量<script type="math/tex">\vec{x}</script>和隐藏层每个节点的权重向量<script type="math/tex">\vec{w_{j}}</script>。令</p>
<script type="math/tex; mode=display">
\vec{x}=\begin{bmatrix}
x_{1}\\ 
x_{2}\\ 
x_{3}
\end{bmatrix}\\
\vec{w_{4}}=\begin{bmatrix}
w_{41}, &w_{42},  & w_{43}, & w_{4b}
\end{bmatrix}\\
\vec{w_{5}}=\begin{bmatrix}
w_{51}, &w_{52},  & w_{53}, & w_{5b}
\end{bmatrix}\\
\vec{w_{6}}=\begin{bmatrix}
w_{61}, &w_{62},  & w_{63}, & w_{6b}
\end{bmatrix}\\
\vec{w_{7}}=\begin{bmatrix}
w_{71}, &w_{72},  & w_{73}, & w_{7b}
\end{bmatrix}\\
f=sigmoid</script><p>代入到前面的一组式子，得到：</p>
<script type="math/tex; mode=display">
a_{4}=f(\vec{w_{4}}\cdot \vec{x})\\
a_{5}=f(\vec{w_{5}}\cdot \vec{x})\\
a_{6}=f(\vec{w_{6}}\cdot \vec{x})\\
a_{7}=f(\vec{w_{7}}\cdot \vec{x})\\</script><p>现在，我们把上述计算<script type="math/tex">a_{4},a_{5},a_{6},a_{7}</script>的四个式子写到一个矩阵里面，每个式子作为矩阵的一行，就可以利用矩阵来表示它们的计算了。令</p>
<script type="math/tex; mode=display">
\vec{a}=\begin{bmatrix}
a_{4}\\ 
a_{5}\\ 
a_{6}\\ 
a_{7}
\end{bmatrix}\\
W=\begin{bmatrix}
\vec{w_{4}}\\ 
\vec{w_{5}}\\ 
\vec{w_{6}}\\ 
\vec{w_{7}}
\end{bmatrix}=\begin{bmatrix}
w_{41} &w_{42}  & w_{43} &w_{4b} \\ 
w_{51} &w_{52}  & w_{53} &w_{5b} \\ 
w_{61} &w_{62}  & w_{63} &w_{6b} \\ 
w_{71} &w_{72}  & w_{73} &w_{7b} 
\end{bmatrix}\\
f(\begin{bmatrix}
x_{1}\\ 
x_{2}\\ 
x_{3}\\ 
.\\ 
.\\ 
.
\end{bmatrix})=\begin{bmatrix}
f(x_{1})\\ 
f(x_{2})\\ 
f(x_{3})\\ 
.\\ 
.\\ 
.
\end{bmatrix}</script><p>带入前面的一组式子，得到</p>
<script type="math/tex; mode=display">
\vec{a}=f(W\cdot\vec{x}) \tag2</script><p>在<strong>式2</strong>中，是激活函数，在本例中是sigmoid函数；W是某一层的权重矩阵；<script type="math/tex">\vec{x}</script>是某层的输入向量；<script type="math/tex">\vec{a}</script>是某层的输出向量。<strong>式2</strong>说明神经网络的每一层的作用实际上就是先将输入向量<strong>左乘</strong>一个数组进行线性变换，得到一个新的向量，然后再对这个向量<strong>逐元素</strong>应用一个激活函数。</p>
<p>每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重矩阵分别为<script type="math/tex">W_{1},W_{2},W_{3},W_{4}</script>，每个隐藏层的输出分别是<script type="math/tex">\vec{a_{1}},\vec{a_{2}},\vec{a_{3}}</script>，神经网络的输入为<script type="math/tex">\vec{x}</script>，神经网络的输出为<script type="math/tex">\vec{y}</script>，如下图所示：</p>
<p><img src="/2021/06/01/deep-learning-tutorial/深层神经网络.png" alt></p>
<p>则每一层的输出向量的计算可以表示为：</p>
<script type="math/tex; mode=display">
\vec{a_{1}}=f(W_{1}\cdot \vec{x})\\
\vec{a_{2}}=f(W_{2}\cdot \vec{a_{1}})\\
\vec{a_{3}}=f(W_{3}\cdot \vec{a_{2}})\\
\vec{y}=f(W_{4}\cdot \vec{a_{3}})</script><p>这就是神经网络输出值的计算方法。</p>
<h3 id="神经网络的训练"><a href="#神经网络的训练" class="headerlink" title="神经网络的训练"></a>神经网络的训练</h3><p>现在，我们需要知道一个神经网络的每个连接上的权值是如何得到的。我们可以说神经网络是一个<strong>模型</strong>，那么这些权值就是模型的<strong>参数</strong>，也就是模型要学习的东西。然而，一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为<strong>超参数(Hyper-Parameters)</strong>。</p>
<p>接下来，我们将要介绍神经网络的训练算法：反向传播算法。</p>
<h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p>我们首先直观的介绍反向传播算法，最后再来介绍这个算法的推导。当然读者也可以完全跳过推导部分，因为即使不知道如何推导，也不影响你写出来一个神经网络的训练代码。事实上，现在神经网络成熟的开源实现多如牛毛，除了练手之外，你可能都没有机会需要去写一个神经网络。</p>
<p>我们设神经元的激活函数f为函数sigmoid函数。</p>
<p>我们假设每个训练样本为<script type="math/tex">(\vec{x},\vec{t})</script>，其中向量<script type="math/tex">\vec{x}</script>是训练样本的特征，而<script type="math/tex">\vec{t}</script>是样本的目标值。</p>
<p><img src="/2021/06/01/deep-learning-tutorial/反向传播.png" alt></p>
<p>首先，我们根据上一节介绍的算法，用样本的特征<script type="math/tex">\vec{x}</script>，计算出神经网络中每个隐藏层节点的输出<script type="math/tex">a_{i}</script>，以及输出层每个节点的输出<script type="math/tex">y_{i}</script>。</p>
<p>我们取网络所有输出层节点的误差平方和作为目标函数：</p>
<script type="math/tex; mode=display">
E_{d}=\frac{1}{2}\sum_{i=1}^{m}(t_{i}-y_{i})^{2}</script><p>其中<script type="math/tex">m</script>表示输出节点数目，<script type="math/tex">E_{d}</script>表示样本d的误差。</p>
<p>然后，我们用<strong>随机梯度下降</strong>算法对目标函数进行优化：</p>
<script type="math/tex; mode=display">
w_{ji}\leftarrow w_{ji}-\eta\frac{\partial E_{d}}{\partial w_{ji}}</script><p>随机梯度下降算法也就是需要求出误差对于每个权重的偏导数（也就是梯度），怎么求呢？</p>
<p><img src="/2021/06/01/deep-learning-tutorial/神经网络过程图.png" alt></p>
<p>观察上图，我们发现权重<script type="math/tex">w_{ji}</script>仅能通过影响节点<script type="math/tex">j</script>的输入值影响网络的其它部分，设<script type="math/tex">net_{j}</script>是节点<script type="math/tex">j</script>的<strong>加权输入</strong>，即</p>
<script type="math/tex; mode=display">
net_{j}=\vec{w_{j}}\cdot \vec{x_{j}}=\sum_{i=1}^{n}w_{ji}x_{ji}</script><script type="math/tex; mode=display">E_{d}$$是$$net_{j}$$的函数，而$$net_{j}$$是$$w_{ji}$$的函数。根据链式求导法则，可以得到：</script><p>\frac{\partial E_{d}}{\partial w_{ji}}=\frac{\partial E_{d}}{\partial net_{j}}\frac{\partial net_{j}}{\partial w_{ji}}=\frac{\partial E_{d}}{\partial net_{j}}\frac{\partial \sum _{i}w_{ji}x_{ji}}{\partial w_{ji}}=\frac{\partial E_{d}}{\partial net_{j}}x_{ji}</p>
<script type="math/tex; mode=display">
上式中，$$x_{ji}$$是节点$$i$$传递给节点$$j$$的输入值，也就是节点$$i$$的输出值。

对于$$\frac{\partial E_{d}}{\partial net_{j}}$$的推导，需要区分**输出层**和**隐藏层**两种情况。

#### 输出层权值训练

对于**输出层**来说，$$net_{j}$$仅能通过节点$$j$$的输出值$$y_{j}$$来影响网络其它部分，也就是说$$E_{d}$$是$$y_{j}$$的函数，而$$y_{j}$$是$$net_{j}$$的函数，其中$$y_{j}=sigmoid(net_{j})$$。所以我们可以再次使用链式求导法则：</script><p>\frac{\partial E_{d}}{\partial net_{j}}=\frac{\partial E_{d}}{\partial y_{j}}\frac{\partial y_{j}}{\partial net_{j}}</p>
<script type="math/tex; mode=display">
考虑上式第一项:</script><p>\frac{\partial E_{d}}{\partial y_{j}}=\frac{\partial \frac{1}{2}\sum (t_{j}-y_{j})^{2}}{\partial y_{j}}=-(t_{j}-y_{j})</p>
<script type="math/tex; mode=display">
考虑上式第二项：</script><p>\frac{\partial y_{j}}{\partial net_{j}}=\frac{\partial sigmoid(net_{j})}{\partial net_{j}}=y_{j}(1-y_{j})</p>
<script type="math/tex; mode=display">
将第一项和第二项带入，得到：</script><p>\frac{\partial E_{d}}{\partial net_{j}}=-(t_{j}-y_{j})y_{j}(1-y_{j})</p>
<script type="math/tex; mode=display">
如果令$$\delta _{j}=-\frac{\partial E_{d}}{\partial net_{j}}$$，也就是一个节点的误差项是网络误差$$\delta$$对这个节点输入的偏导数的相反数。带入上式，得到：</script><p>\delta _{j}=(t_{j}-y_{j})y_{j}(1-y_{j})</p>
<script type="math/tex; mode=display">
将上述推导带入随机梯度下降公式，得到：</script><p>w_{ji}\leftarrow w_{ji}-\eta \frac{\partial E_{d}}{\partial w_{ji}}=w_{ji}-\eta \frac{\partial E_{d}}{\partial net_{ji}}\frac{\partial net_{ji}}{\partial w_{ji}}=w_{ji}+\eta (t_{j}-y_{j})y_{j}(1-y_{j})(1-y_{j})x_{ji}=w_{ji}+\eta \delta _{j}x_{ji}</p>
<script type="math/tex; mode=display">

#### 隐藏层权值训练

现在我们要推导出隐藏层的$$\frac{\partial E_{d}}{\partial net_{j}}$$。

首先，我们需要定义节点$$j$$的所有直接下游节点的集合$$Downstream(j)$$。例如，对于节点4来说，它的直接下游节点是节点8、节点9。可以看到$$net_{j}$$只能通过影响$$Downstream(j)$$再影响$$E_{d}$$。设$$net_{k}$$是节点$$j$$的下游节点的输入，则$$E_{d}$$是$$net_{k}$$的函数，而$$net_{k}$$是$$net_{j}$$的函数。因为$$net_{k}$$有多个，我们应用全导数公式，可以做出如下推导：</script><p>\frac{\partial E_{d}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}\frac{\partial E_{d}}{\partial net_{k}}\frac{\partial net_{k}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}-\delta _{k}\frac{\partial net_{k}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}-\delta _{k}\frac{\partial net_{k}}{\partial a_{j}}\frac{\partial a_{j}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}-\delta _{k}w_{kj}\frac{\partial a_{j}}{\partial net_{j}}\\<br>=\sum_{k\in Downstream(j)}-\delta _{k}w_{kj}a_{j}(1-a_{j})\\<br>=-a_{j}(1-a_{j})\sum_{k\in Downstream(j)}\delta _{k}w_{kj}</p>
<script type="math/tex; mode=display">
因为$$\delta _{j}=-\frac{\partial E_{d}}{\partial net_{j}}$$带入上式得到：</script><p>\delta _{j}=a_{j}(1-a_{j})\sum_{k\in Downstream(j)}\delta _{k}w_{kj}</p>
<script type="math/tex; mode=display">
其中$$a_{j}$$为激活函数。

## 卷积神经网络

### 全连接网络VS卷积网络

全连接神经网络之所以不太适合图像识别任务，主要有以下几个方面的问题：

- **参数数量太多** 考虑一个输入$$1000*1000$$像素的图片(一百万像素，现在已经不能算大图了)，输入层有$$1000*1000=100$$万节点。假设第一个隐藏层有100个节点(这个数量并不多)，那么仅这一层就有$$(1000*1000+1)*100=1$$亿​参数，这实在是太多了！我们看到图像只扩大一点，参数数量就会多很多，因此它的扩展性很差。
- **没有利用像素之间的位置信息** 对于图像识别任务来说，每个像素和其周围像素的联系是比较紧密的，和离得很远的像素的联系可能就很小了。如果一个神经元和上一层所有神经元相连，那么就相当于对于一个像素来说，把图像的所有像素都等同看待，这不符合前面的假设。当我们完成每个连接权重的学习之后，最终可能会发现，有大量的权重，它们的值都是很小的(也就是这些连接其实无关紧要)。努力学习大量并不重要的权重，这样的学习必将是非常低效的。
- **网络层数限制** 我们知道网络层数越多其表达能力越强，但是通过梯度下降方法训练深度全连接神经网络很困难，因为全连接神经网络的梯度很难传递超过3层。因此，我们不可能得到一个很深的全连接神经网络，也就限制了它的能力。

那么，卷积神经网络又是怎样解决这个问题的呢？主要有三个思路：

- **局部连接** 这个是最容易想到的，每个神经元不再和上一层的所有神经元相连，而只和一小部分神经元相连。这样就减少了很多参数。
- **权值共享** 一组连接可以共享同一个权重，而不是每个连接有一个不同的权重，这样又减少了很多参数。
- **下采样** 可以使用Pooling来减少每层的样本数，进一步减少参数数量，同时还可以提升模型的鲁棒性。

对于图像识别任务来说，卷积神经网络通过尽可能保留重要的参数，去掉大量不重要的参数，来达到更好的学习效果。

### 卷积神经网络结构

![](./deep-learning-tutorial/卷积神经网络案例图.png)

三维的层结构

从**图中**我们可以发现**卷积神经网络**的层结构和**全连接神经网络**的层结构有很大不同。**全连接神经网络**每层的神经元是按照**一维**排列的，也就是排成一条线的样子；而**卷积神经网络**每层的神经元是按照**三维**排列的，也就是排成一个长方体的样子，有**宽度**、**高度**和**深度**。

对于**图中**展示的神经网络，我们看到输入层的宽度和高度对应于输入图像的宽度和高度，而它的深度为1。接着，第一个卷积层对这幅图像进行了卷积操作(后面我们会讲如何计算卷积)，得到了三个Feature Map。这里的"3"可能是让很多初学者迷惑的地方，实际上，就是这个卷积层包含三个Filter，也就是三套参数，每个Filter都可以把原始输入图像卷积得到一个Feature Map，三个Filter就可以得到三个Feature Map。至于一个卷积层可以有多少个Filter，那是可以自由设定的。也就是说，卷积层的Filter个数也是一个**超参数**。我们可以把Feature Map可以看做是通过卷积变换提取到的图像特征，三个Filter就对原始图像提取出三组不同的特征，也就是得到了三个Feature Map，也称做三个**通道(channel)**。

继续观察**图**，在第一个卷积层之后，Pooling层对三个Feature Map做了**下采样**(后面我们会讲如何计算下采样)，得到了三个更小的Feature Map。接着，是第二个**卷积层**，它有5个Filter。每个Fitler都把前面**下采样**之后的**3个\**Feature Map**卷积**在一起，得到一个新的Feature Map。这样，5个Filter就得到了5个Feature Map。接着，是第二个Pooling，继续对5个Feature Map进行**下采样**，得到了5个更小的Feature Map。

如图所示网络的最后两层是全连接层。第一个全连接层的每个神经元，和上一层5个Feature Map中的每个神经元相连，第二个全连接层(也就是输出层)的每个神经元，则和第一个全连接层的每个神经元相连，这样得到了整个网络的输出。

至此，我们对**卷积神经网络**有了最基本的感性认识。接下来，我们将介绍**卷积神经网络**中各种层的计算和训练。

### 卷积神经网络输出值的计算

#### 卷积层输出值的计算

我们用一个简单的例子来讲述如何计算**卷积**，然后，我们抽象出**卷积层**的一些重要概念和计算方法。

假设有一个5*5的图像，使用一个3*3的filter进行卷积，想得到一个3*3的Feature Map，如下所示：

![](./deep-learning-tutorial/卷积神经网络案例.png)

为了清楚的描述**卷积**计算过程，我们首先对图像的每个像素进行编号，用$$x_{i,j}$$表示图像的第行第列元素；对filter的每个权重进行编号，用$$w_{m,n}$$表示第$$m$$行第$$n$$列权重，用$$w_{b}$$表示filter的**偏置项**；对Feature Map的每个元素进行编号，用$$a_{i,j}$$表示Feature Map的第$$i$$行第$$j$$列元素；用$$f$$表示**激活函数**(这个例子选择**relu函数**作为激活函数)。然后，使用下列公式计算卷积：</script><p>a_{i,j}=f(\sum_{m=0}^{2}\sum_{n=0}^{2}w_{m,n}x_{i+m,j+n}+w_{b})</p>
<script type="math/tex; mode=display">
例如，对于Feature Map左上角元素来说，其卷积计算方法为：</script><p>a_{0,0}=f(\sum_{m=0}^{2}\sum_{n=0}^{2}w_{m,n}x_{m+0,n+0}+w_{b})\\<br>=relu(w_{0,0}x_{0,0}+w_{0,1}x_{0,1}+w_{0,2}x_{0,2}+w_{1,0}x_{1,0}+w_{1,1}x_{1,1}+w_{1,2}x_{1,2}+w_{2,0}x_{2,0}+w_{2,1}x_{2,1}+w_{2})\\<br>=relu(1+0+1+0+1+0+0+0+1+0)=relu(4)=4</p>
<script type="math/tex; mode=display">
计算结果如下图所示：

![](./deep-learning-tutorial/卷积过程.png)

###  权值共享

神经元的偏置部分也是同一种滤波器共享的。 比如卷积核是三层，每一层使用的偏置项都是相等的。

## 对抗生成网络(GAN)



## 优化器

### Batch Gradient Descent(BGD，批量梯度下降)

BGD训练过程中每次迭代使用所有样本来进行梯度的更新。

#### 优点

- 一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行；
- 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。

#### 缺点

- 当样本数目很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。

### Stochastic Gradient Descent(SGD,随机梯度下降)

SGD训练过程中每次迭代使用一个样本来对参数进行更新。

#### 优点

- 由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。

#### 缺点

- 准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛；
- 可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势；
- 不易于并行实现。

### Mini-Batch Gradient Descent(MBGD,小批量梯度下降)

MBGD训练过程中每次迭代使用**batch size**个样本来对参数进行更新。它是对BGD以及SGD的一个折中办法。

#### 优点

- 通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多；
- 每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果；
- 可实现并行化。

#### 缺点

- batch size的不当选择可能会带来一些问题。

  batcha size的选择带来的影响：

  - 在合理的范围内，增大batch_size的好处：

    - 内存利用率提高了，大矩阵乘法的并行化效率提高。

    - 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。

    - 在一定范围内，一般来说 batch size 越大，其确定的下降方向越准，引起训练震荡越小。

  - 盲目增大batch size的坏处

    - 内存利用率提高了，但是内存容量可能撑不住了。
    - 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。
    - batch size 增大到一定程度，其确定的下降方向已经基本不再变化



### Momentum

[]: https://blog.csdn.net/willduan1/article/details/78070086

SGD参数更新公式为：</script><p>W:=W-\alpha d_{w}\\<br>b:=b-\alpha d_{b}</p>
<script type="math/tex; mode=display">
它的梯度更新路线上下波动很大，收敛速度很慢。因此根据这些原因，有人提出了Momentum优化算法，这个是基于SGD的，简单理解就是为了防止波动，它主要是基于梯度的移动指数加权平均来更新参数。引进超参数beta(一般取0.9)。

在讲这个算法之前说一下移动指数加权平均。移动指数加权平均法加权就是根据同一个移动段内不同时间的数据对预测值的影响程度，分别给予不同的权数，然后再进行平均移动以预测未来值。假定给定一系列数据值$$x_{1}, x_{2},x_{3},……x_{n}$$。那么，我们根据这些数据来拟合一条曲线，所得的值$$v_{1}, v_{2}…..$$就是如下的公式：</script><p>v_{0} = 0\\<br>v_{1} = \beta v_{0}+(1-\beta )x_{0} \\<br>v_{2} = \beta v_{1}+(1-\beta )x_{1} \\</p>
<script type="math/tex; mode=display">
参数更新公式为：</script><p>V_{dw}=\beta V_{dw}+(1-\beta )dW\\<br>V_{db}=\beta V_{db}+(1-\beta )db\\<br>W:=W-\alpha {V_{dw}}\\<br>b:=b-\alpha {V_{db}}</p>
<script type="math/tex; mode=display">

### Nesterov Momentum

Nesterov Momentum是对Momentum的改进，可以理解为nesterov动量在标准动量方法中添加了一个**校正因子**。

### Root Mean Square Prop(RMSProp)

RMSProp思想与Momentum相似，也用到权重超参数beta（一般取0.999）。

参数更新公式为：</script><p>S_{dw}=\beta S_{dw}+(1-\beta )dW^{2}\\<br>S_{db}=\beta S_{db}+(1-\beta )db^{2}\\<br>W:=W-\alpha \frac{dW}{\sqrt{S_{dw}}}\\<br>b:=b-\alpha \frac{db}{\sqrt{S_{db}}}</p>
<script type="math/tex; mode=display">
为了防止分母为0，在分数下加上个特别小的值epsilon，通常选取10^-8。

### Adagrad

大多数优化器训练参数更新过程中都使用了相同的学习率α。Adagrad能够在训练中自动的对learning rate进行调整，对于出现频率较低的参数采用较大的α更新，相反，对于出现频率较高的参数采用较小的α更新。因此，**Adagrad非常适合处理稀疏数据**。

如果是普通的SGD，那么每一时刻梯度的更新公式为：</script><p>\Theta _{t+1}=\Theta _{t,i}-\alpha *g_{t,i}</p>
<script type="math/tex; mode=display">
g<sub>t,i</sub>为第t轮第i个参数的梯度。θ<sub>t,i</sub> 为参数值

Adagrad在每轮训练中对每个参数θ<sub>i</sub> 进行更新，参数更新公式为：</script><p>\Theta _{t+1,i}=\Theta _{t,i}-\frac{\alpha }{\sqrt{G_{t,ii}+\varepsilon }}*g_{t,i}</p>
<script type="math/tex; mode=display">
G<sub>t</sub>为对角矩阵，大小为D*D。每个对角线位置i,i为对应参数θ<sub>i</sub> 从第一轮到第t轮梯度的平方和。varepsilon 是平滑项，用于避免分母为0，一般取值为10^-8。Adagrad的缺点是在训练的中后期，分母上梯度平方的累加会越来越大，从而梯度趋近于0，使得训练提前结束。

### Adadelta

Adadelta 是 Adagrad 的一个具有更强鲁棒性的的扩展版本，它不是累积所有过去的梯度，而是根据渐变更新的移动窗口调整学习速率。 这样，即使进行了许多更新，Adadelta 仍在继续学习。   

与Adagrad 相比，就是分母的 G 换成了过去的梯度平方的衰减平均值，**指数衰减平均值**。

这个分母相当于**梯度的均方根 root mean squared (RMS)**，在数据统计分析中，将所有值平方求和，求其均值，再开平方，就得到均方根值 。

#### 优点

- 防止学习率衰减或梯度消失等问题的出现。

### Adam

该优化器相当于RMSprop+Momentum。

参数更新公式为：</script><p>V_{dw}=\beta _{1} V_{dw}+(1-\beta _{1} )dW\\<br>V_{db}=\beta _{1} V_{db}+(1-\beta _{1} )db\\<br>S_{dw}=\beta_{2} S_{dw}+(1-\beta_{2} )dW^{2}\\<br>S_{db}=\beta_{2} S_{db}+(1-\beta_{2} )db^{2}\\<br>V_{dW}^{corrected}=\frac{V_{dW}}{1-\beta _{1}^{t}}\\<br>V_{db}^{corrected}=\frac{V_{db}}{1-\beta _{1}^{t}}\\<br>S_{dW}^{corrected}=\frac{S_{dW}}{1-\beta _{2}^{t}}\\<br>S_{db}^{corrected}=\frac{S_{db}}{1-\beta _{2}^{t}}\\<br>W:=W-\alpha \frac{V_{dW}}{\sqrt{S_{dW}^{corrected}}}\\<br>b:=b-\alpha \frac{V_{db}}{\sqrt{S_{db}^{corrected}}}</p>
<script type="math/tex; mode=display">
beta1一般为0.9，beta2一般为0.9999。在实际应用中，Adam方法效果良好。与其他自适应学习率算法相比，其收敛速度更快，学习效果更为有效，而且可以纠正其他优化技术中存在的问题，如学习率消失、收敛过慢或是高方差的参数更新导致损失函数波动较大等问题。

## 激活函数

[激活函数详解](<https://zhuanlan.zhihu.com/p/22142013>)

激活函数一般用于神经网络的层与层之间，将上一层的输出转换之后输入到下一层。如果没有激活函数引入的额非线性特性，那么神经网络就只相当于原始感知机的矩阵相乘。  

### Sigmoid

![](./deep-learning-tutorial/sigmoid.jpg)

sigmoid在定义域内处处可导，且两侧导数逐渐趋近于0。

#### 缺点

- 激活函数计算量大，反向传播求误差梯度时，求导涉及除法；
- 反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练；
- Sigmoids函数饱和且kill掉梯度；
- Sigmoids函数收敛缓慢。

### tanh

![](./deep-learning-tutorial/tanh.jpg)

### Relu

![](./deep-learning-tutorial/relu.jpg)

#### 优点

- **速度快** 和sigmoid函数需要计算指数和倒数相比，relu函数其实就是一个max(0,x)，计算代价小很多。
- **减轻梯度消失问题** relu函数的导数是1，不会导致梯度变小。当然，激活函数仅仅是导致梯度减小的一个因素，但无论如何在这方面relu的表现强于sigmoid。使用relu激活函数可以让你训练更深的网络。
- **稀疏性** 通过对大脑的研究发现，大脑在工作的时候只有大约5%的神经元是激活的，而采用sigmoid激活函数的人工神经网络，其激活率大约是50%。有论文声称人工神经网络在15%-30%的激活率时是比较理想的。因为relu函数在输入小于0时是完全不激活的，因此可以获得一个更低的激活率。

### 缺点

- 训练的时候很”脆弱”，很容易就”die”了。

### PRelu

![](./deep-learning-tutorial/PRelu.jpg)

### RReLU

### Maxout

### ELU

![](./deep-learning-tutorial/elu.jpg)



## 问题及解决方法

### 梯度爆炸与梯度消失

深度神经网络训练过程中，使用了反向传播的方式更新参数，该方式基于的是链式求导。计算每层梯度的时候回设计一些连乘操作，如果网络过深，当连乘的因子大部分小于1时，最后的乘积结果可能趋于0，会导致后面的网络层参数不发生变化，不能继续进行学习（梯度消失）。当连乘的因子大部分大于1，最后的乘积结果可能趋于无穷，会导致后面的网络层参数变化过大，导致Loss值出现震荡，收敛不到最低值的情况（梯度爆炸）。

#### 梯度爆炸解决办法

- 降低学习率

- 梯度裁剪（Gradient Clipping）

  如果梯度特别大，那么将其投影到一个比较小的尺度上。

### 线性与非线性

在数学上可理解为一阶导数为常数的函数为线性函数，一阶导数不为常数的函数为非线性函数。

### 为什么RNN一般情况下为等长的

为了让多条数据合并成矩阵进行运算，能够使用并行处理。如果不等长则不能合并为矩阵。tensorflow支持同一批训练数据等长的训练接口。

### Padding 等于SAME和VALID

Padding运算作用于输入向量的每一维，每一维的操作都是一致的，所以理解Padding的操作，只需要理解一维向量的padding过程

假设一个一维向量，输入形状为input_size，经过滤波操作后的输出形状为output_size，滤波窗口为filter_size，需要padding的个数为padding_needed，滤波窗口滑动步长为stride，则之间满足关系：</script><p>output_size=(input_size+padding_needed-filter_size)/stride+1</p>
<p>$$<br>由公式可知，指定padding_needed可以确定output_size的值，反过来，如果已知输出的形状，则进而可以确定padding的数量。</p>
<p>这是两种处理padding的方案，pytorch采用的是第一种，即在卷积或池化时先确定padding数量，自动推导输出形状；tensorflow和caffe采用的是更为人熟知的第二种，即先根据Valid还是Same确定输出大小，再自动确定padding的数量</p>
<p>Valid和Same是预设的两种padding模式，Valid指不padding，same指输出大小尽可能和输入大小成比例</p>
<p>下面是tensorflow计算padding的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">作者：JamesPlur</span><br><span class="line">链接：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;73118626</span><br><span class="line">来源：知乎</span><br><span class="line">著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</span><br><span class="line"></span><br><span class="line">void GetWindowedOutputSize(int64_t input_size, int32_t filter_size, int32_t dilation_rate,</span><br><span class="line">                           int32_t stride, const std::string&amp; padding_type, </span><br><span class="line">                           int64_t* output_size,int32_t* padding_before, </span><br><span class="line">                           int32_t* padding_after) &#123;</span><br><span class="line">  CHECK_GT(stride, 0);</span><br><span class="line">  CHECK_GE(dilation_rate, 1);</span><br><span class="line"></span><br><span class="line">  int32_t effective_filter_size &#x3D; (filter_size - 1) * dilation_rate + 1;</span><br><span class="line">  if (padding_type &#x3D;&#x3D; &quot;valid&quot;) &#123;</span><br><span class="line">    if (output_size) &#123; *output_size &#x3D; (input_size - effective_filter_size + stride) &#x2F; stride; &#125;</span><br><span class="line">    if (padding_before) &#123; *padding_before &#x3D; 0; &#125;</span><br><span class="line">    if (padding_after) &#123; *padding_after &#x3D; 0; &#125;</span><br><span class="line">  &#125; else if (padding_type &#x3D;&#x3D; &quot;same&quot;) &#123;</span><br><span class="line">    int64_t tmp_output_size &#x3D; (input_size + stride - 1) &#x2F; stride;</span><br><span class="line">    if (output_size) &#123; *output_size &#x3D; tmp_output_size; &#125;</span><br><span class="line">    const int32_t padding_needed &#x3D; std::max(</span><br><span class="line">        0,</span><br><span class="line">        static_cast&lt;int32_t&gt;((tmp_output_size - 1) * stride + effective_filter_size - input_size));</span><br><span class="line">    &#x2F;&#x2F; For odd values of total padding, add more padding at the &#39;right&#39;</span><br><span class="line">    &#x2F;&#x2F; side of the given dimension.</span><br><span class="line">    if (padding_before) &#123; *padding_before &#x3D; padding_needed &#x2F; 2; &#125;</span><br><span class="line">    if (padding_after) &#123; *padding_after &#x3D; padding_needed - padding_needed &#x2F; 2; &#125;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    UNIMPLEMENTED();</span><br><span class="line">  &#125;</span><br><span class="line">  if (output_size) &#123; CHECK_GE((*output_size), 0); &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h2><h2 id="AlexNet深度卷积神经网络"><a href="#AlexNet深度卷积神经网络" class="headerlink" title="AlexNet深度卷积神经网络"></a>AlexNet深度卷积神经网络</h2><h3 id="深度卷积神经网络图像集分类"><a href="#深度卷积神经网络图像集分类" class="headerlink" title="深度卷积神经网络图像集分类"></a>深度卷积神经网络图像集分类</h3><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>我们训练了一个庞大的深层卷积神经网络，将ImageNet LSVRC-2010比赛中的120万张高分辨率图像分为1000个不同的类别。在测试数据上，我们取得了37.5％和17.0％的前1和前5的错误率，这比以前的先进水平要好得多。具有6000万个参数和650,000个神经元的神经网络由五个卷积层组成，其中一些随后是最大池化层，三个全连接层以及最后的1000个softmax输出。为了加快训练速度，我们使用非饱和神经元和能高效进行卷积运算的GPU实现。为了减少全连接层中的过拟合，我们采用了最近开发的称为“dropout”的正则化方法，该方法证明是非常有效的。我们还在ILSVRC-2012比赛中使用了这种模式的一个变种，取得了15.3％的前五名测试失误率，而第二名的成绩是26.2％。</p>
<h4 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h4><p>目前，机器学习方法对物体识别非常重要。为了改善他们的表现，我们可以收集更大的数据集，训练更强大的模型，并使用更好的技术来防止过拟合。直到最近，标记好图像的数据集相对还较小——大约上万的数量级（例如，NORB [16]，Caltech-101/256 [8,9]和CIFAR-10/100 [12]）。使用这种规模的数据集可以很好地解决简单的识别任务，特别是如果他们增加了保留标签转换（label-preserving transformations）。例如，目前MNIST数字识别任务的最低错误率（&lt;0.3％）基本达到了人类的识别水平[4]。但是物体在现实环境中可能表现出相当大的变化性，所以要学会识别它们，就必须使用更大的训练集。事实上，小图像数据集的缺点已是众所周知（例如，Pinto[21]），但直到最近才可以收集到数百万的标记数据集。新的大型数据集包括LabelMe [23]，其中包含数十万个完全分割的图像，以及ImageNet [6]，其中包含超过15,000万个超过22,000个类别的高分辨率图像。<br>要从数百万图像中学习数千个类别，我们需要一个具有强大学习能力的模型。然而，物体识别任务的巨大复杂性意味着即使是像ImageNet这样大的数据集也不能完美地解决这个问题，所以我们的模型也需要使用很多先验知识来弥补我们数据集不足的问题。卷积神经网络（CNN）就构成了一类这样的模型[16,11,13,18,15,22,26]。它们的容量可以通过改变它们的深度和宽度来控制，并且它们也对图像的性质（即统计量的定态假设以及像素局部依赖性假设）做出准确而且全面的假设。因此，与具有相同大小的层的标准前馈神经网络相比，CNN具有更少的连接和参数，因此它们更容易训练，而其理论最优性能可能稍微弱一些。<br>尽管CNN具有很好的质量，并且尽管其局部结构的效率相对较高，但将它们大规模应用于高分辨率图像时仍然显得非常昂贵。幸运的是，当前的GPU可以用于高度优化的二维卷积，能够加速许多大型CNN的训练，并且最近的数据集（如ImageNet）包含足够多的标记样本来训练此类模型，而不会出现严重的过度拟合。<br>本文的具体贡献如下：我们在ILSVRC-2010和ILSVRC-2012比赛中使用的ImageNet子集上训练了迄今为止最大的卷积神经网络之一[2]，并在这些数据集上取得了迄今为止最好的结果。我们编写了一个高度优化的2D卷积的GPU实现以及其他训练卷积神经网络的固有操作，并将其公开。我们的网络包含许多新的和不同寻常的功能，这些功能可以提高网络的性能并缩短训练时间，详情请参阅第3节。我们的网络规模较大，即使有120万个带标签的训练样本，仍然存在过拟合的问题，所以我们采用了几个有效的技巧来阻止过拟合，在第4节中有详细的描述。我们最终的网络包含五个卷积层和三个全连接层，并且这个深度似乎很重要：我们发现去除任何卷积层（每个卷积层只包含不超过整个模型参数的1%的参数）都会使网络的性能变差。<br>最后，网络的规模主要受限于目前GPU上可用的内存量以及我们可接受的训练时间。我们的网络需要在两块GTX 580 3GB GPU上花费五到六天的时间来训练。我们所有的实验都表明，通过等待更快的GPU和更大的数据集出现，我们的结果可以进一步完善。</p>
<h4 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2 数据集"></a>2 数据集</h4><p>ImageNet是一个拥有超过1500万个已标记高分辨率图像的数据集，大概有22,000个类别。图像都是从网上收集，并使用Amazon-Mechanical Turk群智工具人工标记。从2010年起，作为Pascal视觉对象挑战赛的一部分，这是每年举办一次的名为ImageNet大型视觉识别挑战赛（ILSVRC）的比赛。 ILSVRC使用的是ImageNet的一个子集，每1000个类别中大约有1000个图像。总共有大约120万张训练图像，50,000张验证图像和150,000张测试图像。<br>ILSVRC-2010是ILSVRC中的唯一可以使用测试集标签的版本，因此这也正是我们进行大部分实验的版本。由于我们也在ILSVRC-2012比赛中引入了我们的模型，因此在第6部分中，我们也会给出此版本数据集的结果，尽管这个版本的测试集标签不可用。在ImageNet上，习惯上使用两种错误率：top-1和top-5，其中top-5错误率是正确标签不在被模型认为最可能的五个标签之中的测试图像的百分率。<br>ImageNet由可变分辨率的图像组成，而我们的系统需要固定的输入尺寸。因此，我们将图像下采样到256×256的固定分辨率。给定一个矩形图像，我们首先重新缩放图像，使得短边长度为256，然后从结果中裁剪出中心的256×256的图片。除了将每个像素中减去训练集的像素均值之外，我们没有以任何其他方式对图像进行预处理。所以我们在像素的（中心）原始RGB值上训练了我们的网络。</p>
<h4 id="3-结构"><a href="#3-结构" class="headerlink" title="3 结构"></a>3 结构</h4><p>图2概括了我们所提出网络的结构。它包含八个学习层——五个卷积层和三个全连接层。下面，我们将描述一些所提出网络框架中新颖或不寻常的地方。 3.1-3.4节按照我们对它们重要性的估计进行排序，其中最重要的是第一个。</p>
<h5 id="3-1-ReLU非线性单元"><a href="#3-1-ReLU非线性单元" class="headerlink" title="3.1 ReLU非线性单元"></a>3.1 ReLU非线性单元</h5><p>对一个神经元模型的输出的常规套路是，给他接上一个激活函数：<script type="math/tex">f(x)=tanh(x)</script>或者<script type="math/tex">f(x)=(1+e^{-x})^{-1}</script>。就梯度下降法的训练时间而言，这些饱和非线性函数比非饱和非线性函数如<script type="math/tex">f(x)=max(0,x)</script>慢得多。根据Nair和Hinton的说法[20]，我们将这种非线性单元称为——修正非线性单元（Rectified Linear Units (ReLUs)）。使用ReLUs做为激活函数的卷积神经网络比起使用tanh单元作为激活函数的训练起来快了好几倍。这个结果从图1中可以看出来，该图展示了对于一个特定的四层CNN，CIFAR-10数据集训练中的误差率达到25%所需要的迭代次数。从这张图的结果可以看出，如果我们使用传统的饱和神经元模型来训练CNN，那么我们将无法为这项工作训练如此大型的神经网络。</p>
<p><strong>饱和激活函数会压缩输入值</strong>：如<script type="math/tex">f(x)=max(0,x)</script>，当<script type="math/tex">x</script>趋于正无穷则<script type="math/tex">f(x)</script>也趋于正无穷，所以该函数是非饱和的，<script type="math/tex">sigmoid</script>函数的范围是<script type="math/tex">[0,1]</script>所以是饱和的，<script type="math/tex">tanh</script>函数也是饱和的，因为其取值范围为<script type="math/tex">[-1,1]</script>。</p>
<h2 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h2><p><a target="_blank" rel="noopener" href="https://github.com/DX2048/text_classification">文本分类模型实现</a></p>
<h3 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32965521">参考</a></p>
<h3 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h3><h3 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h3><h3 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h3><h3 id="Hierarchical-Attention-Network"><a href="#Hierarchical-Attention-Network" class="headerlink" title="Hierarchical Attention Network"></a>Hierarchical Attention Network</h3><h3 id="Seq2seq-With-Attention"><a href="#Seq2seq-With-Attention" class="headerlink" title="Seq2seq With Attention"></a>Seq2seq With Attention</h3><h3 id="Dynamic-Memory-Network"><a href="#Dynamic-Memory-Network" class="headerlink" title="Dynamic Memory Network"></a>Dynamic Memory Network</h3><h3 id="EntityNetwork-tracking-state-of-the-world"><a href="#EntityNetwork-tracking-state-of-the-world" class="headerlink" title="EntityNetwork:tracking state of the world"></a>EntityNetwork:tracking state of the world</h3><h3 id="Ensemble-models"><a href="#Ensemble-models" class="headerlink" title="Ensemble models"></a>Ensemble models</h3><h3 id="Transformer-“Attend-Is-All-You-Need”"><a href="#Transformer-“Attend-Is-All-You-Need”" class="headerlink" title="Transformer(“Attend Is All You Need”)"></a>Transformer(“Attend Is All You Need”)</h3><h2 id="如何选择优化算法"><a href="#如何选择优化算法" class="headerlink" title="如何选择优化算法"></a>如何选择优化算法</h2><p>如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。</p>
<p>RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。</p>
<p>Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum。</p>
<p>随着梯度变的稀疏，Adam 比 RMSprop 效果会好。</p>
<p>整体来讲，<strong>Adam 是最好的选择</strong>。</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>[零基础入门深度学习](</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/01/Reinforcement-Learning-with-Deep-Energy-Based-Policies/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/01/Reinforcement-Learning-with-Deep-Energy-Based-Policies/" class="post-title-link" itemprop="url">Reinforcement Learning with Deep Energy-Based Policies</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-06-01 22:16:57 / Modified: 23:22:35" itemprop="dateCreated datePublished" datetime="2021-06-01T22:16:57+08:00">2021-06-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1F5411W7fN/">视频讲解</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/21/Deterministic-Policy-Gradient-Algorithms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/21/Deterministic-Policy-Gradient-Algorithms/" class="post-title-link" itemprop="url">Deterministic Policy Gradient Algorithms</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-21 19:31:21" itemprop="dateCreated datePublished" datetime="2021-05-21T19:31:21+08:00">2021-05-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-05-22 11:36:08" itemprop="dateModified" datetime="2021-05-22T11:36:08+08:00">2021-05-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    在本文中，我们考虑了确定性策略梯度算法，用于连续动作的强化学习。 确定性策略梯度具有特别吸引人的形式：它是动作值函数的期望梯度。 这种简单的形式意味着可以比通常的随机策略梯度更有效地估计确定性策略梯度。 为了确保进行充分的探索，我们引入了一种off-policy的行为者批评算法，该算法从探索行为策略中学习确定性目标策略。 我们证明，确定性策略梯度算法在高维操作空间中可以明显胜过其随机对应算法。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    策略梯度算法广泛应用于连续动作空间的强化学习问题。基本思想是用参数化的概率分布<script type="math/tex">\pi _{\theta }(a|s)=\mathbb{P}\left [ a|s;\theta  \right ]</script>，根据参数向量<script type="math/tex">\theta</script>随机选择状态中的动作a来表示策略。策略梯度算法通常通过对这个随机策略进行采样，并调整策略参数，以获得更大的累积奖励。</p>
<p>​    在本文中，我们将考虑确定性策略<script type="math/tex">a=\mu _{\theta }\left ( s \right )</script>。很自然会想，是否可以与随机策略遵循同样的方法：按照策略梯度的方向调整策略参数。以前认为确定性策略梯度不存在，或者只能在使用模型时获得。然而，我们证明了确定性策略梯度确实存在，而且它有一个简单的model-free形式，简单地遵循action-value函数的梯度。另外我们表明，由于策略方差趋于零，因此确定性策略梯度是随机策略梯度的极限情况。</p>
<p>​    从实际的角度来看，随机策略梯度和确定性策略梯度之间有一个关键的区别。在随机情况下，策略梯度同时在状态空间和动作空间上集成，而在确定性情况下，它只在状态空间上集成。因此，计算随机策略梯度可能需要更多的样本，特别是当动作空间有很多维数时。</p>
<p>​    为了探索完整的状态和动作空间，通常需要一个随机策略。为了确保我们的确定性策略梯度算法继续令人满意地探索，我们引入了一种off-policy学习算法。基本思想是根据随机行为策略选择行动（确保充分的探索)，但了解确定性目标策略(利用确定性策略梯度的效率）。我们使用确定性策略梯度推导出一个off-policy actor-critic算法，该算法使用可微函数近似器估计动作值函数，然后沿近似action-value梯度的方向更新策略参数。我们还引入了一个确定性策略梯度的compatiable function近似的概念，以确保该近似不会对策略梯度造成偏差。</p>
<p>我们将确定性行动者批评算法应用于以下几个基准问题：高维bandit； 低维动作空间的几个标准基准强化学习任务； 和控制章鱼手臂的高维任务。 我们的结果表明，相对于随机策略梯度，使用确定性策略梯度具有明显的性能优势，特别是在高维任务中。 此外，我们的算法比以前的方法不需要更多的计算：每次更新的计算成本在操作维度和策略参数数量上都是线性的。 最后，在许多应用程序中（例如在机器人技术中）提供了可微分的控制策略，但没有将噪声注入控制器的功能。 在这些情况下，随机策略梯度是不适用的，而我们的方法可能仍然有用。</p>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/20/Continuous-Control-With-Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/20/Continuous-Control-With-Deep-Reinforcement-Learning/" class="post-title-link" itemprop="url">Continuous Control With Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-20 22:21:20" itemprop="dateCreated datePublished" datetime="2021-05-20T22:21:20+08:00">2021-05-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-05-21 00:10:32" itemprop="dateModified" datetime="2021-05-21T00:10:32+08:00">2021-05-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CONTINUOUS-CONTROL-WITH-DEEP-REINFORCEMENT-LEARNING"><a href="#CONTINUOUS-CONTROL-WITH-DEEP-REINFORCEMENT-LEARNING" class="headerlink" title="CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING"></a>CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们将Deep Q-Learning成功的背后思想应用到连续的行动领域。我们提出了一个actor-critic,model-free基于确定性策略梯度的算法，可以在连续动作空间上运行。使用相同的学习算法、网络架构和超参数，我们的算法稳健地解决了20多个模拟物理任务，包括卡杆摆动、灵巧操作、腿运动和汽车驾驶等经典问题。我们的算法能够找到性能与完全访问动态系统的规划算法及其衍生物相当的策略。我们进一步证明，对于许多任务，该算法可以直接通过输入原始像素学习“end-to-end”策略。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    人工智能领域的主要目标之一是从未处理的、高维的、感官的输入中解决复杂的任务。最近，通过将深度学习中的感官处理与强化学习相结合，取得了重大进展，从而形成了DQN算法 。该算法能够在许多Atari电子游戏中，使用未经处理的像素作为输入，达到了人类水平的性能。 为此，使用了深度神经网络函数逼近器来估算动作值函数。</p>
<p>​    但是，尽管DQN解决了高维观测空间的问题，但它只能处理离散和低维动作空间。 许多有趣的任务，尤其是物理控制任务，具有连续的（实际值）和高维度的动作空间。 DQN不能直接应用于连续域，因为它依赖于找到可以最大程度地提高动作值函数，在连续值情况下，每个步骤都需要迭代优化过程。</p>
<p>​    使诸如DQN之类的深度强化学习方法适应连续领域的一种明显方法是简单地离散动作空间。但是，这有很多局限性，最显着的是维度灾难：动作的数量随自由度的增加而呈指数增长。例如，对于每个关节具有最粗糙的离散<script type="math/tex">a_{i}\in\left \{ -k,0,k \right \}</script>的,关节数量为7的系统（如在人类手臂中）会导致具有以下维度的动作空间：<script type="math/tex">3^7=2187</script> 。对于需要对动作进行精细控制的任务，情况甚至更糟，因为它们需要相应地更精细的离散化，从而导致离散动作数量激增。 如此大的动作空间很难有效地探索，因此在这种情况下成功地训练类似DQN的网络可能很棘手。 此外，动作空间的单纯离散化会不必要地丢弃有关动作域结构的信息，这对于解决许多问题可能是必不可少的。</p>
<p>​    在这项工作中，我们提出了一种使用深度函数逼近器的model-free，off-policy的actor-critic算法，该算法可以学习高维，连续动作空间中的策略。我们的工作基于确定性策略梯度（DPG）算法（其自身类似于NFQCA，并且可以找到类似的想法）。 然而，正如我们在下面显示的那样，这种针对行为者的方法与神经函数近似器的单纯应用对于具有挑战性的问题是不稳定的。</p>
<p>​    在这里，我们结合了行动者批评方法和最近从Deep Q Network（DQN）成功获得的见解。 在DQN之前，通常认为使用大型非线性函数逼近器学习价值函数既困难又不稳定。 由于以下两项创新，DQN能够使用此类函数逼近器以稳定且健壮的方式学习价值函数：1.通过从重播缓冲区中抽取样本，对网络进行off-policy训练，以最大程度地减少样本之间的相关性； 2.用目标Q网络训练网络，以在时间差备份期间给出一致的目标。 在这项工作中，我们利用了相同的思想以及批处理规范化，这是深度学习的最新进展。</p>
<p>​    为了评估我们的方法，我们构造了各种具有挑战性的物理控制问题涉及复杂的多关节运动，不稳定和丰富的接触动力学以及步态行为。其中包括经典的问题，例如cartpole swing-up问题以及许多新领域。 机器人控制的长期挑战是直接从原始的感官输入（例如视频）中学习动作策略。 因此，我们将固定的视点相机放置在模拟器中，并尝试使用低维观测（例如，关节角度）以及直接从像素中进行所有任务。</p>
<p>​    我们的无模型方法，我们称为深度DPG(DDPG)，可以使用低维观测(笛卡尔坐标或关节角)来学习我们所有任务的竞争策略。使用相同的超参数和网络结构。在许多情况下，我们还能够直接从像素中学习好的策略，再次保持超参数和网络结构不变。</p>
<p>​    该方法的一个关键特征是它的简单性：它只需要简单的actor-critic体系结构和学习算法，而很少有“运动部件”，因此易于实现和扩展更困难的问题和更大的网络。 对于物理控制问题，我们将我们的结果与规划器计算的基线进行比较，该规划器可以完全访问基础模拟动力学及其派生函数（请参阅补充信息）。 有趣的是，在某些情况下，即使从像素中学习时，DDPG有时也会找到超出规划器性能的策略（规划器始终在底层的低维状态空间上进行规划）。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>​    我们考虑一个标准的强化学习设置，该设置由一个在离散的时间步中与一个环境<script type="math/tex">E</script>交互的代理组成。 代理在每个时间步长<script type="math/tex">t</script>收到观察值<script type="math/tex">x_{t}</script>，采取行动，并收到标量奖励<script type="math/tex">r_{t}</script>。 在这里考虑的所有环境中，这些动作的实值均为<script type="math/tex">a_{t}\in \mathbb{R}^{N}</script>。 通常，可能会部分观察环境，因此可能需要观察的整个历史记录，动作对<script type="math/tex">s_{t}=(x_{1},a_{1},...,a_{t-1},x_{t})</script>来描述状态。 在这里，我们假设环境是完全观察到的，因此<script type="math/tex">s_{t}=x_{t}</script>。</p>
<p>​    代理的行为由策略<script type="math/tex">\pi</script>定义，该策略将状态映射到行为<script type="math/tex">\pi</script>的概率分布：<script type="math/tex">S\rightarrow P(A)</script>。环境<script type="math/tex">E</script>也可能是随机的。 我们将其建模为马尔可夫模型状态空间为<script type="math/tex">S</script>的决策过程，动作空间<script type="math/tex">A=\mathbb{R}^{N}</script>，初始状态分布为<script type="math/tex">p(s_{1})</script>，转移概率<script type="math/tex">p(s_{t+1}|s_{t},a_{t})</script>和奖励函数<script type="math/tex">r(s_{t},a_{t})</script>。</p>
<p>​    从状态的收益被定义为具有折扣因子<script type="math/tex">\gamma \in [0,1]</script>的折扣未来奖励的总和<script type="math/tex">R_{t}=\sum_{i=t}^{T}\gamma ^{(i-t)}r(s_{i},a_{i})</script>。 请注意，返回值取决于选择的操作，因此在策略<script type="math/tex">\pi</script>上，并且可能是随机的。 强化学习的目标是学习一种策略，该策略将从初始分布最大化的预期回报。 我们表示策略<script type="math/tex">\pi</script>的折扣状态访问分布为<script type="math/tex">\rho ^{\pi }</script>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/20/Soft-Actor-Critic/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/20/Soft-Actor-Critic/" class="post-title-link" itemprop="url">Soft Actor-Critic</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-20 21:14:17" itemprop="dateCreated datePublished" datetime="2021-05-20T21:14:17+08:00">2021-05-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 23:17:10" itemprop="dateModified" datetime="2021-06-01T23:17:10+08:00">2021-06-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/17/graph-network-study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/17/graph-network-study/" class="post-title-link" itemprop="url">graph_network_study</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-05-17 22:50:56 / Modified: 22:51:30" itemprop="dateCreated datePublished" datetime="2021-05-17T22:50:56+08:00">2021-05-17</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/12/docker-study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/12/docker-study/" class="post-title-link" itemprop="url">docker study</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-12 21:00:59" itemprop="dateCreated datePublished" datetime="2021-05-12T21:00:59+08:00">2021-05-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-01 23:28:12" itemprop="dateModified" datetime="2021-06-01T23:28:12+08:00">2021-06-01</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/koktlzz/p/14105026.html#%E4%BB%80%E4%B9%88%E6%98%AF%E5%AE%B9%E5%99%A8%E6%95%B0%E6%8D%AE%E5%8D%B7%EF%BC%9F">狂神docker</a></p>
<p>docker run -it -v /home/pql/company_code/tmp:/root/repos/tmp hub.digi-sky.com/aid/aicloud:dante-1.8.0-cuda11.1-cudnn8-runtime /bin/bash</p>
<p>使用-v挂载文件夹时，假如docker容器里已存在挂载的文件夹，并且宿主主机也存在挂载文件夹，宿主主机的文件夹内容会覆盖docker容器里的内容。如果宿主主机不存在挂载文件夹，会新建文件夹，并让docker容器里的文件夹同步（删除之前的内容）。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/" class="post-title-link" itemprop="url">IMPALA:Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-07 20:10:31" itemprop="dateCreated datePublished" datetime="2021-05-07T20:10:31+08:00">2021-05-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/06/Asynchronous-Methods-for-Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/06/Asynchronous-Methods-for-Deep-Reinforcement-Learning/" class="post-title-link" itemprop="url">Asynchronous Methods for Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-05-06 20:26:41 / Modified: 21:04:37" itemprop="dateCreated datePublished" datetime="2021-05-06T20:26:41+08:00">2021-05-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Asynchronous-Methods-for-Deep-Reinforcement-Learning"><a href="#Asynchronous-Methods-for-Deep-Reinforcement-Learning" class="headerlink" title="Asynchronous Methods for Deep Reinforcement Learning"></a>Asynchronous Methods for Deep Reinforcement Learning</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​    我们提出了一个概念上简单和轻量级的深度强化学习框架，它使用异步梯度下降来优化深度神经网络控制器。我们提出了四种标准强化学习算法的异步变体，并表明并行actor-learner对训练有稳定的效果，使所有四种方法都能够成功地训练神经网络控制器。性能最好的方法是actor-critic的异步变体，它在一个多核CPU而不是GPU上训练一半的时间，超过了Atari领域上最先进的方法。此外，我们还证明了异步actor-critic在各种连续电机控制问题以及使用视觉输入导航随机三维迷宫的新任务上取得了成功。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    深度神经网络提供了丰富的表示，可以使强化学习(RL)算法能够有效地执行。然而，以前人们认为，简单的online RL算法与深度神经网络的结合从根本上是不稳定的。相反，已经提出了各种解决方案来稳定该算法（(Riedmiller, 2005; Mnih et al., 2013; 2015; Van Hasselt et al., 2015; Schulman et al., 2015a)。这些方法有一个共同的想法：online RL代理遇到的观察数据序列是非平稳的，与online RL更新有很强的相关性。通过将代理的数据存储在经验回放内存中，数据可以从不同的时间步长进行分组 (Riedmiller, 2005; Schulman et al., 2015a) 或随机采样 (Mnih et al., 2013; 2015; Van Hasselt et al., 2015) 。以这种方式聚合内存可以减少非平稳性和去相关的更新，但同时将这些方法限制为off-policy强化学习算法。</p>
<p>​    基于经验重放的深度RL算法在Atari2600等具有挑战性的领域中取得了前所未有的成功。然而，经验回放有几个缺点：它每次实际交互使用更多的内存和计算；它需要off-policy的学习算法，可以从旧策略生成的数据进行更新。</p>
<p>​    在本文中，我们提供了一个非常不同的深度强化学习范式。我们在环境的多个实例上异步地并行执行多个代理，而不是经验重放。这种并行性还将代理的数据重新关联到一个更平稳的过程中，因为在任何给定的时间步长中，并行代理都将经历各种不同的状态。这个简单的想法使基本的策略RL算法，比如Sarsa,n-step方法,actor-critic方法以及off-policy的RL算法，如Q-learning,将使用深度神经网络稳健和有效地应用。</p>
<p>​    我们的并行强化学习范式也提供了实际的好处。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/06/Grandmaster-level-in-StarCraftII-using-multi-agent-reinforcement-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/06/Grandmaster-level-in-StarCraftII-using-multi-agent-reinforcement-learning/" class="post-title-link" itemprop="url">Grandmaster level in StarCraftII using multi-agent reinforcement learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-06 20:16:47" itemprop="dateCreated datePublished" datetime="2021-05-06T20:16:47+08:00">2021-05-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-04 00:05:14" itemprop="dateModified" datetime="2021-06-04T00:05:14+08:00">2021-06-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    许多现实世界的应用程序都需要人工代理在复杂的环境中与其他代理进行竞争和协调。作为实现这一目标的垫脚石，星际争霸领域已成为人工智能的重要挑战研究，因为它在最困难的职业电子竞技中具有标志性和持久的地位，并且在其原始复杂性和多代理挑战方面与现实世界相关。在过去的十年和无数的比赛中，最强大的代理简化了游戏的重要方面，利用了超人的能力，或者采用了手工制作的子系统。尽管有这些优势，但没有一个之前的智能体能接近顶级星际争霸玩家的全面的水平。我们选择使用原则上适用于其他复杂领域的通用学习方法来应对星际争霸的挑战：一种多智能体强化学习算法，它使用来自人类和智能体游戏的数据，并在不断调整策略和反策略的多样化联盟中 ，每个都由深度神经网络表示 。我们通过一系列和人类玩家在线对战，评估了我们在StarCraft II 的完整游戏中的智能体AlphaStar。AlphaStar在所有三场星际争霸比赛中都被评为大师级，人类玩家的排名在99.8%以上。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>《星际争霸》是一款实时的战略游戏，玩家可以平衡高水平的经济决策和个人对数百个单元的控制。这个领域提出了重要的博弈论挑战：它具有循环、非传递策略和反策略的广阔空间； 单纯的自我对弈探索方法难以发现新策略； 这些策略在当与现实世界的玩家对战时可能不会有效。 此外，星际争霸有一个组合行动空间，一个扩展了数千个实时决策的规划范围和不完善的信息。</p>
<p>​    每局游戏由成千上万的时间步和数千计的动作组成，在大约十分钟的游戏中实时选择。在每一步<script type="math/tex">t</script>，我们的代理AlphaStar接收一个观察<script type="math/tex">o_{t}</script>，其中包括所有可观测单位及其属性的列表。这个信息是不完美的；游戏只包括玩家自己的单位所看到的对手单位，并排除了相机视图之外的一些对手单位属性。</p>
<p>​    每个动作都是高度结构化的：它从几百个动作类型中选择（例如，move或build worker）； 从代理单元的任何子集中向谁发出该操作； 在地图上的位置或摄像机视图内的单位中定位到哪里； 以及何时观察和下一步行动（Fig.1a）。 动作的这种表示导致每一步大约有 <script type="math/tex">10^{26}</script> 种可能的选择。 与人类玩家类似，可以使用特殊动作来移动相机视图，以收集更多信息。</p>
<p>​    人类在物理限制下玩星际争霸，限制了他们的反应时间和行动速度。游戏的设计中考虑到了这些限制，而删除这些限制则改变了游戏的性质。因此，我们选择对AlphaStar施加约束：它由于网络延迟和计算时间而遭受延迟；它的每分钟动作actions per minute (APM)是有限的，峰值统计数字大大低于人类(图2c,3g用于性能分析）。AlphaStar 在这个接口上的玩法和这些限制得到了专业玩家的认可（see ‘Professional  player statement’ in Methods）。</p>
<p><img src="/2021/05/06/Grandmaster-level-in-StarCraftII-using-multi-agent-reinforcement-learning/fig1_a.png" alt></p>
<p>图1a</p>
<p>AlphaStar通过概述地图和单位列表来观察游戏。要执行操作，代理将输出要发布的操作类型（例如build）、应用对象、目标位置以及何时发布下一个操作。动作会通过限制动作速率的监控层发送到游戏中。AlphaStar主张使用来自网络延迟和处理时间的延迟。</p>
<p><img src="/2021/05/06/Grandmaster-level-in-StarCraftII-using-multi-agent-reinforcement-learning/fig1_b_c.png" alt></p>
<p>图1b,c</p>
<p><strong>b.</strong>AlphaStar是通过监督学习和强化学习来训练的。在监督学习（底部）中，参数被更新以优化 Kullback–Leibler(KL)其输出和从集合回放中采样的人类行为之间的差异。在强化学习（顶部）中，使用人类数据来采样统计<script type="math/tex">z</script>，并通过强化学习<script type="math/tex">\left ( TD\left ( \lambda  \right ),V-trace,UPGO \right )</script>结合对监督代理的KL损失来更新策略和价值输出。<strong>c.</strong>三个代理池，每个池由监督学习初始化，随后通过强化学习进行训练。在训练时，这些智能体断断续续地向联盟添加自己的副本——在特定时刻被冻结的“球员”。main agent通过训练来对抗所有这些过去的player，以及他们自己。legue exploiters进行训练以对抗所有过去的球员。Main exploiters被训练起来对付main agent。Main exploiters 和 league exploiters在加入player时可以重置为supervised agent。经星际娱乐公司的许可，星际争霸的图片被复制。</p>
<h1 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h1><p>​    为了解决《星际争霸》的复杂性和博弈论挑战，AlphaStar结合了新的和现有的通用技术，比如神经网络架构、模仿学习、强化学习和多智能体学习。有关这些技术的进一步详细信息详见Methods。</p>
<p>​    AlphaStar的核心是一个策略<script type="math/tex">\pi _{\theta }\left ( a_{t}|s_{t},z \right )=\mathbb{P}[a_{t}|s_{t},z]</script>，由一个神经网络与参数<script type="math/tex">\theta</script>表示，从游戏开始接收所有观察<script type="math/tex">s_{t}=(o_{1:t},a_{1:t-1})</script>作为输入，并选择动作作为输出。该策略还基于一个统计量<script type="math/tex">z</script>，它总结了从人类数据中采样的策略（例如build order）。</p>
<p>​    我们的代理架构由处理星际争霸的原始复杂性的通用神经网络组件组成。使用自注意力机制处理玩家和对手单位的观察。为了整合空间信息和非空间信息，我们引入了散点连接。为了处理部分可观测性，观测的时间序列由一个深度长短期记忆(LSTM)系统来处理。要管理结构化的组合动作空间，代理将使用自动回归策略（参加7,10,11）和循环使用的指针网络(参见12)。扩展数据图3总结了架构和图。Fig.3f显示每个组件的消融。</p>
<p>​    代理参数最初是由监督学习进行训练的。游戏是从一个公开的匿名人类回放数据集中采样的。然后该策略被训练来预测每个行动<script type="math/tex">a_{t}</script>，要么基于<script type="math/tex">s_{t}</script>，也基于<script type="math/tex">z</script>。这导致了一套反映人类游戏模式的不同策略。</p>
<p>​    代理参数随后被一种强化学习算法训练，该算法旨在最大限度地提高对混合对手的胜率（即计算最佳反应）。对手的选择由多代理程序决定，如下所述。AlphaStar的强化学习算法是基于一种类似于advantage actor-critic(参见13)的策略梯度算法。算法使用经验重放(参见15)进行异步（参见14）更新。这需要一种称为off-policy学习(参见5)的方法，即根据以前策略生成的经验更新当前策略。我们的解决方案的动机是观察到，在较大的动作空间中，当前和以前的策略不太可能在许多步骤中相匹配。因此，我们使用了在不匹配的情况下可以有效学习的技术：时间差分学习(<script type="math/tex">TD\left ( \lambda  \right )</script>（参见16）、剪切重要性采样(V-trace)（参见14）和一种新的自模仿算法(UPGO)（参见17），该算法将策略移向优于平均奖励的轨迹移动。为了减少方差，仅在训练过程中，使用玩家和对手角度的信息来估计值函数。图3i,k分析了这些组件的相对重要性。</p>
<p>​    星际争霸的主要挑战之一是发现新的策略。考虑一个已经学习了建立和利用地面部队微观战术的策略。任何建造和单纯地使用空中单元的偏差都会降低性能。简单的探索极不可能执行一个精确的指令序列，超过数千步，构建空中单位，并有效地利用他们的微观战术。为了解决这个问题，并鼓励像人类游戏的稳健行为，我们利用了人类数据。每个代理都初始化为监督学习代理的参数。 随后，在强化学习期间，我们要么以统计量 <script type="math/tex">z</script>为条件来调节智能体，在这种情况下，智能体会因遵循与<script type="math/tex">z</script>对应的策略而获得奖励，要么无条件地训练智能体，在这种情况下，智能体可以自由选择自己的策略。 每当代理的动作概率与监督策略不同时，他们也会受到惩罚。 这种人类探索确保在整个培训过程中继续探索各种相关的游戏模式。 图 3e 显示了人类数据在 AlphaStar 中的重要性。</p>
<h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><ol>
<li><p>Sutton, R. &amp; Barto, A. <em>Reinforcement Learning: An Introduction</em> (MIT Press, 1998).</p>
</li>
<li><p>Vinyals, O. et al. StarCraft II: a new challenge for reinforcement learning. Preprint at  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.04782">https://arxiv.org/abs/1708.04782</a> (2017).</p>
</li>
<li><p>Mikolov, T., Karafiat, M., Burget, L., Cernocky, J. &amp; Khudanpur, S. Recurrent neural network  </p>
<p>based language model. <em>INTERSPEECH-2010</em> 1045–1048 (2010)</p>
</li>
<li><p>Metz, L., Ibarz, J., Jaitly, N. &amp; Davidson, J. Discrete sequential prediction of continuous  </p>
<p>actions for deep RL. Preprint at <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.05035v3">https://arxiv.org/abs/1705.05035v3</a> (2017).</p>
</li>
<li><p>Vinyals, O., Fortunato, M. &amp; Jaitly, N. Pointer networks. <em>Adv. Neural Information Process.</em>  </p>
<p><em>Syst</em>. <strong>28</strong>, 2692–2700 (2015).</p>
</li>
<li><p>Mnih, V. et al. Asynchronous methods for deep reinforcement learning. <em>Proc. Machine</em>  </p>
<p><em>Learning Res</em>. <strong>48</strong>, 1928–1937 (2016).</p>
</li>
<li><p>Espeholt, L. et al. IMPALA: scalable distributed deep-RL with importance  </p>
<p>weighted actor-learner architectures. <em>Proc. Machine Learning Res</em>. <strong>80</strong>, 1407–1416  </p>
<p>(2018).</p>
</li>
<li><p>Wang, Z. et al. Sample efficient actor-critic with experience replay. Preprint at https:// </p>
<p>arxiv.org/abs/1611.01224v2 (2017).</p>
</li>
<li><p>Sutton, R. Learning to predict by the method of temporal differences. <em>Mach. Learn</em>. <strong>3</strong>,  </p>
<p>9–44 (1988).</p>
</li>
<li><p>Oh, J., Guo, Y., Singh, S. &amp; Lee, H. Self-Imitation Learning. <em>Proc. Machine Learning Res</em>. <strong>80</strong>,  </p>
<p>3875–3884 (2018).</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">QilongPan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QilongPan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
