<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>潘其龙</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="潘其龙">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="潘其龙">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="QilongPan">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="潘其龙" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">潘其龙</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-docker-study" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/12/docker-study/" class="article-date">
  <time class="dt-published" datetime="2021-05-12T13:00:59.000Z" itemprop="datePublished">2021-05-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/05/12/docker-study/">docker study</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/koktlzz/p/14105026.html#%E4%BB%80%E4%B9%88%E6%98%AF%E5%AE%B9%E5%99%A8%E6%95%B0%E6%8D%AE%E5%8D%B7%EF%BC%9F">狂神docker</a></p>
<p>docker run -it -v /home/pql/company_code/tmp:/root/repos/tmp hub.digi-sky.com/aid/aicloud:dante-1.8.0-cuda11.1-cudnn8-runtime /bin/bash</p>
<p>使用-v挂载文件夹时，假如docker容器里已存在挂载的文件夹，并且宿主主机也存在挂载文件夹，宿主主机的文件夹内容会覆盖docker容器里的内容。如果宿主主机不存在挂载文件夹，会新建文件夹，并让docker容器里的文件夹同步（删除之前的内容）。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/05/12/docker-study/" data-id="ckoln5scw0009g4va1zrvghw2" data-title="docker study" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/" class="article-date">
  <time class="dt-published" datetime="2021-05-07T12:10:31.000Z" itemprop="datePublished">2021-05-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/">IMPALA:Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/" data-id="ckoln5scp0003g4va7kwoehkb" data-title="IMPALA:Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Asynchronous-Methods-for-Deep-Reinforcement-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/06/Asynchronous-Methods-for-Deep-Reinforcement-Learning/" class="article-date">
  <time class="dt-published" datetime="2021-05-06T12:26:41.000Z" itemprop="datePublished">2021-05-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/05/06/Asynchronous-Methods-for-Deep-Reinforcement-Learning/">Asynchronous Methods for Deep Reinforcement Learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Asynchronous-Methods-for-Deep-Reinforcement-Learning"><a href="#Asynchronous-Methods-for-Deep-Reinforcement-Learning" class="headerlink" title="Asynchronous Methods for Deep Reinforcement Learning"></a>Asynchronous Methods for Deep Reinforcement Learning</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​    我们提出了一个概念上简单和轻量级的深度强化学习框架，它使用异步梯度下降来优化深度神经网络控制器。我们提出了四种标准强化学习算法的异步变体，并表明并行actor-learner对训练有稳定的效果，使所有四种方法都能够成功地训练神经网络控制器。性能最好的方法是actor-critic的异步变体，它在一个多核CPU而不是GPU上训练一半的时间，超过了Atari领域上最先进的方法。此外，我们还证明了异步actor-critic在各种连续电机控制问题以及使用视觉输入导航随机三维迷宫的新任务上取得了成功。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    深度神经网络提供了丰富的表示，可以使强化学习(RL)算法能够有效地执行。然而，以前人们认为，简单的online RL算法与深度神经网络的结合从根本上是不稳定的。相反，已经提出了各种解决方案来稳定该算法（(Riedmiller, 2005; Mnih et al., 2013; 2015; Van Hasselt et al., 2015; Schulman et al., 2015a)。这些方法有一个共同的想法：online RL代理遇到的观察数据序列是非平稳的，与online RL更新有很强的相关性。通过将代理的数据存储在经验回放内存中，数据可以从不同的时间步长进行分组 (Riedmiller, 2005; Schulman et al., 2015a) 或随机采样 (Mnih et al., 2013; 2015; Van Hasselt et al., 2015) 。以这种方式聚合内存可以减少非平稳性和去相关的更新，但同时将这些方法限制为off-policy强化学习算法。</p>
<p>​    基于经验重放的深度RL算法在Atari2600等具有挑战性的领域中取得了前所未有的成功。然而，经验回放有几个缺点：它每次实际交互使用更多的内存和计算；它需要off-policy的学习算法，可以从旧策略生成的数据进行更新。</p>
<p>​    在本文中，我们提供了一个非常不同的深度强化学习范式。我们在环境的多个实例上异步地并行执行多个代理，而不是经验重放。这种并行性还将代理的数据重新关联到一个更平稳的过程中，因为在任何给定的时间步长中，并行代理都将经历各种不同的状态。这个简单的想法使基本的策略RL算法，比如Sarsa,n-step方法,actor-critic方法以及off-policy的RL算法，如Q-learning,将使用深度神经网络稳健和有效地应用。</p>
<p>​    我们的并行强化学习范式也提供了实际的好处。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/05/06/Asynchronous-Methods-for-Deep-Reinforcement-Learning/" data-id="ckoln5sb30000g4va8nlb8vd4" data-title="Asynchronous Methods for Deep Reinforcement Learning" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Grandmaster-level-in-StarCraftII-using-multi-agent-reinforcement-learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/06/Grandmaster-level-in-StarCraftII-using-multi-agent-reinforcement-learning/" class="article-date">
  <time class="dt-published" datetime="2021-05-06T12:16:47.000Z" itemprop="datePublished">2021-05-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/05/06/Grandmaster-level-in-StarCraftII-using-multi-agent-reinforcement-learning/">Grandmaster level in StarCraftII using multi-agent reinforcement learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/05/06/Grandmaster-level-in-StarCraftII-using-multi-agent-reinforcement-learning/" data-id="ckoln5sco0002g4vagk30dx2v" data-title="Grandmaster level in StarCraftII using multi-agent reinforcement learning" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/26/Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning/" class="article-date">
  <time class="dt-published" datetime="2021-04-26T13:10:23.000Z" itemprop="datePublished">2021-04-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/26/Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning/">Genetic State-Grouping Algorithm for Deep Reinforcement Learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning"><a href="#Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning" class="headerlink" title="Genetic State-Grouping Algorithm for Deep Reinforcement Learning"></a>Genetic State-Grouping Algorithm for Deep Reinforcement Learning</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​    虽然强化学习已经被认为是机器学习中最重要和最著名的技术之一，但由于其较长的初始学习时间和学习不稳定，它在现实问题中的适用性仍然有限。特别是在实时约束下，大量分支因子的问题仍然无法克服，这需要一种针对下一代强化学习的新方法。本文提出了基于深度强化学习的Genetic State-Grouping Algorithm。其核心思想是将整个状态集划分为几个状态组。每一组都由相互相似的状态组成，从而代表了它们的共同特征。然后使用遗传优化器来处理状态组，从而发现杰出的操作。这些步骤有助于深度Q网络避免过度的探索，从而导致不能显著减少初始学习时间。对实时战斗电子游戏(FightingICE)的实验证明了该方法的有效性。</p>
<p>关键词：强化学习，遗传算法，混合方法，蒙特卡罗技术</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    强化学习方法(RL)击败了世界上最好的人类围棋玩家之一，引起了专家们的认真关注。它的基于试验和错误的迭代而不是大量的预定义数据集进行培训的性质在全世界引起了热烈的讨论和兴趣。然而，除了强化学习的所有成就外，它还暴露了许多技术困难，如学习不稳定和进入局部最优。具体地说，很难定义一个找到复杂问题中的全局解的奖励函数，它比其他类型的机器学习需要更多的计算资源和初始学习时间。之前的研究提出了各种解决这些缺点的解决方案，有些是利用实时视频游戏环境与现实世界相似，表明它们适合RL对内存和时间的苛刻需求。然而，这些研究的主要贡献仅限于只有几个分支因素的问题，因为提高它们的数量会呈指数级地增加所需的计算资源数量。</p>
<p>​    本文介绍了通过深度强化学习增强的Genetic State-Grouping Algorithm (GSGA)。它是遗传算法和State Grouping的集成，我们真正的方法，以追求提高深度强化学习在时间和资源效率方面的学习效率。我们展示了它在FightingICE中的表现，这是一个实时战斗视频游戏，由IEEE CoG完成。我们认为它是一个适当的试验场来验证我们提出的结构的有效性，因为它包含了各种类似于现实世界的环境因素。</p>
<p>​    FightingICE由 Intelligent Computer Entertainment Lab in Ritsumeikan University, Japan开发。因为它是一个为构建通用战斗游戏ai而设计的环境，所以在属性和风格方面，它是一种相当典型的类型。在每一轮中，两个玩家代理在二维空间中相互战斗，他们之间实例化一个物理距离。游戏向每个代理提供多达56个可执行操作。此外，根据交战规则，每个玩家被允许一次只使用一个线程，以复制真实世界的问题条件，这需要大量使用计算资源。总的来说,FightingICE玩家只利用单个线程在16.67ms的每个时间帧内必须考虑多达3136个动作状态。游戏中的物理空间由$$960\times 640$$像素组成，分为5像素大小的房间，每个代理都可以放置在里面。因此，每个代理总共有$$192\times128$$的状态空间来重新定位。</p>
<p>​    本文的排列格式如下：第二章我们介绍了一些与本文主要思想有关的研究。在第三章中，我们介绍了我们的技术的核心理论，以及关于电子游戏环境的细节。第4章介绍了我们的模型及其竞争对手的过程和实验结果，第五章提供了我们对模型整体性能的评论和关注。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>​    在强化学习过程中，人们多次尝试减少长初始学习时间，稳定整体学习的过程。Justesen等人提出一种强化学习代理，有效解决实时问题的时间划分和对抗环境，Langford提出一种有效的探索方法。一些研究选择通过增强学习稳定性来降低结构复杂性，尽管他们的方法对涉及多个分支因素的问题缺乏足够的适用性，但在这些问题中，环境高度复杂，或者为代理提供了大量的操作选择。此外，(Justesen等人，2016)提出了一个解决在线进化的巨大分支因素的解决方案，尽管它几乎不适用于实时视频游戏。在具有多变量的实时问题中，大量的分支因素特别吓人，其中分支的数量增长得非常快。尽管RL具有探索和经验学习的性质，适合于开发实时自适应AI，但在现实世界中常见的多分支因子问题中，RL的学习效率会大大降低。</p>
<p>​    其他几项研究采用蒙特卡罗树搜索（MCTS），结合进化算法，作为强化学习的补充在开发视频游戏AI时，证明了其在足够时间内找到最佳解决方案的能力。尽管如此，在大多数方面强化学习需要大量计算资源和时间的本质仍然是不可妥协的。</p>
<h2 id="提出的方法"><a href="#提出的方法" class="headerlink" title="提出的方法"></a>提出的方法</h2><p>​    强化学习是一种机器学习技术，它在代理与未知环境交互时为他们找到最佳的学习策略。这种过程通常被形式化为马尔可夫决策过程(MDPs)，它可以由4个元素$$(S,A,P,R)$$来定义。在每个时间戳$$t$$，agent观察一组状态$$s_{t}\in S$$，并采取一组操作$$a_{t}\in A$$。与转换函数$$P$$相关联，所有状态都为马尔可夫的环境，决定奖励$$r_{t}\sim R(s_{t},a_{t})$$和后续或以下状态$$s_{t+1}\sim P(s_{t},a_{t})$$。我们将根据一阶马尔可夫假设来解释所提出的方法。</p>
<p>​    图1从概念上表示了我们提议的GSGA的总体结构。它由三个主体组成：1）state grouping，2） genetic optimizer，和3) deep Q network。其详细的程序描述如下。</p>
<p><img src="/2021/04/26/Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning/figure1.png"></p>
<p>​        State Grouping(SG)是一种技术，通过将相似的状态结合在一起，压缩状态线的大小，包括物理距离。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/04/26/Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning/" data-id="ckoln5scm0001g4vah9ivgmmi" data-title="Genetic State-Grouping Algorithm for Deep Reinforcement Learning" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-TLeague" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/03/17/TLeague/" class="article-date">
  <time class="dt-published" datetime="2021-03-17T13:12:44.000Z" itemprop="datePublished">2021-03-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/03/17/TLeague/">TLeague</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Tleague-A-Framework-for-Competitive-Self-Play-based-Distributed-Multi-Agent-Reinforcement-Learning"><a href="#Tleague-A-Framework-for-Competitive-Self-Play-based-Distributed-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Tleague:A Framework for Competitive Self-Play based Distributed Multi-Agent Reinforcement Learning"></a>Tleague:A Framework for Competitive Self-Play based Distributed Multi-Agent Reinforcement Learning</h1><p>多智能体强化学习需要大量的数据，为了解决这个问题实现了TLeague框架。TLeague的目标是实现大规模训练以及主流的Competitive Self-Play (CSP)算法。训练可以在单机和多机。在 StarCraft II, ViZDoom and Pommerman中展示了它的效率和有效性。<a target="_blank" rel="noopener" href="https://github.com/tencent-ailab/tleague_projpage">code</a></p>
<p>该框架用于多智能体竞争self-play的学习。框架如下图所示</p>
<p><img src="/2021/03/17/TLeague/framework.png"></p>
<ul>
<li>Learner</li>
</ul>
<p>每个Learner从与之对应的Actor接收trajectories进行学习。每个Learner对应一个ReplayMem和DataServer，进行数据预处理和存储在ReplayMem中。学习开始时会从LeagueMgr接收到task，可以从中知道需要学习的策略。在学习过程中也会更新策略到ModelPool中。</p>
<ul>
<li>InfServer</li>
</ul>
<p>从不同的Actor中收集一批observation进行预测，并返回给对应的Actor；</p>
<ul>
<li>ModelPool</li>
</ul>
<p>存储策略参数作为对手模型池。Actor和Learner会读取和写入策略参数。</p>
<ul>
<li>LeagueMgr</li>
</ul>
<p>用于协调其它模块。GameMgr有不同的对手选取算法。HyperMgr进行超参数的管理。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/03/17/TLeague/" data-id="ckoln5scv0008g4va7ugc5w9d" data-title="TLeague" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Sample-Factory-Egocentric-3D-Control-from-Pixels-at-100000-FPS-with-Asynchronous-Reinforcement-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/01/20/Sample-Factory-Egocentric-3D-Control-from-Pixels-at-100000-FPS-with-Asynchronous-Reinforcement-Learning/" class="article-date">
  <time class="dt-published" datetime="2021-01-20T12:25:13.000Z" itemprop="datePublished">2021-01-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/01/20/Sample-Factory-Egocentric-3D-Control-from-Pixels-at-100000-FPS-with-Asynchronous-Reinforcement-Learning/">Sample Factory:Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.11751">paper</a></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    强化学习实验的规模不断扩大，使得研究人员在训练复杂的电子游戏代理和在机器人的模拟到现实转移方面都取得了前所未有的成果。 通常，这种实验依赖于大型分布式系统，需要昂贵的硬件设置，限制了对这一令人兴奋的研究领域的更广泛访问。 在本工作中，我们旨在通过优化强化学习算法的效率和资源利用来解决这个问题，而不是依赖分布式计算。 我们提出了“Sample Factory”，这是一个为单机情况下优化的高吞吐量训练系统。 我们的体系结构结合了一个高效的、异步的、基于GPU的采样器和off-policy校正技术，使我们能够在3D中实现高于$$10^{5}$$个环境帧/秒的吞吐量，而个环境帧/秒的吞吐量。 我们扩展了Sample Factory，以支持self-play和 population-based的训练，并应用这些技术来为多人第一人称射击游戏训练高能力的代理。</p>
<p>[Github](<a target="_blank" rel="noopener" href="https://github.com/">https://github.com/</a> alex-petrenko/sample-factory )</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    模拟环境中的训练代理是当代强化学习研究的基石。 近年来，通过应用强化学习方法在这些快速和高效的环境中训练代理，无论是解决复杂的计算机游戏 (Dosovitskiy &amp; Koltun, 2017; Jaderberg et al., 2019; Vinyals et al., 2019) ，还是通过模拟到真实的转移来解决复杂的机器人控制问题 (Müller et al., 2018;Hwangbo et al., 2019; Molchanov et al., 2019; Andrychowicz et al., 2020)。</p>
<p>​    尽管现代学习方法的采样效率有了很大的提高，但其中大多数仍然需要大量的数据。 在大多数情况下，近年来的结果水平已经上升，因为实验的规模增加，而不是学习的效率。 在复杂环境中进行的数十亿规模的实验现在相对普遍 (Horgan et al., 2018; Espeholt et al., 2018; Kapturowski et al., 2019)，最先进的成果在一次训练中消耗了数万亿次环境转变 (Berner et al., 2019)。</p>
<p>​    为了尽量减少这些大规模实验的周转时间，常用的方法是使用由数百台单独机器组成的分布式超级计算系统 (Berner et al., 2019)。 在这里，我们表明，通过优化结构和提高强化学习算法的资源利用率，我们可以在数十亿个环境转变上训练代理，即使在单个计算节点上也是如此。 我们提出了“Sample Factory”，一个为单机场景优化的高吞吐量训练系统。 Sample Factory基于异步近端策略优化(APPO)算法，是一种强化学习架构，它允许我们在一个只有一个GPU的多核计算节点上积极地并行化经验收集并实现高达130000FPS（每秒环境帧）的吞吐量。 我们描述了理论和实践优化，使我们能够在广泛可用的商品硬件上实现极端帧速率。</p>
<p>​    我们在一组具有挑战性的3D环境中评估我们的算法，并演示如何利用大量的模拟经验来训练达到高技能水平的代理。 然后，我们扩展了Sample Factory，以支持self-play和population-based训练，并应用这些技术来为一个完整的多人游戏的Doom培训高能力的代理 (Kempka et al., 2016)。</p>
<h1 id="Sample-Factory"><a href="#Sample-Factory" class="headerlink" title="Sample Factory"></a>Sample Factory</h1><p>​    Sample factory是一种在单机上进行高吞吐量强化学习的体系结构。 在设计系统时，我们专注于使所有关键计算完全异步，以及最小化组件之间的延迟和通信成本，充分利用快速本地消息传递。</p>
<p>​    一个典型的强化学习场景涉及三个主要的计算工作负载：环境模拟、模型推理和反向传播。 我们的主要动机是构建一个系统，其中最慢的三个工作负载从来不需要等待任何其他进程提供执行下一次计算所需的数据，因为算法的总体吞吐量最终由吞吐量最低的工作负载定义。 为了最小化进程等待的时间，我们需要保证输入的新部分总是可用的，即使在下一步的计算即将开始之前。 计算密集型工作负载从不闲置的系统可以达到最高的资源利用率，从而接近最佳性能。</p>
<h2 id="High-level-design"><a href="#High-level-design" class="headerlink" title="High-level design"></a>High-level design</h2><p>​    最小化所有关键计算的空闲时间的愿望激发了系统的高级设计（图1）。 我们将每个计算工作负载与三种专用类型的组件之一相关联。 这些组件使用基于FIFO队列和共享内存的快速协议相互通信。 队列机制为连续和异步执行提供了基础，只要队列中有要处理的东西，就可以立即启动下一个计算步骤。 将每个工作负载分配给专用组件类型的决定也允许我们独立地并行化它们，从而实现优化的资源平衡。 这与以前的工作不同 (Mnih et al., 2016; Espeholt et al., 2018)，其中单个系统组件，如actor通常负有多重责任。 涉及的三种类型的组件是 rollout workers, policy workers, and learners。</p>
<p><img src="/2021/01/20/Sample-Factory-Egocentric-3D-Control-from-Pixels-at-100000-FPS-with-Asynchronous-Reinforcement-Learning/figure1.png"></p>
<p>Rollout workers单独负责环境模拟。 每个rollout worker托管$$k≥1$$个环境实例，并依次与这些环境交互，收集观察$$x_{t}$$并奖励$$r_{t}$$。 请注意，rollout worker没有自己的策略副本，这使得它们非常轻量级，允许我们在现代多核CPU上大规模并行化收集。</p>
<p>然后将观察$$x_{t}$$和代理的隐藏状态$$h_{t}$$发送给policy worker，policy worker从多个rollout worker收集$$x_{t}$$、$$h_{t}$$的批次，并调用策略$$\pi$$，由神经网络参数化$$\theta <em>{\pi }$$以计算动作分布$$\mu \left ( a</em>{t}|x_{t},h_{t} \right )$$和更新的隐藏状态$$h_{t+1}$$。 然后从分布$$\mu$$中采样$$a_{t}$$的动作，并与$$h_{t+1}$$一起通信回相应的rollout worker。 这个rollout worker使用动作$$a_{t}$$推进模拟和收集下一个观察$$x_{t+1}$$和奖励$$r_{t+1}$$。</p>
<p>Rollout workers保存每个环境转换到共享内存中的轨迹缓冲区。 一旦模拟了$$T$$环境步骤，观察、隐藏状态、动作和奖励的轨迹$$\tau = x_{1},h_{1},a_{1},r_{1},…,x_{T},h_{T},a_{T},r_{T}$$就可以提供给learner。 learner不断地处理批量的轨迹，并更新actor $$\theta _{\pi }$$和critic $$\theta _{V}$$的参数。 这些参数更新一旦可用就发送给policy worker，这减少了前一个版本模型收集的经验量，最小化了平均policy lag。 这就完成了一次训练迭代。</p>
<p>Parallelism。如前所述，rollout worker不拥有该策略的副本，因此基本上是环境实例周围的薄包装。 这允许它们大规模并行化。 此外，Sample Factory还将policy worker并行化。 这可以实现，因为所有当前轨迹数据$$(x_{t},h_{t},a_{t},…)$$都存储在所有进程都可以访问的共享张量中。 这允许policy workers本身处于无状态，因此，他们中的任何一个都可以很容易地处理来自单个环境的连续轨迹步骤。 在实际场景中，2到4个policy worker实例很容易使rollout worker操作饱和，加上一个特殊的采样器设计（第3.2节），使我们能够消除这一潜在的瓶颈。</p>
<p>Learner是我们运行单一副本的唯一组件，至少只要涉及单一策略训练（多策略训练在3.5节中讨论）。 然而，我们可以通过利用Learner上的多个加速器数据并行训练和Hogwild风格的参数更新(Recht et al., 2011)。 加上在复杂环境中稳定训练通常需要的大批处理大小，这使Learner有足够的吞吐量来匹配经验收集率，除非计算图是高度非平凡的。</p>
<h2 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h2><p>​    Rollout workers和policy workers共同组成采样器。 采样子系统对RL算法的吞吐量影响最大，因为它往往是瓶颈。 我们提出了一种具体的实现采样器的方法，它允许通过最小化rollout workers的空闲时间来优化资源利用。</p>
<p>​    首先，注意训练和经验收集是解耦的，因此可以在反向传播步骤中收集新的环境转换。 Rollout workers也没有参数更新，因为操作生成的工作是卸载给policy worker的。 然而，如果没有解决，这仍然使rollout workers等待policy workers产生的行动，并通过进程间通信转移回来。</p>
<p>​    为了缓解这种低效率，我们使用 Double-Buffered Sampling（图2）。 与其只在rollout worker上存储单个环境，不如存储环境$$E_{1},…,E_{k}$$的向量，其中k甚至是为了简单起见。 我们将这个向量分成两组$$E_{1},…,E_{k/2}$$，$$E_{k/2+1},…,E_{k}$$，并在它们之间交替进行。 当第一组环境正在step，第二组的操作是根据policy worker计算的，反之亦然。 有了足够快的policy worker和$$k$$的正确调优值，我们可以完全屏蔽通信开销，并确保在采样期间充分利用CPU核心，如图2所示。 用于双缓冲器的最大性能采样我们希望$$k/2&gt; \left \lceil t_{inf}/t_{env} \right \rceil$$，其中$$t_{inf}$$和$$t_{env}$$分别是平均推理和模拟时间。</p>
<p><img src="/2021/01/20/Sample-Factory-Egocentric-3D-Control-from-Pixels-at-100000-FPS-with-Asynchronous-Reinforcement-Learning/figure2.png"></p>
<h2 id="Communication-between-components"><a href="#Communication-between-components" class="headerlink" title="Communication between components"></a>Communication between components</h2><p>​    解锁本地单机设置的全部潜力的关键是利用系统组件之间的快速通信机制。 如图1所示，信息流有四条主要途径：rollout和policy worker之间的双向沟通，rollout将完整的轨迹传递给learner，以及将参数更新从learner转移到policy worker。 对于前三种交互，我们使用基于PyTorch的共享内存张量机制 (Paszke et al., 2019)。 我们注意到，RL算法中使用的大多数数据结构可以表示为固定形状的张量，无论它们是轨迹、观测还是隐藏状态。 因此，我们在系统RAM中预先分配足够数量的张量。 每当组件需要通信时，我们都会将数据复制到共享张量中，并且只通过FIFO队列发送这些张量的索引，使得消息与传输的总体数据量相比很小。</p>
<p>​    对于参数更新，我们使用GPU上的内存共享。 每当需要更新模型时，policy worker只需将权重从共享内存复制到模型的本地副本。</p>
<p>​    与许多流行的异步和分布式实现不同，我们不执行任何类型的数据序列化作为通信协议的一部分。 在全节流阀下，SampleFactory每秒产生和消耗超过1GB的数据，即使是最快的序列化/去序列化机制也会严重阻碍吞吐量。</p>
<h2 id="Policy-lag"><a href="#Policy-lag" class="headerlink" title="Policy lag"></a>Policy lag</h2><p>​    Policy lag是异步RL算法的固有特性，它是收集经验（行为策略）的策略与学习的目标策略之间的差异。 这种差异的存在为off-policy训练提供了条件。off-policy学习对于策略梯度方法来说是很困难的，其中模型参数通常按照$$\bigtriangledown log\mu (a_{s}|x_{s})q(x_{s},a_{s})$$的方向更新，其中$$q(x_{s},a_{s})$$是策略state-action值的估计。 策略滞后越大，就越难从行为策略中使用一组样本$$x_{s}$$正确估计这个梯度。 从经验上讲，这在学习涉及循环策略、高维观测和复杂行动空间的问题方面变得更加困难，在这些问题中，即使是非常相似的政策也不太可能在很长的轨迹上表现出相同的性能。</p>
<p>​    异步RL方法中的策略滞后可能是通过使用旧策略在环境中的作用引起的，或者在一次迭代中从并行环境中收集更多的轨迹，而不是学习者在一个batch中摄入的轨迹，导致部分经验在处理时变得不符合策略。 我们处理第一个问题，一旦有了新的参数立即更新政策工作者的模型。 在Sample factory中，参数更新很便宜，因为模型存储在共享内存中。 一个典型的更新需要小于1ms，因此我们收集了与“master”不同的策略非常少的经验。</p>
<p>​    然而，不一定能够消除第二个原因。 在RL中，并行收集来自许多环境实例的训练数据是有益的。 这不仅使经验相互关联，它还允许我们利用多核CPU，并且具有更大的$$k$$值（每个核心的环境），充分利用双缓冲采样器。 在一次经验收集的“迭代”中，n个rollout workers，每个运行k个环境，将产生总共$$N_{iter}=n\times k\times T$$个样本。 由于我们在学习者步骤之后立即更新策略工作者，可能在轨迹的中间，这导致轨迹中最早的样本平均滞后于$$N_{iter}/N_{batch}-1$$策略更新，而最新的样本没有滞后。</p>
<p>​    可以通过减小$$T$$或增加最小批量大小$$N_{batch}$$来最小化策略滞后。 两者都对学习有影响。 我们通常想要更大的T，在$$2^5-2^7$$范围内，通过反向传播与时间循环策略，大的最小批量可能会降低样本效率。 最佳批次大小取决于特定的环境，更大的批次被证明适合于具有噪声梯度的复杂问题(McCandlish et al., 2018)。</p>
<p>​    此外，还有两大类用于处理off-policy学习的技术。 第一个想法是应用信任区域方法 (Schulman et al., 2015; 2017)通过在学习期间保持接近行为策略，我们提高了使用该策略的样本获得的梯度估计的质量。 另一种方法是使用重要性采样来纠正目标价值函数$$V^\pi$$以改进目标政策下折扣奖励之和的近似 (Harutyunyan et al., 2016). </p>
<p>IMPALA (Espeholt et al., 2018) 介绍了V-trace算法，该算法使用截断的重要性抽样权重来修正值目标。 这有助于提高off-policy学习的稳定性和样本效率。</p>
<p>​    这两种方法都可以独立应用，因为V-trace纠正了我们的训练目标，信任区域防止破坏性参数更新。 因此，我们在Sample Factory实现了V-trace和PPO clipping。 是否使用这些方法可以被认为是特定实验的超参数选择。 我们发现PPO clipping和V-trace的结合在任务之间很好地工作，并产生稳定的训练，因此我们决定在本文报道的所有实验中使用这两种方法。</p>
<h2 id="Multi-agent-learning-and-self-play"><a href="#Multi-agent-learning-and-self-play" class="headerlink" title="Multi-agent learning and self-play"></a><strong>Multi-agent learning and self-play</strong></h2><p>​    通过多智能体强化学习和self-play，在深度RL方面取得了一些最先进的最新成果 (Bansal et al., 2018; Berner et al., 2019)。 通过self-play训练的特工比在固定场景中训练的对手表现出更高的技能水平 (Jaderberg et al.,2019)。 随着策略在self-play过程中的改进，它们产生了一个逐渐增加复杂性的训练环境，自然为代理提供了一个课程，并允许他们逐步学习更复杂的技能。 复杂行为(例如合作和工具使用)已被证明出现在这些培训情景中 (Baker et al., 2020)。</p>
<p>​    还有证据表明，在多智能体环境中一起训练的代理群体可以避免常规的自玩设置所经历的一些失败模式，例如早期收敛到局部最优或过度拟合。 不同的培训人群可以使代理人接触到更广泛的对抗性政策，并产生更强大的代理人，在复杂的任务中达到更高的技能水平(Vinyals et al., 2019; Jaderberg et al., 2019)。</p>
<p>​    为了释放我们系统的全部潜力，我们增加了对多代理环境的支持，以及对代理的培训人群。Sample Factory自然扩展到多智能体和多策略学习。 由于rollout workers只是环境实例的包装器，他们完全不知道提供操作的策略。 因此，为了在培训过程中增加更多的政策，我们只是催生了更多的policy workers和更多的learners来支持他们。 在rollout workers中，对于每个多代理环境中的每个代理，我们在每一集开始时从族群中抽样一个随机的策略$$\pi_{i}$$。 然后使用一组FIFO队列将操作请求路由到相应的policy workers，每$$\pi_{i}$$一个队列。 我们在这项工作中使用的基于族群的设置在第4节中得到了更详细的解释。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/01/20/Sample-Factory-Egocentric-3D-Control-from-Pixels-at-100000-FPS-with-Asynchronous-Reinforcement-Learning/" data-id="ckoln5scu0007g4va46zg44dm" data-title="Sample Factory:Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Hybrid-Reward-Architecture-for-Reinforcement-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/" class="article-date">
  <time class="dt-published" datetime="2021-01-19T12:54:56.000Z" itemprop="datePublished">2021-01-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/">Hybrid Reward Architecture for Reinforcement Learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    强化学习(RL)的主要挑战之一是泛化。 在典型的深度RL方法中，这是通过使用深度网络用低维表示逼近最优值函数来实现的。 虽然这种方法在许多领域很好地工作，但在最优值函数不能很容易地简化为低维表示的领域，学习可能非常缓慢和不稳定。 本文通过提出一种新的方法——混合奖励体系结构(Hybrid Reward Architecture)，为解决这些具有挑战性的领域做出了贡献。 HRA以分解的奖励函数作为输入，学习每个组件奖励函数的单独值函数。 由于每个组件通常只依赖于所有特征的子集，因此相应的值函数可以更容易地通过低维表示来逼近，从而实现更有效的学习。 我们在a toy-problem(an agent has to eat 5 randomly located fruits)和Atari game Ms.Pac-Man上演示了HRA，在HRA达到了超越人类的表现。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    在强化学习(RL)中，目标是找到一种行为策略，以数据驱动的方式最大限度地提高回报-随着时间的推移获得的折扣奖励之和。 RL的主要挑战之一是对方法进行缩放，使它们能够应用于大型的现实世界问题。 由于这些问题的状态空间通常是巨大的，因此需要强有力的泛化才能有效地学习好的政策。</p>
<p>​    Mnih等人在这方面取得了重大突破：通过将标准RL技术与深度神经网络相结合，他们通过从像素中学习策略，在大量Atari2600游戏中取得了人类以上的表现。 通过逼近最优值函数，得到了深度Q网络(DQN)方法的泛化特性。 值函数在RL中起着重要的作用，因为它预测期望的回报，条件是状态或状态-动作对。 一旦知道了最优值函数，就可以通过对其贪婪地行事来导出最优策略。 通过用深度神经网络对最优值函数的当前估计进行建模，DQN对值函数进行了很强的泛化，从而对策略进行了推广。</p>
<p>​    通过对最优值函数模型的正则化，实现了DQN的泛化行为。 然而，如果最优值函数非常复杂，那么学习精确的低维表示可能是具有挑战性的，甚至是不可能的。 因此，当最优值函数不能很容易地简化为低维表示时，我们认为在目标侧应用一种互补的正则化形式。 具体来说，我们建议用一种更容易学习的替代价值函数来代替最优价值函数作为训练目标，但当对它贪婪地采取行动时，它仍然会产生一个合理但通常不是最优的政策。</p>
<p>​    目标函数正则化背后的关键观察是，当代理对它们贪婪地行为时，两个非常不同的值函数可能导致相同的策略。 同时，一些价值函数比其他函数更容易学习。 Intrinsic motivation (Stout et al., 2005; Schmidhuber, 2010)利用这一观察来改善稀疏回报领域的学习，方法是在来自环境的奖励中添加特定于领域的内在奖励信号。 当内在奖励函数是基于潜力的，则保持由此产生的政策的最优性。 在我们的例子中，我们的目标是更简单的值函数，它们更容易用低维表示来表示。</p>
<p>​    我们构建易学价值函数的主要策略是将环境的奖励函数分解为n个不同的奖励函数。 他们每个人都被分配一个单独的强化学习代理。 类似于 Horde architecture (Sutton et al., 2011)，所有这些代理都可以通过使用off-policy学习在相同的样本序列上并行学习。 每个代理将当前状态的动作值提供给聚合器，聚合器将它们组合成每个动作的单个值。 当前操作是根据这些聚合值选择的。</p>
<p>​    我们在两个领域测试我们的方法：一个玩具问题，代理必须吃5个随机定位的水果，和 Ms. Pac-Man, one of the hard games from the ALE benchmark set。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>​    我们的HRA方法建立在 Horde architecture (Sutton et al., 2011)的基础上。 Horde体系结构由大的“demons”组成，通过off-policy学习并行学习。 每个demon根据自己的策略和伪奖励函数训练一个单独的一般价值函数(GVF)。 伪奖励可以是编码有用信息的任何基于特征的信号。 Horde 体系结构的重点是建立关于世界的一般知识，通过大量的GVF编码。 HRA专注于训练环境奖励功能的独立组件，以便更有效地学习控制策略。UVFA (Schaul et al., 2015)也建立在Horde的基础上，但沿着不同的方向扩展。 UVFA可以在不同的任务/目标之间进行泛化。 它不涉及如何解决一个单一的、复杂的任务，这是HRA的重点。</p>
<p>​    关于多重奖励函数的学习也是多目标学习的一个主题(Roijers et al., 2013)。 因此，HRA可以被看作是应用多目标学习，以便更有效地学习单个奖励函数的策略。</p>
<p>​    Russell &amp; Zimdar (2003) and Sprague &amp; Ballard (2003)等人对奖励函数分解进行了研究。 这一早期的工作集中在实现最佳行为的策略上。 我们的工作旨在通过使用更简单的价值函数和放松最优性要求来提高学习效率。</p>
<p>​    HRA和UNREAL(Jaderberg et al., 2017)也有相似之处。 值得注意的是，两者都解决了多个较小的问题，以解决一个难题。 然而，这两种体系结构在其工作方式上以及它们所解决的挑战类型上是不同的。 UNREAL是一种在困难场景中增强表示学习的技术。 它通过使用辅助任务来帮助训练深层神经网络的lower-level层来实现。 这种具有挑战性的表示学习场景的一个例子是学习在3D迷宫领域中导航。 在Atari游戏中，所报道的UNREAL的性能增益是最小的，这表明标准的深度RL体系结构足够强大，可以提取相关的表示。 相比之下，HRA体系结构将任务分解为更小的部分。 HRA的多个较小的任务不是无监督的；它们是与主要任务直接相关的任务。 此外，虽然UNREAL本质上是一种深度RL技术，但HRA与所使用的函数近似类型无关。 可与深度神经网络结合，但它也适用于精确的表格表示。 HRA对于具有高级表示不足以有效地解决任务的领域是有用的。</p>
<p>​    Diuk的面向对象方法(Diuk et al., 2008) 是第一批在电子游戏中展示高效学习的方法之一。 该方法利用与过渡动态相关的领域知识来有效地学习紧凑的过渡模型，然后利用动态编程技术找到解决方案。 这种固有的基于模型的方法的缺点是，虽然它有效地学习了一个非常紧凑的过渡动力学模型，但它不会减少问题的状态空间。 因此，它没有解决 Ms. Pac-Man的主要挑战：它的巨大状态空间，甚至对于DP方法来说也是棘手的(Diuk将他的方法应用于一个只有6个对象的Atari游戏，而Pac-Man女士有150多个对象)。</p>
<p>​    最后，HRA涉及选项 (Sutton et al., 1999; Bacon et al., 2017)，以及更普遍的hierarchical learning (Barto &amp; Mahadevan, 2003; Kulkarni et al., 2016)。 选项是临时扩展的动作，就像HRA的头一样，可以根据自己的（内在的）奖励功能并行训练。 然而，一旦一个选项被训练，其内在奖励功能的作用就结束了。 使用选项的高级代理将其视为另一个操作，并使用自己的奖励函数对其进行评估。 这可以在学习中产生很大的加速，并有助于更好的探索，但它们并不直接使高级代理的价值功能变得不那么复杂。 HRA的负责人代表价值观，用环境奖励的组成部分来训练。 即使经过训练，这些值也保持相关性，因为聚合器使用它们来选择它的操作。</p>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>​    考虑一个马尔可夫决策过程$$ \left \langle S,A,P,R_{env},\gamma \right \rangle$$，它在离散时间步长$$t$$上模拟与环境交互的代理。它有状态集$$S$$、动作集$$A$$、环境奖励函数$$R_{env}:S\times A \times S\rightarrow \mathbb{R}$$，以及转移概率函数$$P:S\times A\times S\rightarrow \left [ 0,1 \right ]$$。 在时间步$$t$$，代理观察状态$$s_{t}\in S$$，并在$$a_{t}\in A$$处采取行动。 代理观察下一个状态$$s_{t+1}$$，从过渡概率分布$$P\left ( s_{t},a_{t} \right )$$和奖励$$r_{t}= R_{env}\left ( s_{t},a_{t},s_{t+1} \right )$$中提取。 行为由策略$$\pi$$定义：$$S\times A\rightarrow \left [ 0,1 \right ]$$，它表示相对于动作的选择概率。 代理的目标是找到一种最大限度地提高回报期望的策略，即折扣奖励之和：$$G_{t}:=\sum_{i=0}^{\infty }\gamma ^{i}r_{t+i}$$，其中折扣因素$$\gamma \in \left [ 0,1 \right ]$$控制即时奖励与未来奖励的重要性。 每个策略$$ \pi $$都有一个相应的动作-价值函数，当根据该策略行事时，它给出以状态和动作为条件的预期回报：<br>$$<br>Q^{\pi }\left ( s,a \right )=E\left [ G_{t}|s_{t}=s,a_{t}=a,\pi  \right ]\tag{1}<br>$$<br>​    通过迭代改进最优动作值函数$$Q^{*}\left ( s,a \right ):=max_{\pi }Q^{\pi }(s,a)$$的估计，可以找到最优策略$$\pi ^{\ast }$$。 一旦$$Q ^{\ast }$$是足够精确的近似，对它采取贪婪的行动就会产生最优策略。</p>
<h2 id="Hybrid-Reward-Architecture"><a href="#Hybrid-Reward-Architecture" class="headerlink" title="Hybrid Reward Architecture"></a><strong>Hybrid Reward Architecture</strong></h2><p>​    Q值函数通常用权重向量$$\theta :Q(s,a;\theta )$$的函数逼近器来估计。 DQN使用深度神经网络作为函数逼近器，通过最小化损失函数序列来迭代地改进的$$Q ^{\ast }$$估计：</p>
<p><img src="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/loss_function.png"></p>
<p>​    来自上一次迭代的权重向量$$\theta _{i-1}$$，使用单独的目标网络进行编码(nature dqn)。</p>
<p>​    我们指的是最小损失函数作为训练目标的Q值函数。 如果对训练目标贪婪地采取行动导致在环境的奖励函数下最优的政策，我们将称之为一致(consistent)的训练目标；如果对训练目标贪婪地采取行动导致在环境的奖励函数下的良好策略，但不是最优策略-，我们将称之为半一致(semi-consistent)的训练目标。 对于（2）训练目标是$$Q_{env}^{*}$$，是$$R_{env}$$下的最优动作-值函数，是默认的一致训练目标。</p>
<p>​    一个训练目标是一致的，这并不意味着学习这个目标有多容易。 例如如果$$R_{env}$$是稀疏的，那么默认的学习目标可能很难学习。 在这种情况下，添加一个基于潜在的额外奖励信号到$$R_{env}$$可以产生一个替代的一致学习目标，更容易学习。 但是稀疏的环境奖励并不是一个培训目标难以学习的唯一原因。 我们的目标是为默认训练目标$$Q_{env}^{*}$$很难学习的领域找到一个替代的训练目标，因为函数是高维的，很难概括。 我们的方法是基于奖励函数的分解。</p>
<p>​    我们建议将奖励函数$$R_{env}$$分解为n个奖励函数：并就这些奖励功能中的每一个培训一个单独的强化学习代理。 一个奖励函数可能有无穷多个不同的分解，但要实现易于学习的价值函数，分解应该是每个奖励函数主要受少量状态变量的影响。</p>
<p><img src="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/reward_functions.png"></p>
<p>​    因为每个代理$$k$$都有自己的奖励函数，所以也有自己的$$Q$$值函数$$Q_{k}$$。 一般来说，不同的代理可以共享深度Q网络的多个low-level层。 因此，我们将使用一个向量$$\theta $$来描述代理的组合权重。 我们将表示所有$$Q$$值函数的组合网络称为混合奖励体系结构(HRA),参加图1。 HRA的操作选择是基于代理的Q值函数之和，我们称之为$$Q_{HRA}$$：</p>
<p><img src="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/q_hra.png"></p>
<p>​    <img src="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/figure1.png"></p>
<p>代理的集合可以被看作是具有多个头的单个代理，每个头在不同的奖励函数下产生当前状态的动作值。</p>
<p>​    与HRA相关的损失函数序列为：</p>
<p><img src="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/q_hra_loss.png"></p>
<p>​    通过将这些损失函数最小化，HRA的不同头部在不同的奖励函数下近似最优动作值函数:$$Q_{1}^{<em>},…,Q_{n}^{</em>}$$。 此外，$$Q_{HRA}$$近似于$$Q_{HRA}^{*}$$，定义为：</p>
<p><img src="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/q_hrak.png"></p>
<p>​    注意，$$Q_{HRA}^{<em>}$$与$$Q_{env}^{</em>}$$不同，一般不一致。</p>
<p>​    另一个培训目标是评估每个组件奖励函数下的统一随机策略$$v$$的结果：$$Q_{HRA}^{v}:=\sum_{k=1}^{n}Q_{k}^{v}(s,a)$$。 $$Q_{HRA}^{v}$$等于$$Q_{env}^{v}$$。在$$R_{env}$$下的随机策略的$$Q$$值，如下所示：</p>
<p><img src="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/q_env.png"></p>
<p>可以使用预期的Sarsa更新规则 (van Seijen et al., 2009),来学习这个培训目标，方法是将（7）替换为</p>
<p><img src="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/gongsi_8.png"></p>
<p>​    对于随机策略的$$Q$$值，贪婪地行事可能会产生一个比随机略好的策略，但令人惊讶的是，我们发现，对于许多基于导航的域$$Q_{HRA}^{v}$$充当半一致的训练目标。</p>
<h2 id="通过使用高级领域知识进一步提高性能"><a href="#通过使用高级领域知识进一步提高性能" class="headerlink" title="通过使用高级领域知识进一步提高性能"></a>通过使用高级领域知识进一步提高性能</h2><p>​    在其基本设置中，应用于HRA的唯一领域知识是以分解的奖励函数的形式。 然而，HRA的优势之一是它可以很容易地利用更多的领域知识，如果可用的话。 领域知识可以通过以下方式之一加以利用：</p>
<ul>
<li>删除不相关的特征。 不以任何方式（直接或间接）影响所获得奖励的特征只会给学习过程增加噪音，并且可以被移除。</li>
<li>识别终端状态。 终端状态是不能收到进一步奖励的状态；它们的定义是值为0。 利用这些知识，HRA可以避免用值网络逼近这个值，这样权重就可以完全用来表示非终端状态。</li>
<li>使用伪奖励函数。 与其使用环境奖励的组件更新HRA的头部，还可以使用伪奖励来更新。 在这个场景中，一组GVFS使用伪奖励并行训练。</li>
</ul>
<p>虽然这些方法并不特定于HRA，但HRA可以在很大程度上利用领域知识，因为它可以将这些方法单独应用于每个头。 我们在4.1节中经验性地展示了这一点。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="水果收集任务"><a href="#水果收集任务" class="headerlink" title="水果收集任务"></a>水果收集任务</h2><p>​    在我们的第一个领域，我们考虑一个代理，必须尽快收集水果在10×10网格。这里有10个可能的水果位置，分布在整个网格上。 每一轮，一个水果被随机放置在这10个地点中的5个。 代理从一个随机位置开始。 如果水果被吃掉，奖励为1，否则为0。 当所有5个水果被吃掉或者达到300步，一轮结束。</p>
<p>​    我们比较了使用相同网络的DQN和HRA的性能。 对于HRA，我们将奖励功能分解为10个不同的奖励功能，每个可能的水果位置一个。 该网络由长度为110的二进制输入层组成，编码代理的位置和每个位置是否存在水果。接下来是长度为250的完全连接的隐藏层。 该层连接到10个头，每个头由4个线性节点组成，表示4个动作在不同奖励函数下的动作值。 最后，使用最后的长度为4的线性层来计算所有节点跨头的平均值，该层连接每个头中相应节点的输出。 该层具有值1的固定权重（即实现公式5）。 与DQN的区别在于，DQN使用损失函数（2）从第四层更新网络，而HRA使用损失函数（6）从第三层更新网络。</p>
<p><img src="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/figure2.png"></p>
<p>​    除了完整的网络之外，我们还使用不同层次的领域知识进行测试，如第3.2节所述：1)删除每个头部的无关特征（只提供代理的位置相应的水果特征)；2)上述加识别终端状态；3）上述加使用伪奖励来学习GVFs到10个位置中的每个位置（而不是在每个位置学习与水果相关的值函数）。 优点是，即使在一个地点没有水果，这些GVFs也可以被训练。 一个特定位置的头复制相应GVF的Q值，如果该位置当前包含一个水果，或者输出0。 我们将这些分别称为$$HRA+1$$、$$HRA+2$$和$$HRA+3$$。 对于DQN，我们还测试了应用于与$$HRA+1$$相同网络的版本；我们将此版本称为$$DQN+1$$。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/Maluuba/hra">代码位置</a></p>
<p>​    训练样本是由随机策略生成的；训练过程是通过在每一轮之后对学习价值函数的贪婪策略进行评估来跟踪的。 对于HRA，我们以Q∗HRA作为训练目标（使用方程7），以及QυHRA（使用方程8)进行了实验）。 同样，对于DQN，我们使用了默认的培训目标$$Q_{HRA}^{*}$$以及$$Q_{HRA}^{v}$$。 我们分别优化了每个方法的步长和折扣因子。</p>
<p>​    每个方法的最佳设置结果如图3所示。 对于DQN，使用$$Q_{env}^{*}$$作为训练目标会产生最好的性能，而对于HRA，使用$$Q_{HRA}^{v}$$会产生最好的性能。 总的来说，HRA显示了DQN的明显性能提升，尽管网络是相同的。 此外，添加不同形式的领域知识会导致进一步的大改进。 虽然使用由领域知识增强的网络结构可以提高HRA的性能，但使用同一网络进行DQN会导致性能下降。 大的当终端状态被识别时，性能的提高是由于表示成为一个单一的热向量。 因此，我们删除了隐藏层，并直接将这个单热矢量输入到不同的头部。 由于头部是线性的，这种表示简化为精确的表格表示。 对于表格表示，我们使用了与深度网络版本的最佳步长相同的步长。</p>
<p><img src="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/figure3.png"></p>
<h2 id="ATARI-game-Ms-Pac-Man"><a href="#ATARI-game-Ms-Pac-Man" class="headerlink" title="ATARI game: Ms. Pac-Man"></a>ATARI game: Ms. Pac-Man</h2><p>​    我们的第二个领域是Atari2600游戏Ms. Pac-Man（见图4）。 点是通过吃颗粒获得的，同时避免鬼魂(与一个人接触会导致Ms. Pac-Man失去生命)。 吃一种特殊的能量颗粒会使鬼魂在很短的时间内变成蓝色，从而使它们被吃掉以获得额外的分数。 额外的水果可以吃进一步的积分，每级两次。 当所有的球团都被吃掉时，一个新的水平就开始了。 一共有4个不同的地图和7个不同的水果类型，每个都有不同的点值。 我们在补充材料中提供了有关领域的全部细节。</p>
<p><img src="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/pac_game.png"></p>
<ul>
<li> Baselines。虽然我们的Ms. Pac-Man的版本与文献中使用的版本相同，但我们使用不同的预处理。 因此，为了测试我们的预处理效果，我们实现了A3C方法 (Mnih et al., 2016)，并在我们的预处理中运行它。 我们将预处理的版本称为‘A3C（channels）’，标准预处理的版本‘A3C（pixels）’，以及文献‘A3C（reported）’中报告的A3C评分。</li>
<li> Preprocessing。来自ALE的每个帧是210×160像素。 我们切割底部部分和顶部部分160×160像素。 由此，我们提取不同对象的位置，并为每个对象创建一个单独的输入通道，对其位置进行编码，精度为4像素。 这导致11个大小为40×40的二进制通道。 具体来说，Ms. Pac-Man有一个通道，四个鬼魂中的每一个，四个蓝色鬼魂中的每一个（这些都被视为不同的物体)，水果加上一个通道，所有的颗粒(包括动力颗粒）。 对于A3C，我们将鬼魂的4个通道组合成一个单一的通道，使其能够更好地跨越鬼魂。 我们对蓝鬼的4个频道也是如此。 我们没有给出文献中最后4帧的历史，而是给出了Ms. Pac-Man作为长度为4的1-热向量（代表4个指南针方向)的方向）。</li>
<li> HRA architecture。环境奖励信号与游戏中得分相对应。 在分解奖励函数之前，我们通过添加与鬼魂接触的负面奖励-1000来执行奖励整形(这会导致Ms. Pac-Man失去生命)。 在此之后，奖励被分解成游戏中的每个对象（球团/水果/鬼/蓝鬼）都有自己的奖励功能。 因此，有一个单独的RL代理与游戏中的每个对象相关联，它估计其相应奖励函数的Q值函数。</li>
</ul>
<p>为了估计每个组件奖励函数，我们使用3.2节中讨论的三种形式的领域知识。 HRA使用学习伪Q值（值在[0,1]范围内）的GVFs来到达地图上的特定位置(四张地图中的每一张都学习单独的GVFs)。 与水果收集任务（第4.1节）相比，HRA在训练过程中学习了它的部分表示：它从0GVF和0头开始，用于球团。 通过在迷宫周围徘徊，它发现了它可以到达的新地图位置，从而创建了新的GVF。 每当代理在一个新的位置发现一个球团时，它就会创建一个与球团对应的新头。</p>
<p>对象的Q值（球团/水果/鬼魂/蓝色幽灵）被设置为与对象的位置相对应的GVF的伪Q值(即移动对象每次使用不同的GVF)，乘以一个权重，该权重被设置为等于对象被吃掉时收到的奖励。 如果一个对象不在屏幕上，它的所有Q值都是0。</p>
<p>我们测试两种聚合器类型。 第一个是一个线性的，它求和所有头部的Q值（见方程5）。 对于第二个，我们取所有产生点的头的总和，并正常化得到的Q值；然后，我们添加规则鬼头的Q值之和，乘以一个权重向量。</p>
<p>对于探索，我们测试两种互补类型的探索。 每种类型都为体系结构增加一个额外的探索头。 第一种类型，我们称为多样化，产生随机Q值，从均匀分布在[0,20]。 我们发现，只有在前50个步骤中，才能确保随机开始每轮。 第二种类型，我们称之为基于计数的，为状态-动作对增加了一个额外的奖励，这些奖励还没有被探索过很多。 它受到上置信度的启发(Auer et al., 2002)。 详细情况见补充材料。</p>
<p>在我们的最后一个实验中，我们实现了一个特殊的头部激励通过<em>executive-memory</em> literature (Fuster, 2003; Gluck et al., 2013).。 当一个人类游戏玩家达到他的认知和身体能力的最大值时，他开始寻找有利的情况，甚至是故障，并记住它们。 这种认知过程确实是记忆一系列动作（也叫习惯），不一定是最优的。 我们的执行记忆主管记录了每一系列的行动，导致通过一个级别，没有任何杀戮。 然后，当面对相同的水平时，头部给记录的动作一个很高的值，以便强制聚合器的选择。 请注意，我们的执行内存的简化版本没有概括。</p>
<ul>
<li>Evaluation metrics。有两种不同的评价方法在不同的文献中使用，导致非常不同的分数。 由于ALE最终是一个确定性的环境（它使用随机数生成器实现伪随机性，该生成器总是以相同的种子开始），这两个评估度量的目的是在评估中创建随机性，以便对具有更泛化行为的方法进行更高的评级。 第一个度量引入了一种温和的随机性形式，在将控制交给学习算法之前，采取随机数量的no-op操作。 然而，在Ms. Pac-Man的情况下，游戏从一个特定的非活动期开始，超过了最大的无操作步骤，最终导致游戏有一个固定的开始。 第二个度量沿着人类轨迹选择随机起点，并导致更强的随机性，并确实导致预期的随机开始评估。 我们将这些度量称为“固定开始”和“随机开始’。</li>
<li>Results。图5为训练曲线；表1为训练后的最终得分。 报告的最佳固定开始分数来自STRAW(Vezhnevets等人，2016年)；报告的最佳随机开始评分来自 Dueling network architecture (Wang et al., 2016)。 人类固定的起始分数来自Mnih et al. (2015)；人类随机开始分数来自Nair et al. (2015)。 我们训练A3C8亿帧。 因为HRA学得很快，所以我们只训练了5000轮，对应大约1.5亿帧（注意，更好的策略会导致每集更多的帧）。 我们尝试了HRA的几个不同的设置：带/不正常化和带/不带每种类型的探索。 为HRA显示的分数使用了最好的组合：与正常化和两种勘探类型。 所有的组合在训练中都达到了10，000点以上，除了没有任何探索的组合，这一点也不奇怪，它的表现非常糟糕。 有了最好的组合，HRA不仅在这两个指标上的表现都超过了最先进的水平，而且还显著地超过了人类的得分，令人信服地证明了HRA的实力。</li>
</ul>
<p><img src="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/table1.png"></p>
<p>​    <img src="/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/figure5.png"></p>
<p>比较表1中的A3C（pixels）和A3C（channels）显示了一个令人惊讶的结果：虽然我们通过将屏幕图像分离到相关的对象通道来使用高级预处理，但这并没有显著改变A3C的性能。</p>
<p>​    在我们的最后一个实验中，我们测试了HRA的性能，如果它利用了固定启动评估度量的弱点，使用了执行内存的简化版本。 使用这个版本，我们不仅超过了266330分的人类高分，我们在不到3000轮的情况下达到了999，990分的最大可能分数。 曲线在第一阶段是缓慢的，因为模型必须被训练，但即使进一步的水平变得越来越困难，加速，利用已经知道的地图。 获得更多的分数是不可能的，不是因为游戏结束，而是因为当达到百万分时，分数溢出到0。</p>
<h1 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h1><p>​    HRA的优点之一是它可以比单头方法更好地利用领域知识。 水果收集任务清楚地表明了这一点：虽然删除不相关的特征提高了HRA的性能，但当提供相同的网络体系结构时，DQN的性能下降。 此外，将像素图像分离成多个二进制通道只会使A3C的性能在直接从像素学习上有很小的提高。 这表明，现代深层RL与Ms. Pac-Man斗争的原因与从像素学习无关；根本问题是Ms. Pac-Man的最优值函数不能很容易地映射到低维表示。</p>
<p>​    HRA通过学习近1800个一般值函数来解决Ms. Pac-Man的问题。 这导致了问题大小的指数分解：而与二进制通道对应的输入状态空间的顺序是1077，每个GVF有一个状态空间的顺序是103个状态，小到可以在没有任何函数近似的情况下表示。 虽然我们可以使用一个深网络来表示每个GVF，但使用一个深网络来处理这样的小问题比它所能做的更痛苦，正如在水果收集领域的实验所证明的那样。</p>
<p>​    我们认为，许多现实世界的任务允许奖励分解。 即使奖励函数只能分解成两个或三个分量，这已经可以帮助很多，因为分解可能导致的问题大小的指数下降。</p>
<h1 id="个人总结"><a href="#个人总结" class="headerlink" title="个人总结"></a>个人总结</h1><ul>
<li>HRA是将单个价值函数拆分为多个简单的价值函数分别学习，并不一定能得到最优值，降低了最优的标准。</li>
<li>识别终端状态。 终端状态是不能收到进一步奖励的状态；它们的定义是值为0。 利用这些知识，HRA可以避免用值网络逼近这个值，这样权重就可以完全用来表示非终端状态。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/01/19/Hybrid-Reward-Architecture-for-Reinforcement-Learning/" data-id="ckoln5sd2000dg4va7u1k3ps7" data-title="Hybrid Reward Architecture for Reinforcement Learning" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Policy-Gradient-Methods-for-Reinforcement-Learning-with-Function-Approximation" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/01/15/Policy-Gradient-Methods-for-Reinforcement-Learning-with-Function-Approximation/" class="article-date">
  <time class="dt-published" datetime="2021-01-15T14:57:51.000Z" itemprop="datePublished">2021-01-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/01/15/Policy-Gradient-Methods-for-Reinforcement-Learning-with-Function-Approximation/">Policy Gradient Methods for Reinforcement Learning with Function Approximation</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/01/15/Policy-Gradient-Methods-for-Reinforcement-Learning-with-Function-Approximation/" data-id="ckoln5sct0006g4va4l8z9suf" data-title="Policy Gradient Methods for Reinforcement Learning with Function Approximation" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Mastering-Fighting-Game-Using-Deep-Reinforcement-Learning-With-Self-play" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/01/14/Mastering-Fighting-Game-Using-Deep-Reinforcement-Learning-With-Self-play/" class="article-date">
  <time class="dt-published" datetime="2021-01-14T12:21:44.000Z" itemprop="datePublished">2021-01-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/01/14/Mastering-Fighting-Game-Using-Deep-Reinforcement-Learning-With-Self-play/">Mastering Fighting Game Using Deep Reinforcement Learning With Self-play</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​        在游戏AI的研究中，一对一的战斗游戏在棋盘游戏和实时模拟游戏之间起到了桥梁的作用，因为它需要中等水平的计算能力，具有中等大小的复杂性。 在本文中，我们提出了一种使用深度强化学习与self-play和蒙特卡罗树搜索(MCTS)创建战斗游戏AI代理的方法。 我们还分析了各种强化学习设置，如状态向量的变化，reward shaping，以及具有新性能度量的对手组合。 用该方法训练的Agent对其他AI进行了评价。 评估结果表明，将MCTS和self-play混合在1：3的比例下，可以以94.4%的胜率压倒其他AI。 完全训练的代理理解游戏机制，使它等待直到接近敌人，并在最佳时机执行行动。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    FightingICE是一个1v1战斗游戏环境，其名称来源于Intelligent Computer Entertainment lab in Ritsumeikan </p>
<p>university组织的战斗游戏平台。 一对一的战斗游戏对于研究不同的人工智能算法具有明显的特点。 首先，它是介于棋盘游戏和实时策略模拟游戏之间的中间步骤。 棋盘游戏AI，如深蓝[1]或阿尔法围棋[2]有足够的时间来考虑玩家在玩游戏时的未来行动。 然而，一对一的战斗游戏AI几乎没有时间，所以它必须实时反应。 这使得战斗游戏比棋盘游戏更困难。 同时，战斗游戏比星际争霸或DOTA2等实时模拟游戏具有更短的游戏时间和更少的动作复杂度。 它具有设计和评估AI算法的优点，计算量小</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/01/14/Mastering-Fighting-Game-Using-Deep-Reinforcement-Learning-With-Self-play/" data-id="ckoln5scq0004g4vaetql9otg" data-title="Mastering Fighting Game Using Deep Reinforcement Learning With Self-play" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/05/12/docker-study/">docker study</a>
          </li>
        
          <li>
            <a href="/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/">IMPALA:Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</a>
          </li>
        
          <li>
            <a href="/2021/05/06/Asynchronous-Methods-for-Deep-Reinforcement-Learning/">Asynchronous Methods for Deep Reinforcement Learning</a>
          </li>
        
          <li>
            <a href="/2021/05/06/Grandmaster-level-in-StarCraftII-using-multi-agent-reinforcement-learning/">Grandmaster level in StarCraftII using multi-agent reinforcement learning</a>
          </li>
        
          <li>
            <a href="/2021/04/26/Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning/">Genetic State-Grouping Algorithm for Deep Reinforcement Learning</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 QilongPan<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>