<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="潘其龙">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="潘其龙">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="QilongPan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>潘其龙</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">潘其龙</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/06/21/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/06/21/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6/" class="post-title-link" itemprop="url">无人驾驶</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-06-21 21:30:33" itemprop="dateCreated datePublished" datetime="2022-06-21T21:30:33+08:00">2022-06-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-22 00:03:59" itemprop="dateModified" datetime="2022-06-22T00:03:59+08:00">2022-06-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="路径规划"><a href="#路径规划" class="headerlink" title="路径规划"></a>路径规划</h1><h2 id="What-is-motion-planning"><a href="#What-is-motion-planning" class="headerlink" title="What is motion planning?"></a>What is motion planning?</h2><ul>
<li>Planning确实是无人车目前最困难也最有挑战的部分</li>
<li>本质是什么？</li>
</ul>
<script type="math/tex; mode=display">
argmin^{}_{x}f(x)</script><ul>
<li>搜索问题<ul>
<li>Google:Query词，返回最优结果</li>
<li>无人车：当前环境和当前状态，当前路径上最优选择</li>
<li>什么是好规划？</li>
</ul>
</li>
<li>“好”其实就是一个目标函数：<script type="math/tex">f(x)</script><ul>
<li><script type="math/tex">f(x)</script>的最优解</li>
</ul>
</li>
</ul>
<h2 id="Motion-planning的三个领域"><a href="#Motion-planning的三个领域" class="headerlink" title="Motion planning的三个领域"></a>Motion planning的三个领域</h2><ul>
<li>Robotics Fileds:<ul>
<li>生成轨迹实现目标</li>
<li>RRT,<script type="math/tex">A^{*}</script>，<script type="math/tex">D^{*}</script>,<script type="math/tex">D^{*} Lite</script></li>
</ul>
</li>
<li>Control Theory:<ul>
<li>动态系统理论实现目标状态</li>
<li>MPC,LQR</li>
</ul>
</li>
<li>AI：生成状态和Action的一个映射<ul>
<li>Reinforcement learning, Imitation Learning</li>
<li>cited by motion planning by Steve Lavelle <a target="_blank" rel="noopener" href="http://planning.cs.uiuc.edu/par1.pdf">http://planning.cs.uiuc.edu/par1.pdf</a></li>
</ul>
</li>
</ul>
<h2 id="如何解决一个Motion-Planning问题？"><a href="#如何解决一个Motion-Planning问题？" class="headerlink" title="如何解决一个Motion Planning问题？"></a>如何解决一个Motion Planning问题？</h2><ul>
<li><p>找一个简单的突破口</p>
<ul>
<li><p>将问题简化成一个简单的问题：Path Finding Problem</p>
<p>与运动规划的区别：</p>
<ul>
<li>不关心速度，不关心走</li>
<li>周围固定</li>
</ul>
</li>
</ul>
</li>
<li><p>简言之就是：路径选择问题</p>
<ul>
<li>A simple shortest path example:</li>
</ul>
<p><a target="_blank" rel="noopener" href="http://qiao.github.io/PathFinding.js/visual/">http://qiao.github.io/PathFinding.js/visual/</a></p>
<ul>
<li><p>什么样的Path是最好的Path?这是重点</p>
<p>路径最短</p>
<ul>
<li>BFS &amp; DFS</li>
<li>Dijkstra</li>
</ul>
</li>
<li><p>刚刚看到的Search属于Non-informative search，效率比较低</p>
</li>
<li><p><script type="math/tex">A^{*}</script> search:基于Dijkstra的改进算法</p>
<ul>
<li>大概知道了终点位置</li>
<li>Heuristic func</li>
</ul>
</li>
</ul>
</li>
<li><p>无人车中的规划和<script type="math/tex">A^{*}</script>Search 相差多远？</p>
<ul>
<li>部分感知</li>
<li>动态障碍物</li>
<li><script type="math/tex">A^{*}</script>本身是一个Global Algorithm</li>
</ul>
</li>
<li><p>Partial observed situation</p>
<ul>
<li>贪心算法</li>
<li><script type="math/tex; mode=display">D^{*} star</script><ul>
<li>部分环境信息的一个Search</li>
<li>Apollo登月小车</li>
<li><script type="math/tex; mode=display">D^{*} Lite</script></li>
</ul>
</li>
<li>这样可以求解全局最优？<ul>
<li>有难度</li>
<li>一定有必要全局最优</li>
</ul>
</li>
</ul>
</li>
<li><p>Informative &amp; Non-informative search</p>
<ul>
<li>Global &amp; Partial observed</li>
</ul>
</li>
<li><p>至此，我们已经有了如下几个方法：</p>
<ul>
<li>目标函数并结合了平滑性和目标Cost</li>
<li>使用通用的Search方法来最小化Cost从而找到一个最优解</li>
<li>通过Partial observed information来做局部planning</li>
</ul>
</li>
<li><p>我们还缺什么？</p>
<ul>
<li>处理动态障碍物，动态环境<ul>
<li>静止环境</li>
</ul>
</li>
<li>处理交通规则<ul>
<li>公共安全</li>
</ul>
</li>
<li>实时计算<ul>
<li>100ms~150ms</li>
<li>人一般反应时间300~500ms</li>
<li>酒驾1000ms</li>
<li>有限时间内找到最优解</li>
<li>C++</li>
</ul>
</li>
</ul>
</li>
<li><p>给无人车motion planning下一个定义：</p>
<ul>
<li>Safely</li>
<li>Smoothly</li>
<li>Achieve to destination</li>
<li>X,Y,Time: 3D trajectory optimization problem</li>
<li>无人车硬件系统<ul>
<li>定位设备</li>
<li>感知设备</li>
</ul>
</li>
<li>无人车软件信息<ul>
<li>动态信息</li>
<li>静态信息<ul>
<li>HD map<ul>
<li>实时性保证</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>如何设计出一个合理的轨迹？<ul>
<li>路径Path</li>
<li>速度Speed</li>
</ul>
</li>
</ul>
</li>
<li><p>经典参考书籍：</p>
<ul>
<li>Steve Lavelle, Motion Planning Algorithms</li>
<li>Principles of Robot Motion:Theory,Algorithms,and Implementations</li>
</ul>
</li>
<li><p>经典文献</p>
<ul>
<li>A Review of Motion Planning Techniques for Automated Vehicles</li>
</ul>
</li>
</ul>
<h2 id="基本Planning方法"><a href="#基本Planning方法" class="headerlink" title="基本Planning方法"></a>基本Planning方法</h2><ul>
<li>经典基于环境建模的方法<ul>
<li>RRT</li>
<li>Lattice</li>
</ul>
</li>
<li>现代无人车Planning方法<ul>
<li>Darpa</li>
<li>Lattice in Frenet Frame</li>
<li>Spiral polynomial</li>
<li>A Review of Motion Planning Techniques for Automated Vehicles</li>
</ul>
</li>
<li>质点模型<ul>
<li>物体看成一个质点</li>
<li>点与点不碰撞</li>
</ul>
</li>
<li>刚体问题<ul>
<li>BycicleModel</li>
<li>XY Heading</li>
<li>Collision</li>
</ul>
</li>
<li>Planning限制条件<ul>
<li>避免碰撞</li>
<li>边界阈值</li>
</ul>
</li>
<li>连续空间问题怎么解？<ul>
<li>离散化</li>
<li>网格化</li>
</ul>
</li>
</ul>
<h2 id="传统的机器人基础"><a href="#传统的机器人基础" class="headerlink" title="传统的机器人基础"></a>传统的机器人基础</h2><ul>
<li><p>PRM(Probabilistic RoadMap Planning)</p>
<ul>
<li>非常常用的一个方法</li>
</ul>
<p><img src="/2022/06/21/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6/img1.png" alt></p>
<ul>
<li>连续空间离散化<ul>
<li>随机撒点</li>
<li>Obstacle上的点删除</li>
</ul>
</li>
<li>连接可行点，形成可行空间</li>
<li><script type="math/tex; mode=display">A^{*}</script></li>
</ul>
</li>
<li><p>RRT(Incremental version of PRM)</p>
<p>不能处理动态障碍物</p>
<ul>
<li><p>使用增量式的方式来进行</p>
<p><img src="/2022/06/21/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6/img2.png" alt></p>
<ul>
<li><p>找附近可行点的最优点</p>
<ul>
<li>F(x)最小，Cost最小</li>
<li>走过程中也不能有阻碍使Cost小</li>
</ul>
</li>
<li><p>走过程中，还可能碰到障碍物</p>
<p><img src="/2022/06/21/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6/img3.png" alt></p>
<p>上图撒点太远，点虽然可行，但是会穿过障碍物</p>
<ul>
<li><p>撒点搜索距离不能太远</p>
<ul>
<li><p>一步一步移动</p>
<p><img src="/2022/06/21/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6/img4.png" alt></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Lattice方法</p>
<ul>
<li><p>改进了RRT的折现问题</p>
</li>
<li><p>给出Path的平滑曲线</p>
</li>
<li><p>网格化</p>
<ul>
<li>每个采样格中都是用曲线连接</li>
</ul>
<p><img src="/2022/06/21/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6/img5.png" alt></p>
</li>
<li><p>指数级别的一个搜索算法(NP-Hard)</p>
</li>
</ul>
</li>
<li><p>DP(动态规划)</p>
<ul>
<li><p>减少搜索空间</p>
<ul>
<li>复用已有结果</li>
</ul>
<p><img src="/2022/06/21/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6/img6.png" alt></p>
</li>
<li><p>Lattice DP的平滑度够吗</p>
<ul>
<li>曲率连续</li>
<li>曲率的导数不一定连续</li>
</ul>
</li>
</ul>
</li>
<li><p>QP(二次规划)</p>
<ul>
<li>凸优化问题最优化求解</li>
<li>公式表达</li>
</ul>
<p><img src="/2022/06/21/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6/img7.png" alt></p>
<ul>
<li><p>性质：在凸优化中的凸空间问题，用QP有最优解</p>
</li>
<li><p>QP如何找到平滑曲线</p>
</li>
</ul>
<p><img src="/2022/06/21/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6/img8.png" alt></p>
<ul>
<li>其它的平滑曲线方法还有贝塞尔曲线、样条插值方法</li>
</ul>
</li>
<li><p>刚体模型</p>
<p><img src="/2022/06/21/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6/img9.png" alt></p>
<ul>
<li><p>前轮转向和Heading的关系</p>
<ul>
<li>车轮是沿着切线方向行驶</li>
<li>前后轮是同一个旋转中心</li>
<li>左右轮结构相同</li>
</ul>
</li>
<li><p>Bicycle Model</p>
</li>
</ul>
<p><img src="/2022/06/21/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6/img10.png" alt></p>
</li>
</ul>
<p>1.05</p>
<h1 id="Apollo"><a href="#Apollo" class="headerlink" title="Apollo"></a>Apollo</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/24/Deep-Reinforcement-Learning-for-Autonomous-Driving-A-Survey/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/24/Deep-Reinforcement-Learning-for-Autonomous-Driving-A-Survey/" class="post-title-link" itemprop="url">Deep Reinforcement Learning for Autonomous Driving: A Survey</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-24 22:50:21" itemprop="dateCreated datePublished" datetime="2022-05-24T22:50:21+08:00">2022-05-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-13 23:12:02" itemprop="dateModified" datetime="2022-06-13T23:12:02+08:00">2022-06-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    随着深度表示学习的发展，强化学习(RL)领域已经成为一个强大的学习框架，现在能够在高维环境中学习复杂的策略。这篇综述总结了深度强化学习(DRL)算法，并提供了自动驾驶任务的分类，其中(D)RL方法，同时解决了自动驾驶代理部署中的关键计算挑战。它还描述了相邻的领域，如行为克隆，模仿学习，反强化学习，这些领域是相关的，但不是经典的RL算法。讨论了模拟器在训练代理中的作用，验证、测试和解决RL中现有解决方案的方法。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    自动驾驶(AD)系统由多重感知水平任务组成，由于深度学习架构，现在已经实现了很高的精度。除了感知之外，自动驾驶系统还由多个任务组成，其中经典的监督学习方法已不再适用。首先，当对代理动作的预测改变从自动驾驶代理运行的环境中接收到的未来传感器观察结果时，例如在城市区域的最佳驾驶速度任务。其次，监督信号(如碰撞时间(TTC)、代理最优轨迹的横向误差w.r.t，代表了主体的动态，以及环境中的不确定性。这样的问题需要定义随机成本函数才能被最大化。第三，代理需要学习环境的新配置，并在在其环境中驾驶时的每一时刻预测最佳决策。这代表了一个高维空间，给定了所观察到的代理和环境的唯一构型的数量，这是组合上较大的。在所有这些场景中，我们的目标都是解决一个顺序决策过程，这是在强化学习(RL)的经典设置下形式化的，其中代理需要学习和表示其环境，并在每个时刻上给出最优的行动。最优行动被称为策略。</p>
<p>​    在这篇综述中，我们涵盖了强化学习的概念，任务分类，RL是一个有前途的解决方案，特别是在驾驶策略、预测感知、路径和运动规划以及低级控制器设计等领域。我们还重点回顾了RL在自动驾驶领域的不同现实世界部署，扩展了我们的会议论文[2]，因为它们的部署还没有在学术环境中进行审查。最后，我们通过展示在应用当前RL算法如模仿学习、深度Q学习等时的关键计算挑战和风险来激励用户。我们还从图2中的出版物趋势中注意到，使用RL或深度RL应用于自动驾驶或自动驾驶领域是一个新兴领域。这是由于最近使用了RL/DRL算法域，在实现和部署中留下了多个现实世界的挑战。我没在第四节解决了开放的问题。</p>
<p>​    这项工作的主要贡献可以总结如下：</p>
<ul>
<li>独立的RL背景的汽车社区概述，因为它不是广为人知的</li>
<li>RL在不同自动驾驶任务中使用的详细文献综述。</li>
<li>讨论了RL应用于现实世界自动驾驶的关键挑战和机遇。</li>
</ul>
<p>本文的其余部分组织如下。第二节概述了一个典型的自动驾驶系统的组成部分。第三节介绍了强化学习，并简要讨论了关键概念。第四节讨论了在基本RL框架之上的更复杂的扩展。第五节概述了自动驾驶问题的RL应用。第六节讨论了在现实世界的自动驾驶系统中部署RL所面临的挑战。第七节对本文作了最后的总结。</p>
<h1 id="自动驾驶系统组成部分"><a href="#自动驾驶系统组成部分" class="headerlink" title="自动驾驶系统组成部分"></a>自动驾驶系统组成部分</h1><p>​    图1包括一个AD系统的标准块，展示了从传感器流到控制驱动的管道。现代自动驾驶系统中的传感器架构主要包括多套摄像头，雷达和激光雷达，以及用于绝对定位和惯性测量单元(IMUs)的GPS-GNSS系统，提供飞行器在空间中的三维姿态。</p>
<p><img src="/2022/05/24/Deep-Reinforcement-Learning-for-Autonomous-Driving-A-Survey/figure1.png" alt></p>
<p>​    感知模块的目标是创建一个环境状态的中间水平表示（例如所有障碍和代理的鸟瞰视图），稍后将被最终产生驾驶政策的决策系统使用。这种状态将包括车道的位置、可驾驶的区域、代理的位置，如汽车和行人、交通灯的状态等。感知中的不确定性会传播到信息链的其他部分。鲁棒感知对于安全性至关重要，因此使用冗余源增加了检测的可信度。这是通过语义分割[3]、[4]、运动估计[5]、深度估计[6]、污染检测[7]等感知任务的组合来实现的，这些任务可以有效地统一成多任务模型[8]、[9]。</p>
<h2 id="Scene-Understanding"><a href="#Scene-Understanding" class="headerlink" title="Scene Understanding"></a>Scene Understanding</h2><p>​    这个关键模块将从感知模块中获得的感知状态的抽象中层表示映射到高级行动或决策模块中。从概念上讲，这个模块分为三个任务：场景理解、决策和规划，如图1所示，模块旨在提供对场景的更高层次的理解，它建立在检测或本地化的算法任务之上。通过融合异构传感器源，它的目的是稳健地推广到随着内容变得更加抽象的情况。这种信息融合为决策组件提供了一个通用的和简化的上下文。</p>
<p>​    融合提供了一个传感器不可知的环境表示，并模型的传感器噪声和检测不确定性跨多种模式，如激光雷达、照相机、雷达、超声波。这基本上需要以一种有原则的方式来加权预测。</p>
<h2 id="Localization-and-Mapping"><a href="#Localization-and-Mapping" class="headerlink" title="Localization and Mapping"></a>Localization and Mapping</h2><p>​    地图是自动驾驶[10]的关键支柱之一。一旦一个区域被映射出来，就可以在地图内定位车辆的当前位置。谷歌对自动驾驶的第一个可靠演示主要依赖于对预先映射的区域的本地化。由于问题的规模，传统的映射技术通过语义对象检测来增强，以可靠地消除歧义。此外，本地化的高清晰度地图(HD地图)可以作为目标检测的先验。</p>
<h2 id="Planning-and-Driving-policy"><a href="#Planning-and-Driving-policy" class="headerlink" title="Planning and Driving policy"></a>Planning and Driving policy</h2><p>​    轨迹规划是自动驾驶管道中的一个关键模块。给定一个来自高清地图或基于GPS的地图的路线级计划，该模块需要生成引导代理的运动级命令。</p>
<p>​    经典的运动规划忽略了动力学和微分约束，而使用平移和旋转来移动代理从源到目的地的姿态[11]。一种能够控制6个自由度(DOF)的机器人代理被称为是完整的，而一种可控自由度低于其总DOF的代理被称为是非完整的。经典的算法，如基于Djisktra算法的<script type="math/tex">A^{*}</script>算法，在自动驾驶的非完整情况下不起作用。快速探索随机树(RRT)[12]是一种通过随机采样和无障碍路径生成来探索配置空间的非完整算法。目前有各种版本的RRT用于自动驾驶管道中的运动规划。</p>
<h2 id="Control"><a href="#Control" class="headerlink" title="Control"></a>Control</h2><p>​    控制器定义了在路径上的每个点上的必要的速度、转向角度和制动动作，从一个预先确定的地图，如谷歌地图，或在每个路径点的相同值的专家驾驶记录中获得。相比之下，轨迹跟踪涉及一个时间模型的车辆动态查看路径点的时间。</p>
<p>​    目前的车辆控制方法是建立在经典的最优控制理论上的，该理论可以表示为在一组状态x(t)和控制动作u(t)上定义的代价函数x˙=f(x(t)，u(t))的最小化。通常定义控制输入在有限的时间范围内，限制在可行状态空间x∈Xfree[14]。速度控制是基于经典的闭环控制方法，如PID（比例积分导数）控制器，MPC（模型预测控制）。pid的目标是最小化一个成本函数，包括三个具有比例项的项当前误差，具有积分项的过去误差的影响，以及具有导数项的未来误差的影响。而MPC方法家族的目的是稳定车辆的行为，同时跟踪指定的路径[15]。本文为感兴趣的读者提供了关于控制器、运动规划和学习方法的综述。最优控制和强化学习密切相关，其中最优控制可以被视为一个基于模型的强化学习问题，其中车辆/环境的动力学是由定义良好的微分方程建模的。强化学习方法被开发来处理随机控制问题以及具有未知奖励和状态转移概率的不适定问题。自动驾驶汽车的随机控制值为d值较大。</p>
<h1 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h1><p>​    机器学习(ML)是计算机程序从经验中学习以提高其在指定任务[18]中的性能的一个过程。ML算法通常被分为三大类：有监督学习、无监督学习和强化学习(RL)。有监督学习算法基于归纳推理，其中模型通常使用标记数据进行训练，以执行分类或回归，而无监督学习包含了应用于未标记数据的密度估计或聚类等技术。相比之下，在RL范式中，自主代理学习通过与环境交互来提高其在分配任务中的性能。Russel和Norvig将代理定义为“任何可以通过传感器感知环境并通过执行器对环境起作用的东西”，[19]。RL代理没有被专家明确地告诉他们如何行动；相反，代理的表现是通过奖励函数R来评估的。对于每一个经历过的状态，代理会选择一个动作，并根据其决策的有用性从其环境中获得奖励。代理的目标是最大化其生命周期中获得的累积奖励。逐渐地，代理可以通过利用有关不同状态-动作对的预期效用（即预期未来奖励的贴现总和）的知识来增加其长期奖励。强化学习的主要挑战之一是管理探索和开发之间的权衡。为了最大化它所获得的奖励，一个代理必须利用它的知识，选择已知会导致高奖励的行动。另一方面，要发现这些有益的行动，它必须承担尝试新行动的风险，这些行动可能会导致比当前每个系统状态最有价值的行动更高的回报。换句话说，学习主体必须利用它已经知道的东西来获得奖励，但它也必须探索未知的事物，以便在未来做出更好的行动选择。已经提出的管理这种权衡的策略的例子包括<script type="math/tex">\varepsilon-greedy</script>和softmax。当采用<script type="math/tex">\varepsilon-greedy</script>策略时，智能体会有<script type="math/tex">\varepsilon</script>的概率进行随机选择，<script type="math/tex">1-\varepsilon</script>的概率选择最优动作。直观地说，当我们对问题环境知之甚少时，代理应该在培训过程的开始时进行更多的探索。随着培训的进行，代理可能会逐渐进行更多的开发而不是探索。RL代理的探索策略的设计是一个积极的研究领域。</p>
<p>​    当形式化涉及单个RL代理[21]的顺序决策问题时，马尔可夫决策过程(MDPs)被认为是事实上的标准。一个MDP由一组状态S、一组动作a、一个过渡函数T和一个奖励函数R[22]组成，即一个元组。当处于任何状态s∈s时，选择一个动作a∈a将导致环境进入一个新的状态s0∈s，其转移概率为T(s，a，s0)∈（0,1），并给出一个奖励R(s，a)。这个过程如图3所示。随机策略π：S→D是从状态空间到动作集合上的概率的映射，π(a|)表示在状态s处选择动作a的概率。目标是找到最优策略π∗，从而得到折扣奖励的最高期望和[21]：</p>
<p>​    <img src="/2022/05/24/Deep-Reinforcement-Learning-for-Autonomous-Driving-A-Survey/figure3.png" alt></p>
<h1 id="在自动驾驶任务中的强化学习"><a href="#在自动驾驶任务中的强化学习" class="headerlink" title="在自动驾驶任务中的强化学习"></a>在自动驾驶任务中的强化学习</h1><p>​    可以应用RL的自动驾驶任务包括：控制器优化、路径规划和轨迹优化、运动规划和动态路径规划、复杂导航任务开发高级驾驶策略、基于场景的策略学习公路、交叉口、合并和分割、奖励学习与逆强化学习从专家数据意图预测行人等交通参与者、车辆，最后学习政策，确保安全和执行风险估计。在讨论DRL在AD任务中的应用之前，我们简要回顾了自动驾驶设置中的状态空间、动作空间和奖励方案。</p>
<h2 id="State-Spaces-Action-Spaces-and-Rewards"><a href="#State-Spaces-Action-Spaces-and-Rewards" class="headerlink" title="State Spaces, Action Spaces and Rewards"></a><em>State Spaces, Action Spaces and Rewards</em></h2><p>​    为了成功地将DRL应用于自动驾驶任务，设计适当的状态空间、动作空间和奖励函数是重要的。Leurent等人[75]对自动驾驶研究中使用的不同状态和动作表征进行了全面的综述。自动驾驶车辆常用的状态空间特征包括：自我车辆的位置、航向和速度，以及自我车辆的传感器视图范围内的其他障碍。为了避免状态空间维数的变化，经常使用自我载体周围的笛卡尔或极地占用网格。这进一步增强了车道信息，如车道数（自我车道或其他车道）、路径曲率、自我车辆的过去和未来轨迹、纵向信息，如碰撞时间(TTC)，最后是场景信息，如交通规律和信号位置。</p>
<p>​    使用原始的传感器数据，如相机图像、激光雷达、雷达等。提供了更精细的上下文信息的好处，同时使用压缩的抽象数据降低了状态空间的复杂性。在这两者之间，一个中层表示，如二维鸟瞰视图(BEV)是传感器不可知的，但仍然接近于场景的空间组织。图4是一个自上而下的视图，显示了占用网格，过去和投影的轨迹，以及关于场景的语义信息，如交通信号灯的位置。当基于图形的表示不能保留道路时，这种中间格式保留了道路的空间布局。一些模拟器提供了这个视图，如Carla或Flow(见表V-C)。</p>
<p><img src="/2022/05/24/Deep-Reinforcement-Learning-for-Autonomous-Driving-A-Survey/figure4.png" alt></p>
<p>​    车辆策略必须控制许多不同的执行器。用于车辆控制的连续值执行器包括转向角度、油门和刹车。其他的执行机构，如齿轮的变化，都是独立的。为了降低复杂性并允许应用仅使用离散动作空间(例如DQN)的DRL算法，可以通过将连续执行器如转向角、节气门和制动器的范围划分为相同大小的箱来均匀离散动作空间(见第VI-C节)。在对数空间中也被建议进行离散化，因为在实践中选择的许多转向角度都是接近中心[76]的。然而，离散权确实有缺点；如果动作之间的步长值太大，就会导致抖动或不稳定的轨迹。此外，当为执行器选择箱子的数量时，需要有足够的离散步骤来允许平稳控制，而没有太多的步骤来评估动作选择变得非常昂贵。作为离散化的一种替代方法，执行器的连续值也可以通过直接学习策略的DRL算法来处理(例如，DDPG)。时间抽象选项框架[77])也可以用于简化选择操作的过程，其中代理选择选项而不是低级操作。这些选项表示可以在多个时间步长中扩展原始操作的子策略。</p>
<p>​    为自动驾驶的DRL代理设计奖励功能仍然是一个有待解决的问题。自动驾驶任务的标准任务的例子包括：距离前往目的地[78]，自我车辆的速度，保持自我车辆静止[81]，碰撞与其他道路用户或场景对象[78]，[79]，违反人行道[78]，保持车道，保持舒适和稳定，同时避免极端加速，刹车或转向[80]，[81]，并遵循交通规则</p>
<h2 id="Motion-Planning-amp-Trajectory-optimization"><a href="#Motion-Planning-amp-Trajectory-optimization" class="headerlink" title="Motion Planning &amp; Trajectory optimization"></a><em>Motion Planning &amp; Trajectory optimization</em></h2><p>​    运动规划是确保在目标点和目的地之间存在一条路径的任务。这对于在之前的地图上规划车辆的轨迹是必要的，这通常增加了语义信息。动态环境和变化车辆动态下的路径规划是自动驾驶的一个关键问题，例如在[87]路口通过的权利，合并成高速公路。作者[89]最近的工作包含了不同的交通参与者在真实世界中的运动，在不同的交互式驾驶场景中观察到。作者[89]最近的工作包含了不同的交通参与者在真实世界中的运动，在不同的交互式驾驶场景中观察到。最近，作者展示了DRL(DDPG)用于AD的应用。该系统首先接受了模拟训练，然后在车载计算机上进行实时训练，并能够学会跟随一条车道，成功地完成了在一段250米长的道路上的真实世界试验。基于模型的深度RL算法已经被提出，用于直接从原始像素输入[91]，[92]中学习模型和策略。在[93]中，深度神经网络已被用于在模拟环境中生成超过数百个时间步长的预测。RL也适用于控制系统。将LQR/iLQR等经典的最优控制方法与[94]中的RL方法进行了比较。经典的RL方法用于随机环境下的最优控制，例如线性环境中的线性二次调节器(LQR)和非线性环境中的迭代LQR(iLQR)。[95]最近的一项研究表明，对策略网络的参数进行随机搜索可以和LQR一样好。</p>
<h2 id="Simulator-amp-Scenario-generation-tools"><a href="#Simulator-amp-Scenario-generation-tools" class="headerlink" title="Simulator &amp; Scenario generation tools"></a><em>Simulator &amp; Scenario generation tools</em></h2><p>​    自动驾驶数据集解决监督学习设置与训练集包含图像，标签对的各种模式。强化学习需要一个环境，在那里状态-行动对可以恢复，同时分别建模车辆状态、环境以及环境和代理的运动和行动的随机性。各种模拟器被积极地用于训练和验证强化学习算法。表V-C总结了各种能够模拟摄像机、激光雷达和雷达的高保真感知模拟器。一些模拟器还能够提供车辆的状态和动力学。[105]提供读者对自动驾驶社区中使用的传感器和模拟器的完整回顾。学习过的驾驶政策先在模拟环境中进行压力测试，然后在现实世界中进行昂贵的评估。在[106]中提出了多保真度强化学习(MFRL)框架，其中有多个模拟器可用。在MFRL中，使用具有增加保真度的级联模拟器来表示状态动态（以及计算成本），使RL算法的训练和验证成为可能，同时使用遥控汽车为更少昂贵的真实世界样本找到接近真实世界的最优策略。卡拉挑战[107]是一个基于卡拉模拟器的AD比赛，与碰撞前的场景特征在一个国家高速公路交通安全管理局报告了[108]。这些系统是在关键的场景下进行评估的，如：自我车辆失去控制，自我车辆对看不见的障碍做出反应，变道以躲避慢速引导车辆等。代理的得分作为在不同电路中移动的总距离的函数来评估，以及由于违规而导致的总点贴现。</p>
<h2 id="LfD-and-IRL-for-AD-applications"><a href="#LfD-and-IRL-for-AD-applications" class="headerlink" title="LfD and IRL for AD applications"></a><em>LfD and IRL for AD applications</em></h2><p>​    在[109]中驾驶汽车的行为克隆(BC)的早期工作，[110]提出了学习形式演示(LfD)的代理，试图模仿专家的行为。BC通常是作为一种监督学习来实现的，因此，BC很难适应新的、看不见的情况。在[111]，[112]中提出了一种在自动驾驶汽车领域中端到端学习卷积神经网络的体系结构。CNN被训练成将原始像素从单个前置摄像头直接映射到转向命令中。使用来自人类/专家的相对较小的训练数据集，该系统学习在有或没有车道标记的当地道路上和高速公路上驾驶车辆。该网络学习成功检测道路的图像表示，而没有经过明确的训练。[113]的作者提出了使用最大熵逆RL从人类驾驶员那里获得的专家演示来学习舒适的驾驶轨迹优化。[114]的作者使用DQN作为IRL中的细化步骤来提取奖励，以努力学习类人的变道行为。</p>
<p><img src="/2022/05/24/Deep-Reinforcement-Learning-for-Autonomous-Driving-A-Survey/table1.png" alt></p>
<h2 id="Intrinsic-Reward-functions"><a href="#Intrinsic-Reward-functions" class="headerlink" title="Intrinsic Reward functions"></a><em>Intrinsic Reward functions</em></h2><p>​    在游戏等受控的模拟环境中，显式的奖励信号及其传感器流给代理。然而，在现实世界的机器人技术和自动驾驶衍生技术中，设计一个好的奖励功能是必要的，以便能够学习到所期望的行为。最常见的解决方案是奖励塑造[136]，它包括向代理提供额外的设计良好的奖励，以鼓励优化到最优策略的方向。正如本文前面已经指出的那样，奖励可以通过逆RL(IRL)[137]来估计，这依赖于专家的演示。在缺乏明确的奖励塑造和专家演示的情况下，代理可以使用内在奖励或内在动机[138]来评估他们的行为是否好。[139]的作者将好奇心定义为一个主体在通过自我监督的逆动力学模型学习的视觉特征空间中预测自身行为结果的能力上的错误。在[140]中，代理从其经验中学习下一个状态预测器模型，并使用预测的错误作为内在奖励。这使该代理能够确定什么可能是一个用途。</p>
<h2 id="Incorporating-safety-in-DRL"><a href="#Incorporating-safety-in-DRL" class="headerlink" title="Incorporating safety in DRL*"></a>Incorporating safety in DRL*</h2><p>​    经过训练后在真实环境中部署自动驾驶车辆可能是危险的。本文介绍了将安全性纳入DRL算法的不同方法。对于基于模仿学习的系统，SafeDAgger[141]引入了一种安全策略，该策略学习预测最初使用监督学习方法训练的主策略所犯的错误，而不查询参考策略。另一个安全策略将状态和主策略的部分观察作为输入，并返回一个二进制标签，指示主策略是否可能在不查询引用策略的情况下偏离引用策略。[142]的作者讨论了自动驾驶的多智能体强化学习的安全性，即在其他司机或行人的意外行为之间保持平衡，不要过于防御，从而实现正常的交通流量。虽然保持严格的约束以保证驾驶的安全，但问题被分解为一个政策的组成，以实现舒适驾驶和轨迹规划。深度强化学习算法的控制，如DDPG和safet。利用TORCS环境，首先将DDPG应用于在稳定、熟悉的环境下学习驾驶策略，然后将策略网络和基于安全的控制结合起来，避免碰撞。结果发现，DRL和安全控制的结合在大多数情况下都表现良好。为了使DRL逃避本地最优，加快训练过程，避免危险条件或事故，面向生存的强化学习(SORL)模型在[144]提出，生存有利于最大化总奖励通过建模自动驾驶问题约束MDP和引入负回避函数从以前的失败。SORL模型对奖励功能不敏感，可以使用不同的DRL算法，如DDPG。此外，在[145]中可以为感兴趣的读者找到一个关于安全强化学习的全面调查。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>​    强化学习在现实世界的自动驾驶应用中仍然是一个活跃的新兴领域。虽然有一些成功的商业应用，但很少有文献或大规模的公共数据集可用。因此，我们有动力形式化和组织自动驾驶的RL应用程序。自动驾驶场景涉及交互代理，需要适合RL的协商和动态决策。然而，为了有成熟的解决方案，我们有许多挑战需要解决。在这项工作中，提出了一个详细的理论强化学习，以及一个全面的文献调查，关于应用RL在自动驾驶任务。</p>
<p>​    第六节将讨论挑战和机遇、未来的研究方向和机遇。这包括：验证基于RL的系统的性能、模拟-现实差距、样本效率、设计良好的奖励功能、将安全性纳入自主代理的RL系统的决策中。</p>
<p>​    强化学习结果通常很难重现，并且对超参数选择高度敏感，这些选择往往没有详细报告。研究人员和从业者都需要有一个可靠的起点，在那里众所周知的强化学习算法被实现、记录和良好的测试。在表VI-G中已经涵盖了这些框架。</p>
<p>​    发展针对自动驾驶问题的明确多智能体强化学习方法也是一个重要的未来挑战，迄今尚未得到很多的关注。MARL技术有潜力使自动驾驶汽车组之间的协调和高层决策更容易，并为测试和验证自动驾驶政策的安全性提供新的机会。</p>
<p>​    此外，RL算法的实现对研究者和从业者来说都是一项具有挑战性的任务。这项工作提供了众所周知的和活跃的开源RL框架的例子，这些框架提供了良好的文档实现，使您有机会使用、评估和扩展不同的RL算法。最后，我们希望这篇概述论文能鼓励进一步的研究和应用。</p>
<h1 id="PPT"><a href="#PPT" class="headerlink" title="PPT"></a>PPT</h1><ul>
<li>先给出目前强化学习的成功案例</li>
</ul>
<p>比如alphago、星际争霸等</p>
<ul>
<li>讲强化学习的基本要素</li>
<li></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/" class="post-title-link" itemprop="url">Lane Change Decision-Making through Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-19 21:02:05" itemprop="dateCreated datePublished" datetime="2022-05-19T21:02:05+08:00">2022-05-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-27 23:58:20" itemprop="dateModified" datetime="2022-05-27T23:58:20+08:00">2022-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/" itemprop="url" rel="index"><span itemprop="name">自动驾驶</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本文是使用深度强化学习算法DQN结合规则在水平变道上进行决策，最终安全率达到80%。</p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    由于交通环境的复杂性和波动性，自动驾驶的决策是一个显著的难题。在这个项目中，我们使用了一个深度q网络，以及基于规则的约束来做出变道决策。通过将高水平的横向决策与基于低级规则的轨迹监测相结合，可以获得安全有效的变道行为。在总共100集的训练后，代理预计将在一个真实世界的实用模拟器中执行适当的变道动作。结果表明基于规则的DQN方法的性能优于DQN方法。基于规则的DQN达到了0.8的安全率，平均速度为47英里/小时。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>​    自动驾驶汽车近年来越来越受欢迎，同时也已成功地部署在现实场景中。例如Waymo就推出了自动驾驶拼车服务，这是第一个向公众提供完全自动驾驶体验的服务。虽然像特斯拉和comma.ai这样的公司提供了一定程度的自主驾驶，但他们目前还不能将全自动汽车推向市场。组成自动驾驶系统的有几个。首先，这些模块可以分为四个不同的模块：感知模块、决策模块、控制模块和执行机构模块。在这些模块中，这些工作涉及到决策模块。在各种条件下做出决策是一项难以概括的任务。因此，决策者有必要对各种条件具有强大的应对能力，并能从其经验中进行归纳或学习。因此，强化学习(RL)非常方便，因为近年来在RL领域取得了一些突破。</p>
<p>​    RL最突出的是深度RL模型已经在Go和Poker等游戏中取得了超人的表现。像这样的进展已经打开了一个广泛的潜在领域，RL可以用来实现超人的性能。自动驾驶也有几个挑战，目前还没有一个确切的解决方案。例如Waymo 经历了 10 次（总共 16 次模拟中）“同向侧滑”，其中涉及换道或车道合并操作期间的碰撞。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>​    解决自动驾驶问题的方法可分为基于规则算法和基于学习算法两类。虽然基于规则的技术在过去取得了一些成功，但基于学习的方法近年来也显示出了它们的有效性。</p>
<p>​    许多传统的解决方案都是基于显式编写的规则，并依赖于状态机在指定的决策行为之间进行转换。例如，由<a href="Alvinn: An autonomous land vehicle in a neuralnetwork[">CMU</a>训练的“老板”讨论了一种基于规则的方法，斯坦福<a href="The stanford entry in the urban challenge">The stanford entry in the urban challenge</a>的研究团队也使用奖励设计来确定轨迹。然而，可靠的决策在基于规则的方法中具有很高的复杂性。</p>
<p>​    基于学习的技术作为一项关键的人工智能技术，可以为自动驾驶提供更先进和更安全的决策算法。最近，NVIDIA <a href="End to end learning for self-driving cars[">End to end learning for self-driving cars</a>的研究人员训练了一个深度卷积神经网络(CNN)来直接训练图像从摄像机到聚类控制。这个训练过的模型能够处理变道和在碎石道路上驾驶的任务。</p>
<p>​    除了监督学习之外，强化学习的结果在多年来也有了显著的改善。Wolf等人<a href="Learning how to drive in a real world simulation with deep q-networks">Learning how to drive in a real world simulation with deep q-networks</a>提出了一种利用DQN在受刺激环境下开车的方法。只有定义了5个动作，每个动作对应于不同的转向角度以及大小为<script type="math/tex">48 \times 27</script>的训练图像。奖励函数的计算方法是考虑了距离车道中心的距离以及某些辅助信息（如车辆与中心线之间的角度误差）。Hoel等人<a href="Automated speed and lane changedecision making using deep reinforcement learning[">Automated speed and lane change decision making using deep reinforcement learning</a>使用DQN解决变道随车速的问题。他们使用一维向量来定义许多组件，包括速度、周围车辆和相邻车道，而不是使用正面图像。Wang等<a href="Lane change decision making through deep reinforcement learning with rule-based constraints">Lane change decision making through deep reinforcement learning with rule-based constraints</a>使用安全率来测量模型的质量，该模型基于碰撞频率计算，对促进提供了清晰的思路。结果表明，考虑速度和其他因素的模型通常优于其他不考虑车辆平均速度的模型。</p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>​    本报告概述了深度 Q 学习在具有基于规则的约束的车道变更决策问题中的实施情况。本报告的结构如下。第二节讨论了与RL算法相关的方法以及基于规则的约束公式。其实现和仿真结果在第三节中进行了讨论。结果在第四节中进行了讨论。最后，在第五节中进行了总结。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="State-Space"><a href="#State-Space" class="headerlink" title="State Space"></a>State Space</h2><p>​    模拟器提供了关于自己和其他车辆的位置和速度的信息。它包括汽车在地图坐标中的x，y位置和在frenet坐标中的s，d位置，自动驾驶汽车在地图中的偏移角度和MPH速度（英里每小时），以及其他汽车在m/s计算中的x，y速度。使用45×3矩阵表示状态空间，对应于车辆前方60米和后方30米范围内的整个交通情况，如图1所示。每辆车长约5.5米，矩阵中的每一行跨度为2米，一辆车在极端情况下可以占据4个单元。因此，我们用汽车的标准化速度填充与单个汽车对应的4个单元格。速度与框架内车辆的最大速度和框架内车辆的最小速度相标准化。在这里，自己汽车的标准化速度是正的，而其他车是负的。与没有任何汽车的位置相对应的单元格中充满了1s。</p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/figure1.png" alt></p>
<h2 id="Action-Space"><a href="#Action-Space" class="headerlink" title="Action Space"></a>Action Space</h2><p>​    自动驾驶车辆具有去左车道、在当前车道保持、去右车道三个动作，如table1所示。其它的所有控制操作由低级控制器完成。</p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/table1.png" alt></p>
<h2 id="Reward"><a href="#Reward" class="headerlink" title="Reward"></a>Reward</h2><p>​    本项目中的自动驾驶汽车只允许在高速公路的靠右3车道上行驶。在非法改变车道的情况下，即当汽车在最左车道时选择动作2（去左车道）或者在最右车道选择动作1（去右车道）时，汽车被迫保持在同一车道，得到的奖励为<script type="math/tex">r_{ch1} = -5</script> 。当碰撞发生时奖励为<script type="math/tex">r_{co} = -10</script>，用于避免碰撞。当车辆进行无效变道时，比如车辆前面没有车时进行变道，得到的奖励为<script type="math/tex">r_{ch2} = -3</script>。</p>
<p>​    只有在需要的时候才会换车道，自动驾驶汽车也应该尽可能快地行驶（在速度限制范围内）。为了强制执行这一点，将分配以下奖励,如公式<script type="math/tex">2</script>所示。其中<script type="math/tex">v</script>表示自动驾驶汽车的平均速度，<script type="math/tex">v_{ref}</script>表示参考速度（25英里/小时），<script type="math/tex">\lambda</script>表示归一化系数，值为<script type="math/tex">0.04</script>。每次决策的奖励如公式<script type="math/tex">3</script>所示。</p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/formula2.png" alt></p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/formula3.png" alt></p>
<h2 id="Rule-Based-Constraints"><a href="#Rule-Based-Constraints" class="headerlink" title="Rule-Based Constraints"></a>Rule-Based Constraints</h2><p>​    我们基于规划轨迹和其他预期轨迹，应用基于规则的限制，以确保变道行为的绝对安全性。低级控制器可以根据高层决策者所确定的动作来预测自动驾驶车辆和期望车道中相邻车辆的轨迹。</p>
<p>​    假设相邻汽车保持当前速度并停留在当前车道上，计算相邻汽车的轨迹。 如果自动驾驶汽车的预期轨迹与周围汽车的预期轨迹之间的距离小于指定的阈值，则高层决策者做出的选择可能是有害的。 这个动作被低级控制器拒绝，汽车将继续其当前轨迹。</p>
<h2 id="Deep-Q-Network"><a href="#Deep-Q-Network" class="headerlink" title="Deep Q Network"></a>Deep Q Network</h2><p>​    DQN用于确定给定状态下的最优动作。Q-Learing是一种off-policy TD强化学习算法。它使用state-action值函数<script type="math/tex">Q^{\pi }\left ( s,a \right )</script>来评估策略<script type="math/tex">\pi</script>，该函数定义为：</p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/formula4.png" alt></p>
<p>​    Q-learning算法试图找到最优的状态动作值函数：</p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/formula5.png" alt></p>
<p>​    <script type="math/tex">Q^{*}</script>对应于最优策略<script type="math/tex">\pi^{*}</script>。</p>
<p>​    这个值函数<script type="math/tex">Q^{*}</script>遵循贝尔曼最优方程：</p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/formula6.png" alt></p>
<p>​    使用最优值函数<script type="math/tex">Q^{*}</script>，可以通过寻找在每个状态下使值函数最大化的动作来确定最优策略。</p>
<p>​    对于一个离散的和有限的问题，找到一个Q函数是足够简单的。然而对于像我们这样具有高维和连续状态空间的问题，这种方法变得计算成本昂贵和难以处理。一个深度q网络用于近似值函数，如图2所示。以下损失函数使用Adam优化器来最小化：</p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/formula7.png" alt></p>
<p>​    <img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/figure2.png" alt></p>
<p>​    DQN的输入是状态矩阵和一个3D向量，该向量在卷积层之后连接到网络上。向量的第一个元素对应于RL智能体的归一化速度，第二个和第三个元素分别对应于是否有左右两个车道，如果有则为1，没有为0。图2中的input1对应状态矩阵，input2对应3D向量。</p>
<h1 id="Implementation-And-Simmulation"><a href="#Implementation-And-Simmulation" class="headerlink" title="Implementation And Simmulation"></a>Implementation And Simmulation</h1><p><a target="_blank" rel="noopener" href="https://github.com/DRL-CASIA/Autonomous-Driving">项目源码</a></p>
<h2 id="开发环境"><a href="#开发环境" class="headerlink" title="开发环境"></a>开发环境</h2><ul>
<li>CMake 3.22.0 </li>
<li>Python-3.6 </li>
<li>ubuntu</li>
<li>Tensorflflow-GPU </li>
<li>keras </li>
</ul>
<h2 id="模拟器"><a href="#模拟器" class="headerlink" title="模拟器"></a>模拟器</h2><p>​    本项目中使用的模拟器由Udacity开发。仿真设置为一条3车道的高速公路，如图3所示。带有绿色轨迹标记的车辆是RL代理。该模拟器提供了框架中所有汽车的定位和速度。速度限制是50英里/小时，而RL代理的目标是保持速度限制。该模拟器还提供了与汽车在加速度（法向加速度、切向加速度和合成加速度）和颠簸方面的性能相关的反馈。对汽车的要求是它不应该加速超过<script type="math/tex">10m/s^{2}</script>和颠簸大于<script type="math/tex">10m/s^{3}</script>。该车道的总距离为6946米。</p>
<p><strong>法向加速度如下图中的<script type="math/tex">a_{n}</script>, 切向加速度如下图中的<script type="math/tex">a_{t}</script>，合成加速度为<script type="math/tex">a</script>,</strong><a target="_blank" rel="noopener" href="https://sbainvent.com/dynamics/kinematics-of-a-particle/normal-and-tangential-velocity-and-accelerations/">参考链接</a></p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/figure11.png" alt></p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/figure3.png" alt></p>
<h2 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h2><p>​    使用DQN进行了100局的训练。网络的输入由对交通状况进行编码的状态矩阵组成。 网络的输出是每个状态的3个动作值（或 Q 值）。 选择该状态下对应的最大动作值的动作。 在训练期间，代理随机或遵循贪婪策略选择动作。 这种被称为探索-开发权衡的现象是通过实施贪婪策略来处理的。网络的输出是每个动作的三个状态动作值（或Q值）。选择与最大状态操作值对应的操作。在训练过程中，代理可以随机地或遵循贪婪策略来选择动作。这种现象被称为勘探-利用权衡，是通过实现<script type="math/tex">\epsilon -greddy</script>策略来处理的。对于<script type="math/tex">\epsilon -greddy</script>策略，代理随机选择的概率为<script type="math/tex">\epsilon</script>，和选择Q值最大动作的概率为<script type="math/tex">1-\epsilon</script>。训练时会对<script type="math/tex">\epsilon</script>进行衰减，<script type="math/tex">\epsilon</script>初始值为1，衰减系数<script type="math/tex">\lambda_{decay}=0.99985</script>,最小衰减到的值为<script type="math/tex">\epsilon_{min} = 0.03</script>。训练步骤：</p>
<ul>
<li>下载<a target="_blank" rel="noopener" href="https://github.com/DRL-CASIA/Autonomous-Driving">源码</a></li>
<li>检查代码中的端口和主机，并确保该端口是空闲的</li>
<li>运行train.py文件，然后打开模拟器</li>
<li>选择模拟窗口和图形质量，并点击开始</li>
</ul>
<h2 id="模拟环境"><a href="#模拟环境" class="headerlink" title="模拟环境"></a>模拟环境</h2><p>​    我们的仿真实验的实现有三个方面。模拟器是初始组件，它生成环境数据并接收预定的路径。第二个组件是控制器和规划器，它负责速度控制和航向规划。DQN算法是负责高层变道决策的第三个组成部分。</p>
<p>​    本研究的目的是利用深度强化学习来做出横向变道决策。低级控制器包括基于规则的速度控制器和路径规划器，但不包括数据处理。基于规则的技术用于纵向速度控制，样条插值用于基于指定的路径点和与变道决策结果相匹配的预期目标点进行路径规划。同时控制器作为一个低级的修改器，允许修改高级的决策。</p>
<p>​    当训练启动时，模拟器加载环境并等待来自RL代理的控制动作，同时跟随低级控制器。在训练开始时，赛车总是从中间车道开始。基于低级规则的控制器保持汽车的直线运动，避免向前碰撞。RL代理选择的操作（基于策略或随机）会触发变道，并将其传递给模拟器。然后模拟器会根据它接收到的动作来执行变道。其中一个训练实例如图4所示。代理执行的最后一个行动是去右车道。这个动作被传递到模拟器，汽车进入右车道，如图5所示。<a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1e9CaX4bE08x__5hfRL_mxWcqufk7Tjcj">训练视频</a></p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/figure4.png" alt></p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/figure5.png" alt></p>
<p>​    模拟器最初面临的挑战之一是模拟器和终端必须在每一局结束时重新启动。它需要浏览模拟器菜单，如图6和图7所示。在长时间的训练时，选择选项是不可行的。我们使用PyAutoGUI软件包来自动化这个过程。我们发现除了IDE之外，关闭任何额外的窗口对包的性能都是平滑的。此外有一个双监视器设置有时会导致模拟器在屏幕上以随机坐标打开。因此我们使用单个监视器来更易于执行。该软件包可以自动进行鼠标点击和键盘敲击。对于鼠标点击，我们记录了要单击的单选按钮的坐标，并将其传递给PyAutoGUI进行执行。</p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/figure6.png" alt></p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/figure7.png" alt></p>
<h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>​    模拟器实时运行，自动驾驶车辆在模拟环境中完成一圈大约需要6分钟，这被视为我们训练过程中的一个episode。这个训练计划包括100个episode。当其中一个episode结束时，我们将重新启动下一episode的模拟器。图8显示了在训练和测试过程中变道的频率。图中只显示了前10次训练和测试的结果。</p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/figure8.png" alt></p>
<p>​    车道的改变完全取决于在给定的时间步长下的交通状况。从图8中我们可以看出，在训练过程中，代理经常改变车道，平均约为64.1次。经过100个episode的训练和10个episode的测试，我们看到车道变化的频率显著下降，平均每episode车道变化10次。此外随着训练的进行，我们看到车道变化趋势逐渐减少。然而由于探索，我们确实看到了不稳定的行为。训练后的结果表明，代理已经学会了只在必要时改变车道。</p>
<p>​    比较在参考文献中获得的结果<a href="Automated speed and lane change  decision making using deep reinforcement learning">Automated speed and lane change  decision making using deep reinforcement learning</a>，我们在训练中没有看到变道频率的下降。他们不比较训练和测试的结果。一种可能的理论可能是他们的探索率很低，他们的策略是总是选择动作值最大的动作。</p>
<p>​    我们比较了训练过的基于规则的DQN代理的平均速度和基于DQN的策略的平均速度和平均变道时间。我们在仿真环境中运行代理，然后计算它的平均速度、平均车道变化次数和安全率。安全率被定义为没有崩溃的测试事件与总测试事件的比率。研究结果见table2，这里基于规则的DQN方法比传统的DQN具有更高的安全性和更少的变道次数。这意味着我们的技术可以比其他技术产生更有效、更安全的策略。</p>
<p><img src="/2022/05/19/Lane-Change-Decision-Making-through-Deep-Reinforcement-Learning/table2.png" alt></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>​    在几次迭代中，我们注意到虽然自动驾驶车辆造成了一个“轻微”的碰撞，但它导致了自动驾驶车辆后面的一系列碰撞。在现实生活中，这是一个灾难性的事故，必须被高度惩罚。修改奖励功能可以进一步改进这种方法。未来可能的工作是进行消融研究，制定不同的奖励功能如何改变代理的表现。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/" class="post-title-link" itemprop="url">Enhanced Rolling Horizon Evolution Algorithm with Opponent Model Learning Results for the Fighting Game AI Competition</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-04-15 22:30:07 / Modified: 22:33:06" itemprop="dateCreated datePublished" datetime="2022-04-15T22:30:07+08:00">2022-04-15</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the"><a href="#Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the" class="headerlink" title="Enhanced Rolling Horizon Evolution Algorithm with Opponent Model Learning: Results for the"></a>Enhanced Rolling Horizon Evolution Algorithm with Opponent Model Learning: Results for the</h2><p>Fighting Game AI Competition</p>
<p>格斗游戏是一项极具挑战的<strong>双人实时对抗人工智能博弈任务，常具有组合动作空间规模大、角色属性风格多样、实时性计算要求高等特点，</strong>受到了研究人员的广泛关注。对此，日本立命馆大学发布了<strong>双人格斗游戏AI实时对抗平台FightingICE</strong> ，其已成为IEEE Conference on Games（游戏人工智能旗舰会议）的格斗游戏AI竞赛赛道的指定专用测试平台。FightingICE要求参赛算法必须自主适应三种不同类型的格斗角色属性，在有限时间内通过控制本方智能体的格斗动作，快速击败对方算法控制的智能体。不同于回合制双人零和博弈问题，实时双人零和博弈问题属于非完美信息博弈，在同步决策的过程中无法准确获取对方正要采取的动作行为，进而影响智能体的有效决策行为。</p>
<p>此外，我们提出的基于策略梯度的对手模型的机器人是唯一一个没有使用Monte-Carlo树搜索(MCTS)在2019年的竞争中获得第二名的前五名机器人，同时使用的领域知识比赢家少得多。</p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/fightingIcescreenshot.png" alt></p>
<p>​    针对上述存在的问题与挑战，我们提出了一种新型的<strong>基于自适应对手建模的增强型滚动时域演化算法</strong>，并且将滚动时域演化算法首次应用到格斗游戏AI领域。同时，提出了一系列监督强化式对手模型优化方法（包括交叉熵型、Q学习型和策略梯度型等）用于优化对手模型的预测有效性。具体实现为根据格斗对抗时生成的历史交战信息作为训练数据源，通过在线优化的方式滚动更新对手模型参数，增强前向模型推理规划的可靠有效性，从而进一步增强滚动时域演化算法的博弈推理表现性能。</p>
<h3 id="RHEA"><a href="#RHEA" class="headerlink" title="RHEA"></a>RHEA</h3><p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/rhea.png" alt></p>
<p>Population由多个代表不同动作序列的个体组成。该序列中的每个动作都被视为一个gene。你会根据合适的fitnesses(适应度)来选择一定数量的个体。然后，这些个体被分配到交叉的过程中，并且有可能发生突变，从而产生更多的潜在后代或更强大的后代。然后，将个体作为动作序列展开，并将动作序列有序地输入到正向模型中，以推断未来的状态。用score function对未来的状态进行评估，以获得新的适应度。将重复上述优化过程，直到消耗完时间预算。</p>
<p>在我们实现RHEA时，每个个体中的动作序列都是随机初始化的。 初始化后，创建一组长度相同的个体，每个个体都由适应度函数进行评估。</p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/fitness_function.png" alt></p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/fitness_func_explain.png" alt></p>
<p><script type="math/tex">f_{sco}</script>是当前状态的价值进行评估。源码中实现了两种方式，一种是采用MC rollout评估，一种是启发式评估。</p>
<p><script type="math/tex">f_{div}</script>是用于探索。<script type="math/tex">f_{count}</script>是计算每个基因在总的基因里对应的频率，出现次数越多<script type="math/tex">f_{count}</script>越大。<script type="math/tex">\sum_{j=1}^{l}f_{count}(\vec{z}^{l}(j)))</script>表示个体每个基因出现频率的和。<script type="math/tex">n</script>为个体数量,<script type="math/tex">l</script>为个体基因长度</p>
<p><script type="math/tex">f_{FM}</script>是前向推理模型，从一个状态到下一个状态</p>
<p><script type="math/tex">\lambda</script>是平衡状态价值和diversity。类似于利用和探索</p>
<h3 id="RHEAOM"><a href="#RHEAOM" class="headerlink" title="RHEAOM"></a>RHEAOM</h3><p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/rheaom.png" alt></p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/nextgeneration.png" alt></p>
<p>假设我们在当前这一代中总共有<script type="math/tex">n</script>个个体。得分最高的<script type="math/tex">k</script>个人被选为精英，并保存在下一代。剩下的<script type="math/tex">n-k</script>个体进化通过与精英进行交叉，父母分别从精英和剩下的个体中随机抽取。然后，从个体中随机选择一个基因，并通过均匀分布变异成另一个有效基因。最后，通过fitness function(适应度函数)来重新评估这些最新的个体。如果还有时间预算，请为下一代选择前n个排序的个体，然后再次重复上述演变。否则，从排序最高的个体执行第一个操作。算法1给出了基于对手模型的滚平演化算法的整个过程。</p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/rheaom_fig.png" alt></p>
<h3 id="Opponent-Model"><a href="#Opponent-Model" class="headerlink" title="Opponent Model"></a>Opponent Model</h3><p>网络结构需要简单,只有输入输出层，没有隐藏层</p>
<p>此外，其他更复杂的网络架构，如多层感知器和长期短期内存网络，已经进行了测试，但其性能比简单的架构要差。</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>使用不同对手模型的测试结果</p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/result1.png" alt></p>
<p>RHEA(%)表示对手不做任何动作,-R对手随机，-SL对手模型使用监督学习,-Q对手模型使用Q-learning，-PG对手使用policy gradient。</p>
<p>和其它对手的对打测试结果</p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/winrate_other_agent.png" alt></p>
<p>RHEA和MCTS结合对手模型对打测试</p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/rhea_vs_mcts.png" alt></p>
<p>RHEA与对手模型结合比MCTS要好，并且MCTS结合对手模型比单纯使用MCTS要好</p>
<p>2019年FightingICE比赛结果</p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/match_result_2019.png" alt></p>
<p>进化算法的目标与强化学习优化的目标都是预期奖励。但是，强化学习是将噪声注入动作空间并使用反向传播来计算参数更新，而进化策略则是直接向参数空间注入噪声。换个说话，强化学习是在「猜测然后检验」动作，而进化策略则是在「猜测然后检验」参数。因为我们是在向参数注入噪声，所以就有可能使用确定性的策略（而且我们在实验中也确实是这么做的）。也有可能同时将噪声注入到动作和参数中，这样就有可能实现两种方法的结合。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/15/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/15/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">进化算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-15 22:24:40" itemprop="dateCreated datePublished" datetime="2022-04-15T22:24:40+08:00">2022-04-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-04-27 22:16:03" itemprop="dateModified" datetime="2022-04-27T22:16:03+08:00">2022-04-27</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="进化算法"><a href="#进化算法" class="headerlink" title="进化算法"></a>进化算法</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>进化算法是模拟生物在自然界中的进化。达尔文的进化论指出“<strong>物竞天择，适者生存</strong>”。进化论几乎可以解释一切“为什么这种生物是这样的？”这一类的问题。那些更加适应环境的生物，更加容易留下自己的<strong>染色体</strong>。于是，在计算机中，我们模拟生物中的交叉，变异，选择；</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u010451580/article/details/51178225">遗传算法</a></p>
<h2 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h2><ul>
<li>不知道如何解决一个问题时。列出约束条件。</li>
<li>问题是不断变化的，需要一个自适应的解。比如预测股市。</li>
<li>需要搜索大量的或者无限的可能解，来找到最好的或足够好的解。本质上进化算法可以看成搜索算法搜索一组可能的解，来寻找最好的或最适合的解。</li>
<li><p>可以接受足够好的解</p>
</li>
<li><p>可用于解决数值优化、组合优化、机器学习、智能控制、人工生命、图像处理、模式识别等领域的问题。</p>
</li>
</ul>
<p>进化算法包括遗传算法、进化程序设计、进化规划和进化策略等等，进化算法的基本框架还是简单遗传算法所描述的框架，但在进化的方式上有较大的差异，选择、交叉、变异、种群控制等有很多变化。</p>
<p>比如需要得到字符全为1的字符串，比如”1111”，个体可以理解为候选解，比如“1001”；种群可以理解为候选解的集合{“1001”,”1101”……..}</p>
<p><img src="/2022/04/15/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/基本术语.png" alt></p>
<p>交叉:它是生物繁殖的模拟。<a target="_blank" rel="noopener" href="https://blog.csdn.net/ztf312/article/details/82793295">交叉算子</a></p>
<p>变异:新的遗传信息添加到基因组中的过程。<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_30239361/article/details/101686764">变异算子</a></p>
<h2 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h2><p>一般遗传算法过程</p>
<p><img src="/2022/04/15/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/遗传算法过程.png" alt></p>
<p>遗传算法参数：</p>
<ul>
<li>变异率是在解的染色体中，特定基因将要变异的概率。较高的变异率允许种群中有更多的遗传多样性，有助于算法避免局部最优。同样也会导致每一代之间太多的遗传变异，导致失去以前种群中良好的解。变异率低算法可能用过长时间在搜索空间中移动，妨碍找到满意解的能力。</li>
<li>种群规模：遗传算法中任意一代种群的个体数；较大的种群规模算法可以在搜索空间中取样更多，这将有助于将它导向更准确、全局最优解的方向。小的种群规模通常会导致该算法在搜索空间的局部最优区域发现不太理想的解，但是它们每一代需要的计算资源较少；</li>
<li>交叉率。高交叉率在交叉阶段会产生许多新的、潜在优越的解。较低的交叉率有助于保持较适应个体的基因信息在下一代中不受破坏。</li>
<li>精英数量</li>
</ul>
<h2 id="简单实现案例"><a href="#简单实现案例" class="headerlink" title="简单实现案例"></a>简单实现案例</h2><p>该案例是用于生成指定长度的全1字符串，比如”1111111”。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">使用进化算法生成全是1的字符串，比如&quot;1111111111&quot;。最开始字符串是随机的0或者1组成，通过算法得到全1的字符串</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">class Individual(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self,chromosomeLength):</span><br><span class="line">        self.fitness &#x3D; -1</span><br><span class="line">        self.choromosome &#x3D; [ 0 for i in range(chromosomeLength)]</span><br><span class="line">        for gene_index in range(chromosomeLength):</span><br><span class="line">            if (0.5 &lt; random.random()):</span><br><span class="line">                self.setGene(gene_index,1)</span><br><span class="line">            else:</span><br><span class="line">                self.setGene(gene_index,0)</span><br><span class="line">    </span><br><span class="line">    def getChromosome(self):</span><br><span class="line">        return self.choromosome</span><br><span class="line">    </span><br><span class="line">    def getChromosomeLength(self):</span><br><span class="line">        return len(self.choromosome)</span><br><span class="line"></span><br><span class="line">    def setGene(self,offset,gene):</span><br><span class="line">        self.choromosome[offset] &#x3D; gene</span><br><span class="line"></span><br><span class="line">    def getGene(self,offset):</span><br><span class="line">        return self.choromosome[offset]</span><br><span class="line"></span><br><span class="line">    def setFitness(self,fitness):</span><br><span class="line">        self.fitness &#x3D; fitness</span><br><span class="line">    </span><br><span class="line">    def getFitness(self):</span><br><span class="line">        return self.fitness</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        output &#x3D; &quot;&quot;</span><br><span class="line">        for i in range(len(self.choromosome)):</span><br><span class="line">            output +&#x3D; str(self.choromosome[i])</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">class Population(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self,populationSize,chromosomeLength &#x3D; -1):</span><br><span class="line">        self.populationFitness &#x3D; -1</span><br><span class="line">        self.population &#x3D; []</span><br><span class="line">        if chromosomeLength &#x3D;&#x3D; -1:</span><br><span class="line">            pass</span><br><span class="line">        else:</span><br><span class="line">            for i in range(populationSize):</span><br><span class="line">                self.population.append(Individual(chromosomeLength))</span><br><span class="line">    </span><br><span class="line">    def getIndividuals(self):</span><br><span class="line">        return self.population</span><br><span class="line">    </span><br><span class="line">    def getFittest(self,offset):</span><br><span class="line">        self.population.sort(key &#x3D; lambda ele:ele.getFitness(),reverse&#x3D;True)</span><br><span class="line">        return self.population[offset]</span><br><span class="line"></span><br><span class="line">    def setPopulationFitness(self,fitness):</span><br><span class="line">        self.populationFitness &#x3D; fitness</span><br><span class="line">    </span><br><span class="line">    def getPopulationFitness(self):</span><br><span class="line">        return self.populationFitness</span><br><span class="line"></span><br><span class="line">    def size(self):</span><br><span class="line">        return len(self.population)</span><br><span class="line"></span><br><span class="line">    def setIndividual(self,offset,individual):</span><br><span class="line">        self.population[offset] &#x3D; individual </span><br><span class="line">    </span><br><span class="line">    def getIndividual(self,offset):</span><br><span class="line">        return self.population[offset]</span><br><span class="line"></span><br><span class="line">    def shuffle(self):</span><br><span class="line">        for i in range(len(self.population)-1,-1,-1):</span><br><span class="line">            index &#x3D; random.randint(i)</span><br><span class="line">            individual_tmp &#x3D; self.population[index]</span><br><span class="line">            self.population[index] &#x3D; self.population[i]</span><br><span class="line">            self.population[i] &#x3D; individual_tmp</span><br><span class="line"></span><br><span class="line">class GeneticAlgorithm(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self,populationSize,mutationRate,crossoverRate,elitismCount):</span><br><span class="line">        self.populationSize &#x3D; populationSize</span><br><span class="line">        self.mutationRate &#x3D; mutationRate</span><br><span class="line">        self.crossoverRate &#x3D; crossoverRate</span><br><span class="line">        self.elitismCount &#x3D; elitismCount</span><br><span class="line"></span><br><span class="line">    def initPopulation(self,chromosomeLength):</span><br><span class="line">        population &#x3D; Population(self.populationSize,chromosomeLength)</span><br><span class="line">        return population</span><br><span class="line">    </span><br><span class="line">    def calcFitness(self,individual):</span><br><span class="line">        correctGenes &#x3D; 0</span><br><span class="line">        for i in range(individual.getChromosomeLength()):</span><br><span class="line">            if individual.getGene(i) &#x3D;&#x3D; 1:</span><br><span class="line">                correctGenes +&#x3D; 1</span><br><span class="line">        fitness &#x3D; correctGenes &#x2F; individual.getChromosomeLength()</span><br><span class="line">        individual.setFitness(fitness)</span><br><span class="line">        return fitness</span><br><span class="line"></span><br><span class="line">    def evalPopulation(self,population):</span><br><span class="line">        populationFitness &#x3D; 0</span><br><span class="line">        for individual in population.getIndividuals():</span><br><span class="line">            populationFitness +&#x3D; self.calcFitness(individual)</span><br><span class="line">        population.setPopulationFitness(populationFitness)</span><br><span class="line"></span><br><span class="line">    def isTerminationConditionMet(self,population):</span><br><span class="line">        for individual in population.getIndividuals():</span><br><span class="line">            if individual.getFitness() &#x3D;&#x3D; 1:</span><br><span class="line">                return True </span><br><span class="line">        return False </span><br><span class="line"></span><br><span class="line">    def selectParent(self,population):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        根据每个个体的适应度权重进行选择</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        individuals &#x3D; population.getIndividuals()</span><br><span class="line">        populationFitness &#x3D; population.getPopulationFitness()</span><br><span class="line">        rouletteWheelPosition &#x3D; random.random() * populationFitness</span><br><span class="line">        spinWheel &#x3D; 0</span><br><span class="line">        for individual in individuals:</span><br><span class="line">            spinWheel +&#x3D; individual.getFitness()</span><br><span class="line">            if spinWheel &gt;&#x3D; rouletteWheelPosition:</span><br><span class="line">                return individual</span><br><span class="line">        return individuals[population.size() - 1]</span><br><span class="line">    </span><br><span class="line">    def crossoverPopulation(self,population):</span><br><span class="line">        newPopulation &#x3D; Population(population.size())</span><br><span class="line">        for populationIndex in range(population.size()):</span><br><span class="line">            parent1 &#x3D; population.getFittest(populationIndex)</span><br><span class="line">            if self.crossoverRate &gt; random.random() and populationIndex &gt; self.elitismCount:</span><br><span class="line">                offspring &#x3D; Individual(parent1.getChromosomeLength())</span><br><span class="line">                parent2 &#x3D; self.selectParent(population)</span><br><span class="line">                for geneIndex in range(parent1.getChromosomeLength()):</span><br><span class="line">                    if 0.5 &gt; random.random():</span><br><span class="line">                        offspring.setGene(geneIndex,parent1.getGene(geneIndex))</span><br><span class="line">                    else:</span><br><span class="line">                        offspring.setGene(geneIndex,parent2.getGene(geneIndex))</span><br><span class="line">                newPopulation.population.append(offspring)</span><br><span class="line">            else:</span><br><span class="line">                newPopulation.population.append(parent1)</span><br><span class="line">        return newPopulation</span><br><span class="line"></span><br><span class="line">    def mutatePopulation(self,population):</span><br><span class="line">        newPopulation &#x3D; Population(self.populationSize)</span><br><span class="line">        for populationIndex in range(population.size()):</span><br><span class="line">            individual &#x3D; population.getFittest(populationIndex)</span><br><span class="line">            for geneIndex in range(individual.getChromosomeLength()):</span><br><span class="line">                if populationIndex &gt;&#x3D; self.elitismCount:</span><br><span class="line">                    if self.mutationRate &gt; random.random():</span><br><span class="line">                        newGene &#x3D; 1</span><br><span class="line">                        if individual.getGene(geneIndex) &#x3D;&#x3D; 1:</span><br><span class="line">                            newGene &#x3D; 0</span><br><span class="line">                        individual.setGene(geneIndex,newGene)</span><br><span class="line">            newPopulation.append(individual)</span><br><span class="line">        return newPopulation</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    ga &#x3D; GeneticAlgorithm(100,0.5,0.95,30)</span><br><span class="line">    population &#x3D; ga.initPopulation(200)</span><br><span class="line">    ga.evalPopulation(population)</span><br><span class="line">    generation &#x3D; 1</span><br><span class="line">    while not ga.isTerminationConditionMet(population):</span><br><span class="line">        print(f&quot;Best solution &#123;population.getFittest(0)&#125;&quot;)</span><br><span class="line">        population &#x3D; ga.crossoverPopulation(population)</span><br><span class="line">        ga.evalPopulation(population)</span><br><span class="line">        generation +&#x3D; 1</span><br><span class="line">    print(f&quot;Found solution in &#123;generation&#125; generations&quot;)</span><br><span class="line">    print(f&quot;Best solution &#123;population.getFittest(0)&#125;&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">多智能体强化学习算法总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-15 22:12:57" itemprop="dateCreated datePublished" datetime="2022-04-15T22:12:57+08:00">2022-04-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-04-27 22:15:56" itemprop="dateModified" datetime="2022-04-27T22:15:56+08:00">2022-04-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93/" itemprop="url" rel="index"><span itemprop="name">多智能体</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>​    强化学习的核心思想是“试错”（trial-and-error）：智能体通过与环境的交互，根据获得的反馈信息迭代地优化。在 RL 领域，待解决的问题通常被描述为<strong>马尔科夫决策过程</strong></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/concept_1.png" alt></p>
<p>​    当同时存在多个智能体与环境交互时，整个系统就变成一个多智能体系统（multi-agent system）。每个智能体仍然是遵循着强化学习的目标，也就是是最大化能够获得的累积回报，而此时环境全局状态的改变就和所有智能体的联合动作（joint action）相关了。因此在智能体策略学习的过程中，需要考虑联合动作的影响。</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/concept_2.png" alt></p>
<p>马尔科夫决策过程拓展到多智能体系统，被定义为马尔科夫博弈。在马尔科夫博弈中，所有智能体根据当前的环境状态（或者是观测值）来同时选择并执行各自的动作，该各自动作带来的联合动作影响了环境状态的转移和更新，并决定了智能体获得的奖励反馈。它可以通过元组 <script type="math/tex">\left \langle S,A_{1},...,A_{N},T,R_{1},...,R_{n} \right \rangle</script>来表示，其中<script type="math/tex">S</script>表示状态集合，<script type="math/tex">A_{i}</script>和<script type="math/tex">R_{i}</script>分别表示智能体<script type="math/tex">i</script>的动作集合和奖励集合，T 表示环境状态转移概率。</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/concept_3.png" alt></p>
<p>​    一个完全合作式的多智能体任务(我们有<script type="math/tex">n</script>个智能体，这<script type="math/tex">n</script>个智能体需要相互配合以获取最大奖励)可以描述为<strong>去中心化的部分可观测马尔可夫决策模型</strong>(<code>Dec-POMDP</code>)，通常用一个元组<img src="https://www.zhihu.com/equation?tex=G" alt="[公式]">来表示：</p>
<script type="math/tex; mode=display">
G=\left \langle S,U,P,r,Z,O,n,\gamma  \right \rangle</script><p>其中<script type="math/tex">s\in S</script>表示环境的真实状态信息。在每一个时间步，对于每个智能体<script type="math/tex">a\in A\equiv \left \{ 1,...,n \right \}</script>都需要去选择一个动作去<script type="math/tex">u^{a}\in U</script>组成一个联合动作<script type="math/tex">u\in U\equiv U^{n}</script>这个联合动作给到环境中去进行状态转移:<script type="math/tex">P\left ( s^{'}|s,u \right ):S\times U\times S\rightarrow \left [ 0,1 \right ]</script>。之后，所有的智能体都会收到一个相同的奖励:<script type="math/tex">r\left ( s,u \right ):S\times U\rightarrow \mathbb{R}</script>。与单智能体一样<script type="math/tex">\gamma</script>表示折扣因子。</p>
<p>对于每个单智能体<script type="math/tex">a</script>来说，它接收的是一个独立的部分可观测的状态<script type="math/tex">z \in Z</script>，不同的智能体<script type="math/tex">a</script>具备不同的观测，但是所有的观测都来自环境的真实状态信息，所以可以用函数表示为：<script type="math/tex">O\left ( s,a \right ):S\times A\rightarrow Z</script>。对于每个智能体<script type="math/tex">a</script>它都有一个动作观测历史<script type="math/tex">\tau ^{a}\in T\equiv \left ( Z\times U \right )^{*}</script>，基于这个动作-观测的历史来构建随机策略函数<script type="math/tex">\pi ^{a}\left ( u^{a}|\tau ^{a} \right ):T\times U\rightarrow \left [ 0,1 \right ]</script>。联合动作策略<script type="math/tex">\pi</script>是基于状态信息<script type="math/tex">s_{t}</script>构建的联合动作值函数<script type="math/tex">Q^{\pi }\left ( s_{t},u_{t} \right )= E_{s_{t+1}:\infty ,u_{t+1}:\infty }\left [ R_{t}|s_{t},u_{t} \right ]</script>其中<script type="math/tex">R_{t}= \sum_{i=0}^{\infty }\gamma ^{i}r_{t+i}</script>是折扣回报。</p>
<h2 id="传统RL算法在协同任务中的不足"><a href="#传统RL算法在协同任务中的不足" class="headerlink" title="传统RL算法在协同任务中的不足"></a>传统RL算法在协同任务中的不足</h2><p>若使用传统的RL算法来解决多智能体的问题，则会存在以下三个不足之处：</p>
<ul>
<li><p>输入的 action space 应该是所有 agent 的 <strong>联合动作空间</strong>（joint action space），这个空间会随着 agent 数量增加而增加。</p>
</li>
<li><p>由于部分可观测性（即单个agent在某一时刻只能观测到部分环境的信息，无法获得全局信息，比如一个小兵只能看到视野范围内的地图信息，视野外的地图信息是无法观测的），使得 agent 在做决策时只能依照自己当前的部分观测信息（local observation），没有与其他 agent 进行信息共享的能力。</p>
</li>
<li><p>使用联合动作空间获得的 reward 是来自所有 agent 采取的所有 action 共同得到的reward，这就很难知道每一个 agent 的 action 应该得到的多少子回报。</p>
</li>
</ul>
<h2 id="IQL"><a href="#IQL" class="headerlink" title="IQL"></a>IQL</h2><p>IQL算法中将其余智能体直接看作环境的一部分，也就是对于每个智能体<script type="math/tex">a</script>都是在解决一个单智能体任务，很显然，由于环境中存在智能体，因此环境是一个非稳态的，这样就无法保证收敛性了，并且智能体会很容易陷入无止境的探索中，但是在工程实践上，效果还是比较可以的。</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_2.png" alt></p>
<p>是否可以关闭经验回放</p>
<p>使用循环神经网络处理部分可观察</p>
<h2 id="VDN"><a href="#VDN" class="headerlink" title="VDN"></a>VDN</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>​    本文是考虑协作任务的多智能体强化学习问题，即所有的智能体共享同一个奖励值，VDN是一种基于值函数的方法。智能体共享团队奖励会带来“credit assignment ” 问题，即利用该团队奖励拟合出的值函数不能评价每个智能体的策略对整体的贡献。作者认为，由于每个智能体都是局部观测，那么对其中一个智能体来说，其获得的团队奖励很有可能是其队友的行为导致的。也就是说该奖励值对该智能体来说，是“<strong>虚假奖励 (spurious reward signals)</strong>”。因此，每个智能体独立使用强化学习算法学习 (即independent RL) 往往效果很差。</p>
<p>这种虚假奖励还会伴随一种现象，作者称作 “<strong>lazy agent (惰性智能体)</strong>”。当团队中的部分智能体学习到了比较好的策略并且能够完成任务时，其它智能体不需要做什么也能获得不错的团队奖励，这些智能体就被称作“惰性智能体”。</p>
<p>其实不管是“虚假奖励”还是“惰性智能体”，本质上还是credit assignment问题。如果每个智能体都会根据自己对团队的贡献，优化各自的目标函数，就能够解决上述问题。基于这样的动机，作者提出了“值函数分解”的研究思路，将团队整体的值函数分解成<script type="math/tex">N</script>个子值函数，分别作为各智能体执行动作的依据。</p>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_1.png" alt></p>
<p>假设<script type="math/tex">Q\left ( \left ( h^{1},h^{2},...h^{d} \right ),\left ( a^{1},a^{2},...,a^{d} \right ) \right )</script>是多智能体团队的整体Q函数,<script type="math/tex">d</script>是智能体个数，<script type="math/tex">h^{i}</script>是智能体<script type="math/tex">i</script>的历史序列信息,<script type="math/tex">a^{i}</script>是其动作。该Q函数的输入集中了所有智能体的观测和动作，可通过团队奖励<script type="math/tex">r</script>来迭代拟合。为了得到各个智能体的值函数，作者提出如下假设：系统的联合动作值函数可以分解为多个代理值函数相加。<script type="math/tex">\tilde{Q}_{i}</script>只依赖于局部观察</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_7.png" alt></p>
<p>该假设表明团队的Q函数可以通过求和的方式近似分解成<script type="math/tex">d</script>个子Q函数，分别对应<script type="math/tex">d</script>个不同的智能体，且每个子Q函数的输入为该对应智能体的局部观测序列和动作，互相不受影响。每个智能体就有了自己的值函数，它们就可以根据自己的局部值函数来进行决策。<script type="math/tex">\tilde{Q}_{i}\left ( h^{i},a^{i} \right )</script>并不是任何严格意义上的Q值函数。因为并没有理论依据表明一定存在一个reward函数，使得该<script type="math/tex">\tilde{Q}_{i}</script>满足贝尔曼方程。</p>
<p>上面等式成立需要满足<script type="math/tex">r\left ( s,a \right )=\sum_{i=1}^{d}r\left ( o^{i},a^{i} \right )</script>,其中<script type="math/tex">s</script>表示系统全局状态,<script type="math/tex">a</script>表示智能体联合动作。<script type="math/tex">r\left ( s,a \right )=\sum_{i=1}^{d}r\left ( o^{i},a^{i} \right )</script>表明团队的整体奖励应由所有智能体各自的奖励函数求和得到。然而，即使这个条件成立，根据文章中的证明，Q函数的分解也应该写成<script type="math/tex">Q\left ( s,a \right )=\sum_{i=1}^{d}Q_{i}\left ( s,a \right )</script>,子Q函数的输入应该还是全局状态<script type="math/tex">s</script>和联合动作<script type="math/tex">a</script>，而不是上式中的形式。所以，此处的bug是VDN算法的硬伤所在，作者也因此将每个智能体历史状态、动作、奖励的序列信息作为其值函数<script type="math/tex">\tilde{Q}_{i}</script>的输入，以此弥补局部观测上的不足。</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_8.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_9.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_2.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_3.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_4.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_5.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_6.png" alt></p>
<h2 id="QMIX"><a href="#QMIX" class="headerlink" title="QMIX"></a>QMIX</h2><p>VDN算法考虑将<script type="math/tex">Q_{tot}</script>分解成<script type="math/tex">Q_{i}</script>，以<script type="math/tex">Q_{i}</script>作为每个智能体的Q值函数来计算最优动作，并以<script type="math/tex">Q_{tot} = \sum_{i=1}^{n}Q_{i}</script>的分解形式端到端地训练网络。然而这种以简单的求和方式对整体值函数进行分解，将会导致网络的函数表达能力受到很大限制，难以拟合出真实的<script type="math/tex">Q_{tot}</script>。</p>
<p>如果我们直接使用普通的神经网络对<script type="math/tex">Q_{tot}</script>进行分解，例如 <script type="math/tex">Q_{tot}= MLP\left ( Q_{1},...,Q_{n} \right )</script> ，倒是可以提升网络的函数表达能力，但会遇到另外一个问题：<strong>非单调性（non-monotonicity）</strong>，导致算法难以保证分布式策略的最优性。所谓<strong>单调性</strong>，就是指通过分布式策略计算出来的动作，和通过整体Q函数计算出来的动作，在“性能最优”上需要保持一致，即</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/qmix_2.png" alt></p>
<p>值函数分解的单调性只需要满足如下条件即可:</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/qmix_3.png" alt></p>
<p><script type="math/tex">Q_{a}</script>代表每个智能体a的值函数</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/qmix_1.png" alt></p>
<p>​    其中<script type="math/tex">\epsilon</script>表示探索的概率,<script type="math/tex">o_{t}^{a}</script>表示t时刻智能体a的观测，<script type="math/tex">u_{t}^{a}</script>表示t时刻智能体a的动作。加粗的<script type="math/tex">u</script>表示联合行动，加粗的<script type="math/tex">\tau</script>表示联合动作-观测历史。</p>
<p>​    混合网络的权值是由单独的hypernetworks产生的。每个hypernetwork以状态s作为输入，生成混合网络的一层的权值。每个超网络由一个线性层组成，然后是一个绝对激活函数，以确保混合网络的权值是非负的。超网络的输出是一个向量，它被重塑为一个适当大小的矩阵。这些bias也是以同样的方式产生的，但并不局限于非负的。最终的偏差是由一个具有ReLU非线性的2层超网络产生的。图2a说明了混合网络和超网络。</p>
<p>​    由于<script type="math/tex">Q_{tot}</script>被允许以非单调的方式依赖于额外的状态信息，因此该状态被超网络使用，而不是直接传递到混合网络中。因此，将s的某些函数通过单调网络一起传递将是过度约束的。相反，超网络的使用使得以任意的方式条件s上的权值成为可能，从而将完整状态s尽可能灵活的集成到联合动作值估计中。</p>
<h2 id="MADDPG"><a href="#MADDPG" class="headerlink" title="MADDPG"></a>MADDPG</h2><p>​    该算法针对连续动作场景，可用于竞争、合作、通信。</p>
<p>​    如果直接使用DDPG算法来学习每个智能体的策略，只考虑自己的观测和动作，将其它智能体看作环境中的一部分，则训练过程会非常不稳定，甚至最终无法收敛。这是因为其它智能体的策略也在不断更新变化，智能体的采样数据并不服从一个稳定的概率分布。所以，作者同样采用了“集中式训练和分布式执行 (centralized training with decentralized execution)”的学习框架，允许智能体在训练的过程中获取更多其它智能体的数据，其算法结构如下图所示:</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/maddpg_1.png" alt></p>
<p>actor更新:</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/maddpg_4.png" alt></p>
<p>critic更新：</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/maddpg_3.png" alt></p>
<p><script type="math/tex">\mu ^{'}= \left \{ \mu _{\theta _{1}^{'}},...,\mu _{\theta _{N}^{'}} \right \}</script>为具有延时参数的target policy set</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/maddpg_2.png" alt></p>
<h2 id="COMA"><a href="#COMA" class="headerlink" title="COMA"></a>COMA</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/346076252">参考1</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361648110">参考2</a></p>
<p>VDN是一种基于值函数的方法，而COMA基于策略梯度。在介绍VDN的时候，我们提到过智能体共享团队奖励会带来“credit assignment ” 问题，即利用该团队奖励拟合出的值函数不能评价每个智能体的策略对整体的贡献。在这篇文章中，同样存在这样的问题。该算法是集中式训练，分布式执行。COMA 利用全局评价网络（critic）来评价 Q 值，利用非全局行为网络（actor）来决定 agent 的行为。由于在训练时使用的是全局网络进行评价，并且采用参数共享的方式，使得agent能够在做行为选择的时候参考其他 agent 的状态再做决定，这就加入了“协同”的功能。</p>
<h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><ul>
<li>使用一个集中式critic网络如图1c，在训练的过程中可以获取所有智能体的信息；</li>
</ul>
<p>critic网络的输入包括其它智能体的动作<script type="math/tex">u_{t}^{-a}</script>，全局状态<script type="math/tex">s_{t}</script>，该智能体的局部观测<script type="math/tex">o_{t}^{a}</script>，该智能体的id(one hot形式)，以及上一时刻所有智能体的动作<script type="math/tex">u_{t-1}</script>。这样在critic网络的输出端就能够直接得到该智能体各个动作的反事实Q值。</p>
<ul>
<li>采用反事实基线（counterfactual baseline）来解决信用分配的问题；通过以下公式计算</li>
</ul>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/coma_2.png" alt></p>
<p>为了解决 Individual Reward Assignment 的问题，反事实准则提出，每个agent应该拥有不同的reward，这样才能知道在这一次的全局行为决策中单个agent的action贡献是多少。而单个agent的reward通过两个值计算得来：当前情况下的全局reward和将该agent行为替换为一个默认行为后的全局reward,表示为<script type="math/tex">D^{a}=r\left ( s,u \right )-r\left ( s,\left ( u^{-a},c^{a} \right ) \right )</script>可以这样理解：该回报值其实计算的是Agent <script type="math/tex">a</script>采取行为 <script type="math/tex">u</script> 会比采取默认行为 <script type="math/tex">c_a</script> 要更好（<script type="math/tex">D^a</script> &gt; 0）还是更坏（<script type="math/tex">D^a</script> &lt; 0）。这个<strong>特定agent</strong>下<strong>特定动作</strong>的<strong>reward</strong>就被称为<strong>counterfactual baseline</strong>，COMA使得每一个agent的每一个action都有一个自身的counterfactual baseline。但是要想计算出每一个动作的<script type="math/tex">D^{a}</script>值，就需要将每个动作都替换成默认行为<script type="math/tex">c_{a}</script>去与环境互动一次得到最终结果，这样采样次数会非常多；此外，默认行为的选取也是无法预测的，到底选择哪一个行为当作默认行为才是最合适的也是比较难决定的。因此，文中提出使用”函数拟合”的方式来计算<script type="math/tex">D^{a}</script> 。</p>
<p>前面提到，中心评价网络可以评价一个联合动作空间<script type="math/tex">u</script>在一个状态 s 下的 Q 值。由于默认行为很难定义，于是我们把<strong>采取 “默认行为” 得到的效用值近似为采取一个Agent “所有可能行为” 的效用值总和</strong>。因此， <script type="math/tex">D^{a}</script>就可以用上面等式进行计算</p>
<p>critic网络可以输出在其它智能体动作确定的情况下，自己采取每个动作的Q值。在训练时可以根据当前状态自己采取的action得到<script type="math/tex">Q(s,u)</script>。公式后半部分是对各个动作的Q值求期望，即每个动作的概率(actor网络输出)乘对应的Q值。</p>
<ul>
<li>Critic网络要能够对反事实基线进行高效的计算。</li>
</ul>
<p>尽管baseline的方式解决了独立回报的问题，但是如果要建立一个网络，接收<script type="math/tex">s,u</script>两个输入，输出为所有 agent 的所有 action 的话，那么输出神经元的个数就等于<script type="math/tex">|U|^{n}</script>（n个agent有|U|个动作）。当agent数目很多或动作空间很大的时候就会造成输出层无法实现。为此，COMA构造了一种网络，如下图c所示。</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/coma_1.png" alt></p>
<p>策略梯度更新公式(actor)如下：</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/coma_3.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/coma_4.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/coma_5.png" alt></p>
<p>critic更新使用TD error的方式</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361648110">coma</a></p>
<h2 id="QATTEN"><a href="#QATTEN" class="headerlink" title="QATTEN"></a>QATTEN</h2><h2 id="QTRAN"><a href="#QTRAN" class="headerlink" title="QTRAN"></a>QTRAN</h2><h2 id="WQMIX"><a href="#WQMIX" class="headerlink" title="WQMIX"></a>WQMIX</h2><h2 id="MAPPO"><a href="#MAPPO" class="headerlink" title="MAPPO"></a>MAPPO</h2><h2 id="MAD4PG"><a href="#MAD4PG" class="headerlink" title="MAD4PG"></a>MAD4PG</h2><h2 id="RIAL"><a href="#RIAL" class="headerlink" title="RIAL"></a>RIAL</h2><p>关闭经验回放，为了避免不稳定环境下带来的经验失效或误导</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/11/Trust-Region-Policy-Optimization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/11/Trust-Region-Policy-Optimization/" class="post-title-link" itemprop="url">Trust Region Policy Optimization</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-11 21:51:25" itemprop="dateCreated datePublished" datetime="2022-04-11T21:51:25+08:00">2022-04-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/" class="post-title-link" itemprop="url">PerfectDou:Dominating DouDizhu with Perfect Information Distillation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-07 21:32:23" itemprop="dateCreated datePublished" datetime="2022-04-07T21:32:23+08:00">2022-04-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-04-27 22:19:38" itemprop="dateModified" datetime="2022-04-27T22:19:38+08:00">2022-04-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E6%A3%8B%E7%89%8C%E6%B8%B8%E6%88%8FAI/" itemprop="url" rel="index"><span itemprop="name">棋牌游戏AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><strong>Perfect information distillation</strong>是一个使用完全信息训练，不完全信息执行（perfect-training-imperfect-execution (PTIE)）的actor-critic框架，类似于集中式训练分布式执行，critic利用完全信息（全局信息），actor利用不完全信息（局部信息）。Actor-critic是一个策略梯度(PG)方法的模板，通过一个具有价值函数的PG来最大化策略函数的预期回报：</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/formula1.png" alt></p>
<p>其中s表示RL问题中的一个状态，Q是由一个函数逼近器学习到的state-action值函数，通常称为critic。请注意，critic扮演的角色是只在训练时评估在特定情况下采取的行动的良好程度。当智能体被部署到推理中时，只能使用策略<script type="math/tex">\pi</script>(actor)来获取可行的动作。因此，对于不完全信息游戏，我们可以提供关于玩家的额外信息通过self-play来训练critic，只要actor不使用这些信息来进行决策。直观地说，就是把完全的信息提炼成不完全的策略。可以将公式1重写为：</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/formula2.png" alt></p>
<p><script type="math/tex">h</script>表示为局部信息构成的状态，<script type="math/tex">D(h)</script>（包括额外的信息比如其它玩家手中的牌）表示为全局信息构成的状态。</p>
<p>PTIE是训练不完美信息游戏代理的一种通用方法。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure1.png" alt></p>
<h2 id="PerfectDou系统设计"><a href="#PerfectDou系统设计" class="headerlink" title="PerfectDou系统设计"></a>PerfectDou系统设计</h2><h3 id="card-representation"><a href="#card-representation" class="headerlink" title="card representation"></a>card representation</h3><p>使用<script type="math/tex">12 \times 15</script>的矩阵来表示各种可行的牌的组合，如图2所示。使用<script type="math/tex">4 \times 15</script> 的矩阵来表示牌的数值及数量，列对应15种不同的牌值，第1-4行中每列对应的1的数量表示玩家手牌中该牌值对应的数量。将手牌的合法组合进行编码，合法组合见table6。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure2.png" alt></p>
<p>特征的设计如table1所示，分别给出了不完全特征设计（actor网络使用）和完全的特征设计（critic网络使用）。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table1.png" alt></p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table6.png" alt></p>
<h3 id="网络结构和动作表示"><a href="#网络结构和动作表示" class="headerlink" title="网络结构和动作表示"></a>网络结构和动作表示</h3><p>​    PerfectDou使用PPO作为学习算法。对于价值网络，使用MLP来处理编码的特性。</p>
<p>​    期望价值函数（critic）能够利用全局信息来评估玩家的现状的，可以向critic提供智能体不允许看到的额外信息。如图11所示，价值网络（critic）的不完全特征采用策略网络(actor)中的共享网络进行编码，此外还对策略在博弈过程中无法观察到的完全特征进行了编码。然后将编码的向量连接到一个简单的MLP上，以得到Value。</p>
<p>​    对于策略网络，首先使用LSTM对所有设计的特性进行编码；为了鼓励代理注意特定的卡牌类型，所提出的网络结构将把所有可用的动作编码为特征向量，如table7所示。然后利用动作和游戏特征计算合法动作概率的输出，如图3所示。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure11.png" alt></p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table7.png" alt></p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure3.png" alt></p>
<h3 id="reward-design"><a href="#reward-design" class="headerlink" title="reward design"></a>reward design</h3><p>​    如果只关心游戏结束时的结果，那么奖励相当稀疏；此外玩家只能在游戏中使用不完全的信息来估计他们赢得游戏的优势，这可能是不准确和波动的。为此，我们提出了一个在每个状态上的增强奖励函数。我们不是让玩家单独评估优势，而是使用一个公式来评估每个玩家，特别是出完所有牌所需的最小步骤，这可以被视为对获胜距离的简单估计。奖励函数被定义为由两个阵营在两个连续时间步中获胜的相对距离计算出的优势差。在时间<script type="math/tex">t</script>，奖励函数是：</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/formula4.png" alt></p>
<p>其中<script type="math/tex">l</script>是一个比例因子，<script type="math/tex">N_{t}</script>是在时间步长<script type="math/tex">t</script>上出完所有牌的最小步骤。</p>
<p>​    例如在一局中，在时间戳t，地主获胜的距离是5，两个农民的距离是3和7，这意味着农民有较大的优势，因为农民的相对距离是2，地主是-2。如果地主出牌好，所有农民都不能压制，由于地主的相对距离减少，即从2到1，地主就会得到积极的奖励。相应地，随着农民相对距离的增大，农民会得到负奖励。相反，如果距离为3的农民只是压制房东的牌手，其他农民pass，两个阵营的奖励都是0。这种奖励功能可以鼓励农民之间的合作，因为获胜距离是由双方的最小步数定义的。在实现中，奖励的计算是在一局游戏后进行的，从而提高训练效率。</p>
<h3 id="分布训练细节"><a href="#分布训练细节" class="headerlink" title="分布训练细节"></a>分布训练细节</h3><p>​    为了进一步加快训练过程，设计了一个如图4所示的分布式训练系统。具体来说，该系统包含一组rollout worker，用于收集self-play经验数据并将其发送到gpu池；这些gpu异步接收数据并将其存储到本地缓冲区中。然后，每个学习者从自己的缓冲区中随机抽取小批量样本，并分别计算梯度，然后在所有gpu上同步平均，并反向传播以更新神经网络。在每一轮更新后，新的参数将被发送给每个rollout worker。每个worker将在24步（每个玩家8步后）加载最新的模型。这种解耦的训练采样结构将使PerfectDou能够扩展到大规模的实验中。我们的分布式系统设计大量借鉴了IMPALA它还维持了一组rollout workers来接收更新的模型与环境交互，并将rollout轨迹发送给学习者。主要的区别来自学习算法，本文使用PPO而不是V-trace。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure4.png" alt></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><p>​    使用880个cpu核心和8个gpu构建了一个小型的分布式训练集群。Horovod用于同步GPU之间的梯度，总的batch size 为1024，每个GPU的批处理大小为128。在早期训练阶段，总奖励函数将是一个基本的奖励(WP或ADP)，使用公式4中的奖励来增强，帮助收敛。在后期阶段，增强的奖励将被删除。实验的超参数如table8所示。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table8.png" alt></p>
<p>给定两种算法A对B，我们计算：</p>
<ul>
<li>WP（获胜率）：在一些比赛中A获胜的比例。</li>
<li>ADP（平均得分差）：A和B之间每场比赛的平均得分差。基础得分是1，每个炸弹使得分翻倍。这是一个评估斗地主AI系统的更合理的指标。</li>
</ul>
<p>​    在DouZero中，他们更关注比赛的输赢结果，而不太关心分数。然而，在真正的比赛中，玩家必须玩大量的游戏，并根据他们获胜的分数进行排名。这就是为什么我们认为ADP是评估AI人工智能系统的更好指标，因为一个糟糕的人工智能玩家可以用很少的分数赢得游戏，但失去更多的分数。</p>
<p>​    具体来说，在每个游戏中，地主和农民的基础得分分别是2和1。当游戏中出现炸弹时，每个玩家的分数都会加倍。例如，一个农民玩家首先出一个4的炸弹，然后地主玩家用火箭压制它，然后每个农民的基础分数变成4，地主变成8。玩家在赢得比赛后将赢得所有的分数，或者输掉所有的分数。</p>
<p>​    <strong><em>在我们的实验中，我们选择ADP作为基本奖励，在早期训练阶段，用公式4中提出的奖励信号来增强。</em></strong></p>
<p>​    每两个智能体会测试10000局，忽略了叫分阶段，主要考虑出牌阶段。所有的游戏都是随机生成的，每局游戏将进行两次，即每个竞争算法被分配为地主或农民一次。我们分别使用WP和ADP作为基本奖励来比较所有评估方法的这两个指标，测试结果如table2所示。</p>
<p>​    总体而言，PerfectDou击败了所有现有的人工智能程序，无论是基于规则还是基于学习的算法，在WP和ADP上都具有显著优势。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table2.png" alt></p>
<p>​    与DouZero的训练效率比较如table3所示。可以看出PerfectDou使用1e9个样本训练出的模型强于DouZero使用1e10训练出的模型。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table3.png" alt></p>
<p>​    图12显示了PerfectDou在训练过程中与DouZero最终模型对战的WP和ADP变化。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure12.png" alt></p>
<h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>​    进一步研究PerfectDou成功的关键。具体来说是为了分析特征设计和训练框架是如何帮助PerfectDou主宰斗地竹锦标赛的。为此评估了PerfectDou的不同变体和之前的SoTA AI系统(DouZero)，包括：</p>
<ul>
<li>ImperfectDouZero:DouZero与本文提出的不完全信息特征结合</li>
<li>ImperfectDou：PerfectDou只有不完全的特征作为值函数的输入</li>
<li>RewardlessDou：PerfectDou不使用公式4中的增强reward</li>
<li>Vanilla PPO：简单的actor-critic训练，只具有不完全的特征，而且没有公式4中额外的奖励。</li>
</ul>
<p>结果如table4所示。即使只有不完全的特征，PerfectDou仍然可以用同样的训练步骤轻松击败DouZero；然而DouZero用更多的训练数据来扭转结果。此外，我们提出的特征似乎不适合DouZero获得比原始设计更好的结果。此外在没有公式4的奖励的情况下，PerfectDou仍然以更高的WP击败了DouZero(尽管牺牲了大量的ADP)，这表明了增强奖励在训练中的有效性，如果没有它，就会冒着失去分数赢得一场比赛的风险。最后，如果没有公式4中的增强奖励和完全特征设计用于价值网络，简单的PPO根本不能很好地执行。因此可以得出结论，基于actor-critic的算法和PTIE训练在我们的特征设计下提供了较高的样本效率，并且增强奖励有利于我们的人工智能的合理性。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table4.png" alt></p>
<h3 id="运行时间分析"><a href="#运行时间分析" class="headerlink" title="运行时间分析"></a>运行时间分析</h3><p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure5.png" alt></p>
<p>​    DouZero的平均推理时间为2毫秒，而PerfectDou的平均推理时间为6毫秒。而PerfectDou比DouZero慢一点的原因可能是因为更复杂的特征处理过程。以上分析表明，PerfectDou适用于高级游戏人工智能等现实世界的应用。</p>
<h3 id="深入统计分析"><a href="#深入统计分析" class="headerlink" title="深入统计分析"></a>深入统计分析</h3><p>​    我们对DouZero和PerfectDou之间的游戏进行了深入的分析和收集了统计数据。特别是我们组织了PerfectDou和DouZero之间的游戏，在每个设置的10万局游戏中扮演不同的角色。由于在我们的实验中，角色是分配而不是选择，而且地主有额外的三张牌和有更高的基础分数（例如农民基础分为1，地主基础分为2），从table5中显示的统计数据中可以看出扮演地主总是一个低劣的选择，导致负的adp。扮演地主时，PerfectDou倾向于控制游戏，甚至农民出更多的炸弹；(ii)两个农民可以更好地合作，减少地主的控制时间和玩炸弹的机会。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table5.png" alt></p>
<h3 id="案例研究：对比DouZero与PerfectDou的行为"><a href="#案例研究：对比DouZero与PerfectDou的行为" class="headerlink" title="案例研究：对比DouZero与PerfectDou的行为"></a>案例研究：对比DouZero与PerfectDou的行为</h3><p>​    DouZero更有侵略性，但考虑更少。第一个观察结果是，DouZero在不考虑剩下手牌的情况下非常具有攻击性。 例如，如图6所示，在一开始DouZero选择了一个单打链，但留下了一对3，这可能是危险的，因为这对3是最小的牌之一，不能压制任何牌; 图7是另一个有力的例子，在图7中，DouZero选择了单顺来压制对手，而不考虑手中留下许多单牌的后果。 相反，PerfectDou则更加保守和稳健。 我们认为提出的完全信息蒸馏机制有助于PerfectDou以更合理的方式推断全局信息。  </p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure6.png" alt></p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure7.png" alt></p>
<p>​    PerfectDou更善于猜测和压制。我们观察到在PTIE框架中使用完全的信息蒸馏，通过提前压制对手，会有很大的好处。在图8中显示了当队友出一对10的情况下，DouZero选择通过；相反，PerfectDou选择一对Q压制，刚好是地主的最小对。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure8.png" alt></p>
<p>​    PerfectDou更擅长牌组合。在图9所示的战斗中，PerfectDou在组合牌策略上表现出更好的能力。具体来说，PerfectDou选择分割飞机(999，101010，因为它认为还有一个单顺(910JQK)。然而DouZero只采取了三带一，这将很容易被对手压制。这得益于牌表示的正确设计和PerfectDou的动作特征。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure9.png" alt></p>
<p>​    PerfectDou更冷静。图10描绘了一个典型而有趣的场景，PerfectDou展示了它对全局的冷静和仔细的考虑。在游戏中，上家地主出了单2，手中只剩下8张牌。DouZero似乎很害怕，并分裂了火箭炸弹；然而PerfectDou受益于优势奖励设计，考虑到保留炸弹可以以更高的分数赢得比赛。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure10.png" alt></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​    在本文中，我们提出了一个最强的斗地主AI系统。PerfectDou利用了完全信息训练-不完全信息执行的训练方式，并在分布式训练框架内进行训练。在实验中，我们广泛地研究了PefectDou如何以及为什么能够通过以合理的战略行动击败所有现有的人工智能程序来实现SoTA的性能。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul>
<li>没有谈到合法动作作为特征时如何处理每次手牌合法动作数量不一致的问题，会导致网络结构的输入不一样。在实现时是设置了最大合法动作数量？</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/" class="post-title-link" itemprop="url">Curiosity-driven Exploration by Self-supervised Prediction</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-27 20:55:03" itemprop="dateCreated datePublished" datetime="2021-11-27T20:55:03+08:00">2021-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-29 23:30:00" itemprop="dateModified" datetime="2021-11-29T23:30:00+08:00">2021-11-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    在许多现实世界中，代理外部的奖励是非常稀疏的，或者完全没有。在这种情况下，好奇心可以作为一种内在的奖励信号，使智能体能够探索其环境，并学习可能在以后的生活中有用的技能。我们将好奇心表示为一个代理在self-supervised逆动力学模型学习的视觉特征空间中预测其自身行为的结果的能力的误差。我们的公式可以扩展到图像等高维连续状态空间，绕过了直接预测像素的困难，而且忽略了环境中不能影响代理的方面。该方法在两种环境下进行了评估：VizDoom和Super Mario Bros。研究了三种广泛的情况：1)稀疏的外部奖励，好奇心允许与环境的互动更少，以达到目标；2)没有外部奖励的探索，好奇心推动智能体更有效地探索；3)推广到看不见的场景（例如，同一游戏的新关卡），从早期经验中获得的知识有助于主体探索。</p>
<h1 id="Curiosity-Driven-Exploration"><a href="#Curiosity-Driven-Exploration" class="headerlink" title="Curiosity-Driven Exploration"></a>Curiosity-Driven Exploration</h1><p>​    我们的代理由两个子系统组成：一个奖励生成器，它输出一个好奇心驱动的内在奖励信号和一个输出一系列行动来最大化奖励信号的策略。除了内在奖励外，主体还从可以选择性的从环境中获得一些外在奖励。智能体在时间<script type="math/tex">t</script>产生的内在好奇心奖励为<script type="math/tex">r_{t}^{i}</script>，外在奖励<script type="math/tex">r_{t}^{e}</script>。策略子系统被训练，使这两种奖励的总和<script type="math/tex">r_{t}= r_{t}^{i}+r_{t}^{e}</script>最大化，大多数情况<script type="math/tex">r_{t}^{e}</script>为零。</p>
<p>​    我们用一个参数为<script type="math/tex">\theta_{P}</script>的深度神经网络来表示策略<script type="math/tex">\pi \left ( s_{t}:\theta _{P} \right )</script>。给定智能体状态为<script type="math/tex">s_{t}</script>，它执行从策略中采样的动作<script type="math/tex">a_{t}\sim \pi \left ( s_{t}:\theta _{P} \right )</script>。<script type="math/tex">\theta_{P}</script>使用最大化预期的奖励总和进行优化。</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula1.png" alt></p>
<p>​    在本文讨论的实验中，我们使用asynchronous advantage actor critic策略梯度(A3C)进行策略学习。</p>
<h2 id="Prediction-error-as-curiosity-reward"><a href="#Prediction-error-as-curiosity-reward" class="headerlink" title="Prediction error as curiosity reward"></a>Prediction error as curiosity reward</h2><p>​    在原始感觉空间(例如，当<script type="math/tex">s_{t}</script>对应于图像时)进行预测是不可取的，不仅因为很难直接预测像素，而且因为不清楚预测像素是否是优化的正确目标。假如使用像素空间中的预测误差作为好奇心的奖励，想象一下这样一个场景，代理在微风中观察树叶的运动。由于微风天生很难建模，因此甚至很难预测每个叶子的像素位置。这意味着像素预测误差将保持很高，并且代理将始终对叶子保持好奇。但是叶子的运动对代理是无关紧要的，因此它对它们的持续好奇心是不可取的。潜在的问题是，代理没有意识到状态空间的某些部分根本无法被建模，因此代理可能会陷入一个人为的好奇心陷阱，阻碍其探索。以表格形式记录被访问状态的数量（或它们对连续状态空间的扩展）的寻求新奇的探索方案也存在这个问题。在过去，测量学习进步而不是预测误差已经被提出作为一种解决方案(Schmidhuber, 1991)。不幸的是，目前还没有已知的计算上可行的机制来衡量学习进展。</p>
<p>​    如果不是原始的观测空间，那么什么是正确的特征空间来进行预测，从而使预测误差提供了一个很好的好奇心测量方法呢？为了回答这个问题，让我们将所有可以修改代理观察结果的源分为三种情况：（1）可以由代理控制的东西；（2）代理无法控制但会影响代理（例如由另一个代理驾驶的车辆）（3）超出代理控制而不影响代理的东西（例如正在移动的树叶）。一个很好的好奇心特性空间应该建模（1）和（2），并且不受（3）的影响。后者是因为，如果有一个对代理来说无关紧要的变异源，那么代理就没有动机去知道它。</p>
<h2 id="Self-supervised-prediction-for-exploration"><a href="#Self-supervised-prediction-for-exploration" class="headerlink" title="Self-supervised prediction for exploration"></a>Self-supervised prediction for exploration</h2><p>​    我们的目标是不需要手工设计每个环境的特征表示，而是提出一种学习特征表示的一般机制，使学习特征空间中的预测误差提供一个很好的内在奖励信号。我们提出可以通过具有两个子模块的深度神经网络训练的特征空间：第一个子模块编码原始状态<script type="math/tex">s_{t}</script>成一个特征向量<script type="math/tex">\phi \left ( s_{t} \right )</script>和第二个子模块作为输入时间<script type="math/tex">t</script>和<script type="math/tex">t+1</script>的特征编码<script type="math/tex">\phi \left ( s_{t} \right )</script>，<script type="math/tex">\phi \left ( s_{t+1} \right )</script>，预测使状态从<script type="math/tex">s_{t}</script>到<script type="math/tex">s_{t+1}</script>的动作。训练这个神经网络相当于学习函数<script type="math/tex">g</script>，定义为：</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula2.png" alt></p>
<p>其中<script type="math/tex">\hat{a}_{t}</script>为对动作的预测估计，并对神经网络参数<script type="math/tex">\theta _{I}</script>进行优化</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula3.png" alt></p>
<p>其中<script type="math/tex">L_{I}</script>是衡量预测行动和实际行动之间差异的损失函数。在<script type="math/tex">a_{t}</script>是离散的情况下，<script type="math/tex">g</script>的输出是所有可能动作的soft-max分布，最小化<script type="math/tex">L_{I}</script>相当于多项分布下<script type="math/tex">\theta _{I}</script>的最大似然估计。学习到的函数<script type="math/tex">g</script>也被称为逆动力学模型，当代理使用其当前的策略<script type="math/tex">\pi(s)</script>与环境交互时，可以获得学习<script type="math/tex">g</script>所需的元组<script type="math/tex">(s_{t},a_{t},s_{t+1})</script>。</p>
<p>​    除了逆动力学模型，我们还训练了另一个神经网络，以<script type="math/tex">a_{t}</script>和<script type="math/tex">\phi \left ( s_{t} \right )</script>作为输入，并预测时间步<script type="math/tex">t+1</script>状态的特征编码。</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula4.png" alt></p>
<p>其中，<script type="math/tex">\hat\phi \left ( s_{t+1} \right )</script>)为<script type="math/tex">\phi \left ( s_{t+1} \right )</script>的预测估计值，通过最小化损失函数<script type="math/tex">L_{F}</script>来优化神经网络参数<script type="math/tex">\theta_{F}</script>：</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula5.png" alt></p>
<p>学习到的函数<script type="math/tex">f</script>也被称为前向动力学模型。内在奖励信号<script type="math/tex">r_{t}^{i}</script>的计算方法为</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula6.png" alt></p>
<p>其中，<script type="math/tex">\eta > 0</script>是一个缩放因子。为了生成基于好奇心的内在奖励信号，我们分别共同优化了公式3和公式5中所描述的正向动力学损失和逆动力学损失。逆模型学习一个特征空间，该特征空间只编码与预测代理行为相关的信息，而前向模型在该特征空间中进行预测。我们将好奇心公式定义为Intrinsic Curiosity Module (ICM)。由于该特征空间没有动机对任何不受代理行为影响的环境特征进行编码，因此我们的代理将不会因达到本质上不可预测的环境状态而获得奖励，其探索策略具有鲁棒性。</p>
<p>训练结构如Figure2所示</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/figure2.png" alt></p>
<p>学习智能体的总体优化可以写成</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula7.png" alt></p>
<p>其中<script type="math/tex">0\leq \beta \leq 1</script>是一个衡量逆模型损失与正向模型损失的标量，而<script type="math/tex">\lambda > 0</script>是一个衡量策略梯度损失的重要性与学习内在奖励信号的重要性的标量。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/27/Mastering-the-game-of-Go-without-human-knowledge/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/27/Mastering-the-game-of-Go-without-human-knowledge/" class="post-title-link" itemprop="url">Mastering the game of Go without human knowledge</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-27 11:25:26" itemprop="dateCreated datePublished" datetime="2021-11-27T11:25:26+08:00">2021-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-13 23:36:19" itemprop="dateModified" datetime="2021-12-13T23:36:19+08:00">2021-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    最近，AlphaGo成为第一个在围棋比赛中击败世界冠军的项目。在AlphaGo中进行的树形搜索使用深度神经网络来评估位置和选择移动。这些神经网络是通过对人类专家动作的监督学习和对自我游戏的强化学习来训练的。这里我们介绍了一种仅基于强化学习的算法，没有游戏之外的人类数据、指导或领域知识规则。AlphaGo成为了它自己的老师：一个神经网络被训练来预测AlphaGo自己的移动选择以及AlphaGo游戏的获胜者。该神经网络提高了树状搜索的强度，从而产生了更高质量的移动选择和在下一次迭代中更强的self-play。从白板开始，我们的新程序AlphaGo Zero取得了超人的表现，以100-0战胜了之前发表的冠军AlphaGo。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    我们的程序，AlphaGoZero，在几个重要方面不同于AlphaGo。首先，它是完全通过self-play强化学习进行训练，从随机游戏开始，没有任何监督或使用人类数据。其次，它只使用棋盘上的黑白棋子作为输入特征。第三，它使用了一个单一的神经网络，而不是单独的策略和价值网络。最后，它使用了一个更简单的树形搜索，它依赖于这个单一的神经网络来评估位置和采样动作，而不执行任何Monte Carlo rollouts。为了实现这些结果，我们引入了一种新的强化学习算法，该算法在训练循环中引入了前瞻搜索，从而实现了快速改进和精确稳定的学习。Methods章节进一步描述了搜索算法、训练过程和网络结构的的技术差异。</p>
<h1 id="Reinforcement-learning-in-AlphaGo-Zero"><a href="#Reinforcement-learning-in-AlphaGo-Zero" class="headerlink" title="Reinforcement learning in AlphaGo Zero"></a>Reinforcement learning in AlphaGo Zero</h1><p>​    我们的新方法使用了一个参数为<script type="math/tex">\theta</script>的深度神经网络<script type="math/tex">f_{\theta}</script>。该神经网络将位置及其历史的棋盘表示<script type="math/tex">s</script>作为输入，并输出移动概率和一个值<script type="math/tex">\left ( p,v \right )=f_{\theta }\left ( s \right )</script>。移动概率的向量<script type="math/tex">p</script>表示选择每个move <script type="math/tex">a</script>（包括pass）,<script type="math/tex">p_{a}= Pr\left ( a|s \right )</script>的概率。值<script type="math/tex">v</script>是一个标量评估，估计当前玩家从状态<script type="math/tex">s</script>中获胜的概率。这个神经网络将策略网络和价值网络的作用结合成一个单一的体系结构。该神经网络由卷积层的许多残差块组成，具有批处理归一化和非线性激活函数（见Methods章节）。</p>
<p>​    AlphaGo Zero中的神经网络是通过一种新的强化学习算法从self-play游戏中训练出来的。在每个位置<script type="math/tex">s</script>，在神经网络<script type="math/tex">f_{\theta}</script>的引导下执行MCTS搜索。MCTS搜索输出每个move的概率<script type="math/tex">\pi</script>。神经网络<script type="math/tex">f_{\theta}</script>通常选择比原始移动概率<script type="math/tex">p</script>好得多的move；因此，MCT可被视为一个强有力的policy improvement操作。使用改进的基于MCTS的策略选择每一步，然后使用游戏赢家<script type="math/tex">z</script>作为值的样本进行搜索，可将其视为强大的policy evaluation操作。我们的强化学习算法的主要思想是在策略迭代过程中重复使用这些搜索操作。更新神经网络的参数，使移动概率和值<script type="math/tex">(p,v)=f_{\theta}(s)</script>更接近改进的搜索概率和self-play的赢家<script type="math/tex">(\pi,z)</script>；这些新参数将在下一次的self-play迭代中使用，使搜索更强。图1展示了自我游戏的训练流程。</p>
<p><img src="/2021/11/27/Mastering-the-game-of-Go-without-human-knowledge/figure1.png" alt></p>
<p>​    MCTS使用神经网络<script type="math/tex">f_{\theta}</script>来指导其模拟(见图2)。搜索树中的每个边<script type="math/tex">(s,a)</script>都存储先验概率<script type="math/tex">P(s,a)</script>、访问计数<script type="math/tex">N(s,a)</script>和动作值<script type="math/tex">Q(s,a)</script>。每次模拟都从根状态开始，并迭代地选择使置信上限<script type="math/tex">Q(s,a)+U(s,a)</script>能够最大化的移动，其中<script type="math/tex">U(s,a)\propto P(s,a)/(1+N(s,a))</script>，直到遇到一个叶子节点<script type="math/tex">s^{'}</script>。这个叶子的位置只被网络expanded和evaluated一次，以生成先验概率和评估<script type="math/tex">(P(s^{'},\cdot ),V(s^{'})=f_{\theta}(s^{'})</script>。模拟经过每条边<script type="math/tex">(s,a)</script>更新增加其访问数<script type="math/tex">N(s,a)</script>， 并将其动作值更新为这些模拟的平均值评估<script type="math/tex">Q(s,a)=1/N(s,a)\sum _{s^{'}|s,a\rightarrow s^{'}}V(s^{'})</script>，其中<script type="math/tex">s,a\rightarrow s^{'}</script>表明模拟从<script type="math/tex">s</script>采取动作<script type="math/tex">a</script>后最终达到<script type="math/tex">s^{'}</script>。</p>
<p><img src="/2021/11/27/Mastering-the-game-of-Go-without-human-knowledge/figure2.png" alt></p>
<p>​    MCTS可以被视为一种self-play算法，给定神经网络参数<script type="math/tex">\theta</script>和根位置<script type="math/tex">s</script>，计算搜索概率向量<script type="math/tex">\pi=\alpha_{\theta}(s)</script>，与每次移动的访问次数指数成比例，<script type="math/tex">\pi _{a}\propto N\left ( s,a \right )^{1/\tau }</script>，其中τ是一个温度参数。    </p>
<p>​    神经网络由一个self-play强化学习算法来训练，该算法使用MCTS来选择每个动作。首先将神经网络初始化为随机权值<script type="math/tex">\theta_{0}</script>。在随后的每次迭代<script type="math/tex">i\geq 1</script>中，都生成self-play(图1a)。在每个时间步长<script type="math/tex">t</script>中，使用神经网络fθi−1的上一次迭代来执行一个MCTS搜索π=αθ−()tti1，并通过对搜索概率πt进行采样来执行一个动作。当两个玩家都通过游戏，当搜索值低于辞职阈值或游戏超过最大长度时，游戏在步骤T步结束；然后对游戏进行评分，给予rT∈{−1，+1}的最终奖励（详见方法）。每个时间步t的数据存储为(st，πt，zt)，其中zt=±从t步当前玩家的角度来看是游戏赢家。并行地(图。1b)，从自游戏(s)最后一次迭代的所有时间步长中均匀采样的数据(s、π、z)中训练新的网络参数θi。对神经网络=θpvfs（，）(i)进行调整，使预测值v与自玩赢家z之间的误差最小化，并使神经网络移动概率p与搜索概率的相似性最大化</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">QilongPan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">53</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QilongPan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
