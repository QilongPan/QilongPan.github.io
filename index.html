<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="潘其龙">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="潘其龙">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="QilongPan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>潘其龙</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">潘其龙</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/" class="post-title-link" itemprop="url">Enhanced Rolling Horizon Evolution Algorithm with Opponent Model Learning Results for the Fighting Game AI Competition</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-04-15 22:30:07 / Modified: 22:33:06" itemprop="dateCreated datePublished" datetime="2022-04-15T22:30:07+08:00">2022-04-15</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the"><a href="#Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the" class="headerlink" title="Enhanced Rolling Horizon Evolution Algorithm with Opponent Model Learning: Results for the"></a>Enhanced Rolling Horizon Evolution Algorithm with Opponent Model Learning: Results for the</h2><p>Fighting Game AI Competition</p>
<p>格斗游戏是一项极具挑战的<strong>双人实时对抗人工智能博弈任务，常具有组合动作空间规模大、角色属性风格多样、实时性计算要求高等特点，</strong>受到了研究人员的广泛关注。对此，日本立命馆大学发布了<strong>双人格斗游戏AI实时对抗平台FightingICE</strong> ，其已成为IEEE Conference on Games（游戏人工智能旗舰会议）的格斗游戏AI竞赛赛道的指定专用测试平台。FightingICE要求参赛算法必须自主适应三种不同类型的格斗角色属性，在有限时间内通过控制本方智能体的格斗动作，快速击败对方算法控制的智能体。不同于回合制双人零和博弈问题，实时双人零和博弈问题属于非完美信息博弈，在同步决策的过程中无法准确获取对方正要采取的动作行为，进而影响智能体的有效决策行为。</p>
<p>此外，我们提出的基于策略梯度的对手模型的机器人是唯一一个没有使用Monte-Carlo树搜索(MCTS)在2019年的竞争中获得第二名的前五名机器人，同时使用的领域知识比赢家少得多。</p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/fightingIcescreenshot.png" alt></p>
<p>​    针对上述存在的问题与挑战，我们提出了一种新型的<strong>基于自适应对手建模的增强型滚动时域演化算法</strong>，并且将滚动时域演化算法首次应用到格斗游戏AI领域。同时，提出了一系列监督强化式对手模型优化方法（包括交叉熵型、Q学习型和策略梯度型等）用于优化对手模型的预测有效性。具体实现为根据格斗对抗时生成的历史交战信息作为训练数据源，通过在线优化的方式滚动更新对手模型参数，增强前向模型推理规划的可靠有效性，从而进一步增强滚动时域演化算法的博弈推理表现性能。</p>
<h3 id="RHEA"><a href="#RHEA" class="headerlink" title="RHEA"></a>RHEA</h3><p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/rhea.png" alt></p>
<p>Population由多个代表不同动作序列的个体组成。该序列中的每个动作都被视为一个gene。你会根据合适的fitnesses(适应度)来选择一定数量的个体。然后，这些个体被分配到交叉的过程中，并且有可能发生突变，从而产生更多的潜在后代或更强大的后代。然后，将个体作为动作序列展开，并将动作序列有序地输入到正向模型中，以推断未来的状态。用score function对未来的状态进行评估，以获得新的适应度。将重复上述优化过程，直到消耗完时间预算。</p>
<p>在我们实现RHEA时，每个个体中的动作序列都是随机初始化的。 初始化后，创建一组长度相同的个体，每个个体都由适应度函数进行评估。</p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/fitness_function.png" alt></p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/fitness_func_explain.png" alt></p>
<p><script type="math/tex">f_{sco}</script>是当前状态的价值进行评估。源码中实现了两种方式，一种是采用MC rollout评估，一种是启发式评估。</p>
<p><script type="math/tex">f_{div}</script>是用于探索。<script type="math/tex">f_{count}</script>是计算每个基因在总的基因里对应的频率，出现次数越多<script type="math/tex">f_{count}</script>越大。<script type="math/tex">\sum_{j=1}^{l}f_{count}(\vec{z}^{l}(j)))</script>表示个体每个基因出现频率的和。<script type="math/tex">n</script>为个体数量,<script type="math/tex">l</script>为个体基因长度</p>
<p><script type="math/tex">f_{FM}</script>是前向推理模型，从一个状态到下一个状态</p>
<p><script type="math/tex">\lambda</script>是平衡状态价值和diversity。类似于利用和探索</p>
<h3 id="RHEAOM"><a href="#RHEAOM" class="headerlink" title="RHEAOM"></a>RHEAOM</h3><p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/rheaom.png" alt></p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/nextgeneration.png" alt></p>
<p>假设我们在当前这一代中总共有<script type="math/tex">n</script>个个体。得分最高的<script type="math/tex">k</script>个人被选为精英，并保存在下一代。剩下的<script type="math/tex">n-k</script>个体进化通过与精英进行交叉，父母分别从精英和剩下的个体中随机抽取。然后，从个体中随机选择一个基因，并通过均匀分布变异成另一个有效基因。最后，通过fitness function(适应度函数)来重新评估这些最新的个体。如果还有时间预算，请为下一代选择前n个排序的个体，然后再次重复上述演变。否则，从排序最高的个体执行第一个操作。算法1给出了基于对手模型的滚平演化算法的整个过程。</p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/rheaom_fig.png" alt></p>
<h3 id="Opponent-Model"><a href="#Opponent-Model" class="headerlink" title="Opponent Model"></a>Opponent Model</h3><p>网络结构需要简单,只有输入输出层，没有隐藏层</p>
<p>此外，其他更复杂的网络架构，如多层感知器和长期短期内存网络，已经进行了测试，但其性能比简单的架构要差。</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>使用不同对手模型的测试结果</p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/result1.png" alt></p>
<p>RHEA(%)表示对手不做任何动作,-R对手随机，-SL对手模型使用监督学习,-Q对手模型使用Q-learning，-PG对手使用policy gradient。</p>
<p>和其它对手的对打测试结果</p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/winrate_other_agent.png" alt></p>
<p>RHEA和MCTS结合对手模型对打测试</p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/rhea_vs_mcts.png" alt></p>
<p>RHEA与对手模型结合比MCTS要好，并且MCTS结合对手模型比单纯使用MCTS要好</p>
<p>2019年FightingICE比赛结果</p>
<p><img src="/2022/04/15/Enhanced-Rolling-Horizon-Evolution-Algorithm-with-Opponent-Model-Learning-Results-for-the-Fighting-Game-AI-Competition/match_result_2019.png" alt></p>
<p>进化算法的目标与强化学习优化的目标都是预期奖励。但是，强化学习是将噪声注入动作空间并使用反向传播来计算参数更新，而进化策略则是直接向参数空间注入噪声。换个说话，强化学习是在「猜测然后检验」动作，而进化策略则是在「猜测然后检验」参数。因为我们是在向参数注入噪声，所以就有可能使用确定性的策略（而且我们在实验中也确实是这么做的）。也有可能同时将噪声注入到动作和参数中，这样就有可能实现两种方法的结合。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/15/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/15/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">进化算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-15 22:24:40" itemprop="dateCreated datePublished" datetime="2022-04-15T22:24:40+08:00">2022-04-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-04-27 22:16:03" itemprop="dateModified" datetime="2022-04-27T22:16:03+08:00">2022-04-27</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="进化算法"><a href="#进化算法" class="headerlink" title="进化算法"></a>进化算法</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>进化算法是模拟生物在自然界中的进化。达尔文的进化论指出“<strong>物竞天择，适者生存</strong>”。进化论几乎可以解释一切“为什么这种生物是这样的？”这一类的问题。那些更加适应环境的生物，更加容易留下自己的<strong>染色体</strong>。于是，在计算机中，我们模拟生物中的交叉，变异，选择；</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u010451580/article/details/51178225">遗传算法</a></p>
<h2 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h2><ul>
<li>不知道如何解决一个问题时。列出约束条件。</li>
<li>问题是不断变化的，需要一个自适应的解。比如预测股市。</li>
<li>需要搜索大量的或者无限的可能解，来找到最好的或足够好的解。本质上进化算法可以看成搜索算法搜索一组可能的解，来寻找最好的或最适合的解。</li>
<li><p>可以接受足够好的解</p>
</li>
<li><p>可用于解决数值优化、组合优化、机器学习、智能控制、人工生命、图像处理、模式识别等领域的问题。</p>
</li>
</ul>
<p>进化算法包括遗传算法、进化程序设计、进化规划和进化策略等等，进化算法的基本框架还是简单遗传算法所描述的框架，但在进化的方式上有较大的差异，选择、交叉、变异、种群控制等有很多变化。</p>
<p>比如需要得到字符全为1的字符串，比如”1111”，个体可以理解为候选解，比如“1001”；种群可以理解为候选解的集合{“1001”,”1101”……..}</p>
<p><img src="/2022/04/15/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/基本术语.png" alt></p>
<p>交叉:它是生物繁殖的模拟。<a target="_blank" rel="noopener" href="https://blog.csdn.net/ztf312/article/details/82793295">交叉算子</a></p>
<p>变异:新的遗传信息添加到基因组中的过程。<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_30239361/article/details/101686764">变异算子</a></p>
<h2 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h2><p>一般遗传算法过程</p>
<p><img src="/2022/04/15/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/遗传算法过程.png" alt></p>
<p>遗传算法参数：</p>
<ul>
<li>变异率是在解的染色体中，特定基因将要变异的概率。较高的变异率允许种群中有更多的遗传多样性，有助于算法避免局部最优。同样也会导致每一代之间太多的遗传变异，导致失去以前种群中良好的解。变异率低算法可能用过长时间在搜索空间中移动，妨碍找到满意解的能力。</li>
<li>种群规模：遗传算法中任意一代种群的个体数；较大的种群规模算法可以在搜索空间中取样更多，这将有助于将它导向更准确、全局最优解的方向。小的种群规模通常会导致该算法在搜索空间的局部最优区域发现不太理想的解，但是它们每一代需要的计算资源较少；</li>
<li>交叉率。高交叉率在交叉阶段会产生许多新的、潜在优越的解。较低的交叉率有助于保持较适应个体的基因信息在下一代中不受破坏。</li>
<li>精英数量</li>
</ul>
<h2 id="简单实现案例"><a href="#简单实现案例" class="headerlink" title="简单实现案例"></a>简单实现案例</h2><p>该案例是用于生成指定长度的全1字符串，比如”1111111”。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">使用进化算法生成全是1的字符串，比如&quot;1111111111&quot;。最开始字符串是随机的0或者1组成，通过算法得到全1的字符串</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">class Individual(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self,chromosomeLength):</span><br><span class="line">        self.fitness &#x3D; -1</span><br><span class="line">        self.choromosome &#x3D; [ 0 for i in range(chromosomeLength)]</span><br><span class="line">        for gene_index in range(chromosomeLength):</span><br><span class="line">            if (0.5 &lt; random.random()):</span><br><span class="line">                self.setGene(gene_index,1)</span><br><span class="line">            else:</span><br><span class="line">                self.setGene(gene_index,0)</span><br><span class="line">    </span><br><span class="line">    def getChromosome(self):</span><br><span class="line">        return self.choromosome</span><br><span class="line">    </span><br><span class="line">    def getChromosomeLength(self):</span><br><span class="line">        return len(self.choromosome)</span><br><span class="line"></span><br><span class="line">    def setGene(self,offset,gene):</span><br><span class="line">        self.choromosome[offset] &#x3D; gene</span><br><span class="line"></span><br><span class="line">    def getGene(self,offset):</span><br><span class="line">        return self.choromosome[offset]</span><br><span class="line"></span><br><span class="line">    def setFitness(self,fitness):</span><br><span class="line">        self.fitness &#x3D; fitness</span><br><span class="line">    </span><br><span class="line">    def getFitness(self):</span><br><span class="line">        return self.fitness</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        output &#x3D; &quot;&quot;</span><br><span class="line">        for i in range(len(self.choromosome)):</span><br><span class="line">            output +&#x3D; str(self.choromosome[i])</span><br><span class="line">        return output</span><br><span class="line"></span><br><span class="line">class Population(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self,populationSize,chromosomeLength &#x3D; -1):</span><br><span class="line">        self.populationFitness &#x3D; -1</span><br><span class="line">        self.population &#x3D; []</span><br><span class="line">        if chromosomeLength &#x3D;&#x3D; -1:</span><br><span class="line">            pass</span><br><span class="line">        else:</span><br><span class="line">            for i in range(populationSize):</span><br><span class="line">                self.population.append(Individual(chromosomeLength))</span><br><span class="line">    </span><br><span class="line">    def getIndividuals(self):</span><br><span class="line">        return self.population</span><br><span class="line">    </span><br><span class="line">    def getFittest(self,offset):</span><br><span class="line">        self.population.sort(key &#x3D; lambda ele:ele.getFitness(),reverse&#x3D;True)</span><br><span class="line">        return self.population[offset]</span><br><span class="line"></span><br><span class="line">    def setPopulationFitness(self,fitness):</span><br><span class="line">        self.populationFitness &#x3D; fitness</span><br><span class="line">    </span><br><span class="line">    def getPopulationFitness(self):</span><br><span class="line">        return self.populationFitness</span><br><span class="line"></span><br><span class="line">    def size(self):</span><br><span class="line">        return len(self.population)</span><br><span class="line"></span><br><span class="line">    def setIndividual(self,offset,individual):</span><br><span class="line">        self.population[offset] &#x3D; individual </span><br><span class="line">    </span><br><span class="line">    def getIndividual(self,offset):</span><br><span class="line">        return self.population[offset]</span><br><span class="line"></span><br><span class="line">    def shuffle(self):</span><br><span class="line">        for i in range(len(self.population)-1,-1,-1):</span><br><span class="line">            index &#x3D; random.randint(i)</span><br><span class="line">            individual_tmp &#x3D; self.population[index]</span><br><span class="line">            self.population[index] &#x3D; self.population[i]</span><br><span class="line">            self.population[i] &#x3D; individual_tmp</span><br><span class="line"></span><br><span class="line">class GeneticAlgorithm(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self,populationSize,mutationRate,crossoverRate,elitismCount):</span><br><span class="line">        self.populationSize &#x3D; populationSize</span><br><span class="line">        self.mutationRate &#x3D; mutationRate</span><br><span class="line">        self.crossoverRate &#x3D; crossoverRate</span><br><span class="line">        self.elitismCount &#x3D; elitismCount</span><br><span class="line"></span><br><span class="line">    def initPopulation(self,chromosomeLength):</span><br><span class="line">        population &#x3D; Population(self.populationSize,chromosomeLength)</span><br><span class="line">        return population</span><br><span class="line">    </span><br><span class="line">    def calcFitness(self,individual):</span><br><span class="line">        correctGenes &#x3D; 0</span><br><span class="line">        for i in range(individual.getChromosomeLength()):</span><br><span class="line">            if individual.getGene(i) &#x3D;&#x3D; 1:</span><br><span class="line">                correctGenes +&#x3D; 1</span><br><span class="line">        fitness &#x3D; correctGenes &#x2F; individual.getChromosomeLength()</span><br><span class="line">        individual.setFitness(fitness)</span><br><span class="line">        return fitness</span><br><span class="line"></span><br><span class="line">    def evalPopulation(self,population):</span><br><span class="line">        populationFitness &#x3D; 0</span><br><span class="line">        for individual in population.getIndividuals():</span><br><span class="line">            populationFitness +&#x3D; self.calcFitness(individual)</span><br><span class="line">        population.setPopulationFitness(populationFitness)</span><br><span class="line"></span><br><span class="line">    def isTerminationConditionMet(self,population):</span><br><span class="line">        for individual in population.getIndividuals():</span><br><span class="line">            if individual.getFitness() &#x3D;&#x3D; 1:</span><br><span class="line">                return True </span><br><span class="line">        return False </span><br><span class="line"></span><br><span class="line">    def selectParent(self,population):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        根据每个个体的适应度权重进行选择</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        individuals &#x3D; population.getIndividuals()</span><br><span class="line">        populationFitness &#x3D; population.getPopulationFitness()</span><br><span class="line">        rouletteWheelPosition &#x3D; random.random() * populationFitness</span><br><span class="line">        spinWheel &#x3D; 0</span><br><span class="line">        for individual in individuals:</span><br><span class="line">            spinWheel +&#x3D; individual.getFitness()</span><br><span class="line">            if spinWheel &gt;&#x3D; rouletteWheelPosition:</span><br><span class="line">                return individual</span><br><span class="line">        return individuals[population.size() - 1]</span><br><span class="line">    </span><br><span class="line">    def crossoverPopulation(self,population):</span><br><span class="line">        newPopulation &#x3D; Population(population.size())</span><br><span class="line">        for populationIndex in range(population.size()):</span><br><span class="line">            parent1 &#x3D; population.getFittest(populationIndex)</span><br><span class="line">            if self.crossoverRate &gt; random.random() and populationIndex &gt; self.elitismCount:</span><br><span class="line">                offspring &#x3D; Individual(parent1.getChromosomeLength())</span><br><span class="line">                parent2 &#x3D; self.selectParent(population)</span><br><span class="line">                for geneIndex in range(parent1.getChromosomeLength()):</span><br><span class="line">                    if 0.5 &gt; random.random():</span><br><span class="line">                        offspring.setGene(geneIndex,parent1.getGene(geneIndex))</span><br><span class="line">                    else:</span><br><span class="line">                        offspring.setGene(geneIndex,parent2.getGene(geneIndex))</span><br><span class="line">                newPopulation.population.append(offspring)</span><br><span class="line">            else:</span><br><span class="line">                newPopulation.population.append(parent1)</span><br><span class="line">        return newPopulation</span><br><span class="line"></span><br><span class="line">    def mutatePopulation(self,population):</span><br><span class="line">        newPopulation &#x3D; Population(self.populationSize)</span><br><span class="line">        for populationIndex in range(population.size()):</span><br><span class="line">            individual &#x3D; population.getFittest(populationIndex)</span><br><span class="line">            for geneIndex in range(individual.getChromosomeLength()):</span><br><span class="line">                if populationIndex &gt;&#x3D; self.elitismCount:</span><br><span class="line">                    if self.mutationRate &gt; random.random():</span><br><span class="line">                        newGene &#x3D; 1</span><br><span class="line">                        if individual.getGene(geneIndex) &#x3D;&#x3D; 1:</span><br><span class="line">                            newGene &#x3D; 0</span><br><span class="line">                        individual.setGene(geneIndex,newGene)</span><br><span class="line">            newPopulation.append(individual)</span><br><span class="line">        return newPopulation</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    ga &#x3D; GeneticAlgorithm(100,0.5,0.95,30)</span><br><span class="line">    population &#x3D; ga.initPopulation(200)</span><br><span class="line">    ga.evalPopulation(population)</span><br><span class="line">    generation &#x3D; 1</span><br><span class="line">    while not ga.isTerminationConditionMet(population):</span><br><span class="line">        print(f&quot;Best solution &#123;population.getFittest(0)&#125;&quot;)</span><br><span class="line">        population &#x3D; ga.crossoverPopulation(population)</span><br><span class="line">        ga.evalPopulation(population)</span><br><span class="line">        generation +&#x3D; 1</span><br><span class="line">    print(f&quot;Found solution in &#123;generation&#125; generations&quot;)</span><br><span class="line">    print(f&quot;Best solution &#123;population.getFittest(0)&#125;&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">多智能体强化学习算法总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-15 22:12:57" itemprop="dateCreated datePublished" datetime="2022-04-15T22:12:57+08:00">2022-04-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-04-27 22:15:56" itemprop="dateModified" datetime="2022-04-27T22:15:56+08:00">2022-04-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93/" itemprop="url" rel="index"><span itemprop="name">多智能体</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>​    强化学习的核心思想是“试错”（trial-and-error）：智能体通过与环境的交互，根据获得的反馈信息迭代地优化。在 RL 领域，待解决的问题通常被描述为<strong>马尔科夫决策过程</strong></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/concept_1.png" alt></p>
<p>​    当同时存在多个智能体与环境交互时，整个系统就变成一个多智能体系统（multi-agent system）。每个智能体仍然是遵循着强化学习的目标，也就是是最大化能够获得的累积回报，而此时环境全局状态的改变就和所有智能体的联合动作（joint action）相关了。因此在智能体策略学习的过程中，需要考虑联合动作的影响。</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/concept_2.png" alt></p>
<p>马尔科夫决策过程拓展到多智能体系统，被定义为马尔科夫博弈。在马尔科夫博弈中，所有智能体根据当前的环境状态（或者是观测值）来同时选择并执行各自的动作，该各自动作带来的联合动作影响了环境状态的转移和更新，并决定了智能体获得的奖励反馈。它可以通过元组 <script type="math/tex">\left \langle S,A_{1},...,A_{N},T,R_{1},...,R_{n} \right \rangle</script>来表示，其中<script type="math/tex">S</script>表示状态集合，<script type="math/tex">A_{i}</script>和<script type="math/tex">R_{i}</script>分别表示智能体<script type="math/tex">i</script>的动作集合和奖励集合，T 表示环境状态转移概率。</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/concept_3.png" alt></p>
<p>​    一个完全合作式的多智能体任务(我们有<script type="math/tex">n</script>个智能体，这<script type="math/tex">n</script>个智能体需要相互配合以获取最大奖励)可以描述为<strong>去中心化的部分可观测马尔可夫决策模型</strong>(<code>Dec-POMDP</code>)，通常用一个元组<img src="https://www.zhihu.com/equation?tex=G" alt="[公式]">来表示：</p>
<script type="math/tex; mode=display">
G=\left \langle S,U,P,r,Z,O,n,\gamma  \right \rangle</script><p>其中<script type="math/tex">s\in S</script>表示环境的真实状态信息。在每一个时间步，对于每个智能体<script type="math/tex">a\in A\equiv \left \{ 1,...,n \right \}</script>都需要去选择一个动作去<script type="math/tex">u^{a}\in U</script>组成一个联合动作<script type="math/tex">u\in U\equiv U^{n}</script>这个联合动作给到环境中去进行状态转移:<script type="math/tex">P\left ( s^{'}|s,u \right ):S\times U\times S\rightarrow \left [ 0,1 \right ]</script>。之后，所有的智能体都会收到一个相同的奖励:<script type="math/tex">r\left ( s,u \right ):S\times U\rightarrow \mathbb{R}</script>。与单智能体一样<script type="math/tex">\gamma</script>表示折扣因子。</p>
<p>对于每个单智能体<script type="math/tex">a</script>来说，它接收的是一个独立的部分可观测的状态<script type="math/tex">z \in Z</script>，不同的智能体<script type="math/tex">a</script>具备不同的观测，但是所有的观测都来自环境的真实状态信息，所以可以用函数表示为：<script type="math/tex">O\left ( s,a \right ):S\times A\rightarrow Z</script>。对于每个智能体<script type="math/tex">a</script>它都有一个动作观测历史<script type="math/tex">\tau ^{a}\in T\equiv \left ( Z\times U \right )^{*}</script>，基于这个动作-观测的历史来构建随机策略函数<script type="math/tex">\pi ^{a}\left ( u^{a}|\tau ^{a} \right ):T\times U\rightarrow \left [ 0,1 \right ]</script>。联合动作策略<script type="math/tex">\pi</script>是基于状态信息<script type="math/tex">s_{t}</script>构建的联合动作值函数<script type="math/tex">Q^{\pi }\left ( s_{t},u_{t} \right )= E_{s_{t+1}:\infty ,u_{t+1}:\infty }\left [ R_{t}|s_{t},u_{t} \right ]</script>其中<script type="math/tex">R_{t}= \sum_{i=0}^{\infty }\gamma ^{i}r_{t+i}</script>是折扣回报。</p>
<h2 id="传统RL算法在协同任务中的不足"><a href="#传统RL算法在协同任务中的不足" class="headerlink" title="传统RL算法在协同任务中的不足"></a>传统RL算法在协同任务中的不足</h2><p>若使用传统的RL算法来解决多智能体的问题，则会存在以下三个不足之处：</p>
<ul>
<li><p>输入的 action space 应该是所有 agent 的 <strong>联合动作空间</strong>（joint action space），这个空间会随着 agent 数量增加而增加。</p>
</li>
<li><p>由于部分可观测性（即单个agent在某一时刻只能观测到部分环境的信息，无法获得全局信息，比如一个小兵只能看到视野范围内的地图信息，视野外的地图信息是无法观测的），使得 agent 在做决策时只能依照自己当前的部分观测信息（local observation），没有与其他 agent 进行信息共享的能力。</p>
</li>
<li><p>使用联合动作空间获得的 reward 是来自所有 agent 采取的所有 action 共同得到的reward，这就很难知道每一个 agent 的 action 应该得到的多少子回报。</p>
</li>
</ul>
<h2 id="IQL"><a href="#IQL" class="headerlink" title="IQL"></a>IQL</h2><p>IQL算法中将其余智能体直接看作环境的一部分，也就是对于每个智能体<script type="math/tex">a</script>都是在解决一个单智能体任务，很显然，由于环境中存在智能体，因此环境是一个非稳态的，这样就无法保证收敛性了，并且智能体会很容易陷入无止境的探索中，但是在工程实践上，效果还是比较可以的。</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_2.png" alt></p>
<p>是否可以关闭经验回放</p>
<p>使用循环神经网络处理部分可观察</p>
<h2 id="VDN"><a href="#VDN" class="headerlink" title="VDN"></a>VDN</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>​    本文是考虑协作任务的多智能体强化学习问题，即所有的智能体共享同一个奖励值，VDN是一种基于值函数的方法。智能体共享团队奖励会带来“credit assignment ” 问题，即利用该团队奖励拟合出的值函数不能评价每个智能体的策略对整体的贡献。作者认为，由于每个智能体都是局部观测，那么对其中一个智能体来说，其获得的团队奖励很有可能是其队友的行为导致的。也就是说该奖励值对该智能体来说，是“<strong>虚假奖励 (spurious reward signals)</strong>”。因此，每个智能体独立使用强化学习算法学习 (即independent RL) 往往效果很差。</p>
<p>这种虚假奖励还会伴随一种现象，作者称作 “<strong>lazy agent (惰性智能体)</strong>”。当团队中的部分智能体学习到了比较好的策略并且能够完成任务时，其它智能体不需要做什么也能获得不错的团队奖励，这些智能体就被称作“惰性智能体”。</p>
<p>其实不管是“虚假奖励”还是“惰性智能体”，本质上还是credit assignment问题。如果每个智能体都会根据自己对团队的贡献，优化各自的目标函数，就能够解决上述问题。基于这样的动机，作者提出了“值函数分解”的研究思路，将团队整体的值函数分解成<script type="math/tex">N</script>个子值函数，分别作为各智能体执行动作的依据。</p>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_1.png" alt></p>
<p>假设<script type="math/tex">Q\left ( \left ( h^{1},h^{2},...h^{d} \right ),\left ( a^{1},a^{2},...,a^{d} \right ) \right )</script>是多智能体团队的整体Q函数,<script type="math/tex">d</script>是智能体个数，<script type="math/tex">h^{i}</script>是智能体<script type="math/tex">i</script>的历史序列信息,<script type="math/tex">a^{i}</script>是其动作。该Q函数的输入集中了所有智能体的观测和动作，可通过团队奖励<script type="math/tex">r</script>来迭代拟合。为了得到各个智能体的值函数，作者提出如下假设：系统的联合动作值函数可以分解为多个代理值函数相加。<script type="math/tex">\tilde{Q}_{i}</script>只依赖于局部观察</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_7.png" alt></p>
<p>该假设表明团队的Q函数可以通过求和的方式近似分解成<script type="math/tex">d</script>个子Q函数，分别对应<script type="math/tex">d</script>个不同的智能体，且每个子Q函数的输入为该对应智能体的局部观测序列和动作，互相不受影响。每个智能体就有了自己的值函数，它们就可以根据自己的局部值函数来进行决策。<script type="math/tex">\tilde{Q}_{i}\left ( h^{i},a^{i} \right )</script>并不是任何严格意义上的Q值函数。因为并没有理论依据表明一定存在一个reward函数，使得该<script type="math/tex">\tilde{Q}_{i}</script>满足贝尔曼方程。</p>
<p>上面等式成立需要满足<script type="math/tex">r\left ( s,a \right )=\sum_{i=1}^{d}r\left ( o^{i},a^{i} \right )</script>,其中<script type="math/tex">s</script>表示系统全局状态,<script type="math/tex">a</script>表示智能体联合动作。<script type="math/tex">r\left ( s,a \right )=\sum_{i=1}^{d}r\left ( o^{i},a^{i} \right )</script>表明团队的整体奖励应由所有智能体各自的奖励函数求和得到。然而，即使这个条件成立，根据文章中的证明，Q函数的分解也应该写成<script type="math/tex">Q\left ( s,a \right )=\sum_{i=1}^{d}Q_{i}\left ( s,a \right )</script>,子Q函数的输入应该还是全局状态<script type="math/tex">s</script>和联合动作<script type="math/tex">a</script>，而不是上式中的形式。所以，此处的bug是VDN算法的硬伤所在，作者也因此将每个智能体历史状态、动作、奖励的序列信息作为其值函数<script type="math/tex">\tilde{Q}_{i}</script>的输入，以此弥补局部观测上的不足。</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_8.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_9.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_2.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_3.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_4.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_5.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/vdn_6.png" alt></p>
<h2 id="QMIX"><a href="#QMIX" class="headerlink" title="QMIX"></a>QMIX</h2><p>VDN算法考虑将<script type="math/tex">Q_{tot}</script>分解成<script type="math/tex">Q_{i}</script>，以<script type="math/tex">Q_{i}</script>作为每个智能体的Q值函数来计算最优动作，并以<script type="math/tex">Q_{tot} = \sum_{i=1}^{n}Q_{i}</script>的分解形式端到端地训练网络。然而这种以简单的求和方式对整体值函数进行分解，将会导致网络的函数表达能力受到很大限制，难以拟合出真实的<script type="math/tex">Q_{tot}</script>。</p>
<p>如果我们直接使用普通的神经网络对<script type="math/tex">Q_{tot}</script>进行分解，例如 <script type="math/tex">Q_{tot}= MLP\left ( Q_{1},...,Q_{n} \right )</script> ，倒是可以提升网络的函数表达能力，但会遇到另外一个问题：<strong>非单调性（non-monotonicity）</strong>，导致算法难以保证分布式策略的最优性。所谓<strong>单调性</strong>，就是指通过分布式策略计算出来的动作，和通过整体Q函数计算出来的动作，在“性能最优”上需要保持一致，即</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/qmix_2.png" alt></p>
<p>值函数分解的单调性只需要满足如下条件即可:</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/qmix_3.png" alt></p>
<p><script type="math/tex">Q_{a}</script>代表每个智能体a的值函数</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/qmix_1.png" alt></p>
<p>​    其中<script type="math/tex">\epsilon</script>表示探索的概率,<script type="math/tex">o_{t}^{a}</script>表示t时刻智能体a的观测，<script type="math/tex">u_{t}^{a}</script>表示t时刻智能体a的动作。加粗的<script type="math/tex">u</script>表示联合行动，加粗的<script type="math/tex">\tau</script>表示联合动作-观测历史。</p>
<p>​    混合网络的权值是由单独的hypernetworks产生的。每个hypernetwork以状态s作为输入，生成混合网络的一层的权值。每个超网络由一个线性层组成，然后是一个绝对激活函数，以确保混合网络的权值是非负的。超网络的输出是一个向量，它被重塑为一个适当大小的矩阵。这些bias也是以同样的方式产生的，但并不局限于非负的。最终的偏差是由一个具有ReLU非线性的2层超网络产生的。图2a说明了混合网络和超网络。</p>
<p>​    由于<script type="math/tex">Q_{tot}</script>被允许以非单调的方式依赖于额外的状态信息，因此该状态被超网络使用，而不是直接传递到混合网络中。因此，将s的某些函数通过单调网络一起传递将是过度约束的。相反，超网络的使用使得以任意的方式条件s上的权值成为可能，从而将完整状态s尽可能灵活的集成到联合动作值估计中。</p>
<h2 id="MADDPG"><a href="#MADDPG" class="headerlink" title="MADDPG"></a>MADDPG</h2><p>​    该算法针对连续动作场景，可用于竞争、合作、通信。</p>
<p>​    如果直接使用DDPG算法来学习每个智能体的策略，只考虑自己的观测和动作，将其它智能体看作环境中的一部分，则训练过程会非常不稳定，甚至最终无法收敛。这是因为其它智能体的策略也在不断更新变化，智能体的采样数据并不服从一个稳定的概率分布。所以，作者同样采用了“集中式训练和分布式执行 (centralized training with decentralized execution)”的学习框架，允许智能体在训练的过程中获取更多其它智能体的数据，其算法结构如下图所示:</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/maddpg_1.png" alt></p>
<p>actor更新:</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/maddpg_4.png" alt></p>
<p>critic更新：</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/maddpg_3.png" alt></p>
<p><script type="math/tex">\mu ^{'}= \left \{ \mu _{\theta _{1}^{'}},...,\mu _{\theta _{N}^{'}} \right \}</script>为具有延时参数的target policy set</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/maddpg_2.png" alt></p>
<h2 id="COMA"><a href="#COMA" class="headerlink" title="COMA"></a>COMA</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/346076252">参考1</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361648110">参考2</a></p>
<p>VDN是一种基于值函数的方法，而COMA基于策略梯度。在介绍VDN的时候，我们提到过智能体共享团队奖励会带来“credit assignment ” 问题，即利用该团队奖励拟合出的值函数不能评价每个智能体的策略对整体的贡献。在这篇文章中，同样存在这样的问题。该算法是集中式训练，分布式执行。COMA 利用全局评价网络（critic）来评价 Q 值，利用非全局行为网络（actor）来决定 agent 的行为。由于在训练时使用的是全局网络进行评价，并且采用参数共享的方式，使得agent能够在做行为选择的时候参考其他 agent 的状态再做决定，这就加入了“协同”的功能。</p>
<h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><ul>
<li>使用一个集中式critic网络如图1c，在训练的过程中可以获取所有智能体的信息；</li>
</ul>
<p>critic网络的输入包括其它智能体的动作<script type="math/tex">u_{t}^{-a}</script>，全局状态<script type="math/tex">s_{t}</script>，该智能体的局部观测<script type="math/tex">o_{t}^{a}</script>，该智能体的id(one hot形式)，以及上一时刻所有智能体的动作<script type="math/tex">u_{t-1}</script>。这样在critic网络的输出端就能够直接得到该智能体各个动作的反事实Q值。</p>
<ul>
<li>采用反事实基线（counterfactual baseline）来解决信用分配的问题；通过以下公式计算</li>
</ul>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/coma_2.png" alt></p>
<p>为了解决 Individual Reward Assignment 的问题，反事实准则提出，每个agent应该拥有不同的reward，这样才能知道在这一次的全局行为决策中单个agent的action贡献是多少。而单个agent的reward通过两个值计算得来：当前情况下的全局reward和将该agent行为替换为一个默认行为后的全局reward,表示为<script type="math/tex">D^{a}=r\left ( s,u \right )-r\left ( s,\left ( u^{-a},c^{a} \right ) \right )</script>可以这样理解：该回报值其实计算的是Agent <script type="math/tex">a</script>采取行为 <script type="math/tex">u</script> 会比采取默认行为 <script type="math/tex">c_a</script> 要更好（<script type="math/tex">D^a</script> &gt; 0）还是更坏（<script type="math/tex">D^a</script> &lt; 0）。这个<strong>特定agent</strong>下<strong>特定动作</strong>的<strong>reward</strong>就被称为<strong>counterfactual baseline</strong>，COMA使得每一个agent的每一个action都有一个自身的counterfactual baseline。但是要想计算出每一个动作的<script type="math/tex">D^{a}</script>值，就需要将每个动作都替换成默认行为<script type="math/tex">c_{a}</script>去与环境互动一次得到最终结果，这样采样次数会非常多；此外，默认行为的选取也是无法预测的，到底选择哪一个行为当作默认行为才是最合适的也是比较难决定的。因此，文中提出使用”函数拟合”的方式来计算<script type="math/tex">D^{a}</script> 。</p>
<p>前面提到，中心评价网络可以评价一个联合动作空间<script type="math/tex">u</script>在一个状态 s 下的 Q 值。由于默认行为很难定义，于是我们把<strong>采取 “默认行为” 得到的效用值近似为采取一个Agent “所有可能行为” 的效用值总和</strong>。因此， <script type="math/tex">D^{a}</script>就可以用上面等式进行计算</p>
<p>critic网络可以输出在其它智能体动作确定的情况下，自己采取每个动作的Q值。在训练时可以根据当前状态自己采取的action得到<script type="math/tex">Q(s,u)</script>。公式后半部分是对各个动作的Q值求期望，即每个动作的概率(actor网络输出)乘对应的Q值。</p>
<ul>
<li>Critic网络要能够对反事实基线进行高效的计算。</li>
</ul>
<p>尽管baseline的方式解决了独立回报的问题，但是如果要建立一个网络，接收<script type="math/tex">s,u</script>两个输入，输出为所有 agent 的所有 action 的话，那么输出神经元的个数就等于<script type="math/tex">|U|^{n}</script>（n个agent有|U|个动作）。当agent数目很多或动作空间很大的时候就会造成输出层无法实现。为此，COMA构造了一种网络，如下图c所示。</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/coma_1.png" alt></p>
<p>策略梯度更新公式(actor)如下：</p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/coma_3.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/coma_4.png" alt></p>
<p><img src="/2022/04/15/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/coma_5.png" alt></p>
<p>critic更新使用TD error的方式</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361648110">coma</a></p>
<h2 id="QATTEN"><a href="#QATTEN" class="headerlink" title="QATTEN"></a>QATTEN</h2><h2 id="QTRAN"><a href="#QTRAN" class="headerlink" title="QTRAN"></a>QTRAN</h2><h2 id="WQMIX"><a href="#WQMIX" class="headerlink" title="WQMIX"></a>WQMIX</h2><h2 id="MAPPO"><a href="#MAPPO" class="headerlink" title="MAPPO"></a>MAPPO</h2><h2 id="MAD4PG"><a href="#MAD4PG" class="headerlink" title="MAD4PG"></a>MAD4PG</h2><h2 id="RIAL"><a href="#RIAL" class="headerlink" title="RIAL"></a>RIAL</h2><p>关闭经验回放，为了避免不稳定环境下带来的经验失效或误导</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/11/Trust-Region-Policy-Optimization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/11/Trust-Region-Policy-Optimization/" class="post-title-link" itemprop="url">Trust Region Policy Optimization</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-11 21:51:25" itemprop="dateCreated datePublished" datetime="2022-04-11T21:51:25+08:00">2022-04-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/" class="post-title-link" itemprop="url">PerfectDou:Dominating DouDizhu with Perfect Information Distillation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-04-07 21:32:23" itemprop="dateCreated datePublished" datetime="2022-04-07T21:32:23+08:00">2022-04-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-04-27 22:19:38" itemprop="dateModified" datetime="2022-04-27T22:19:38+08:00">2022-04-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E6%A3%8B%E7%89%8C%E6%B8%B8%E6%88%8FAI/" itemprop="url" rel="index"><span itemprop="name">棋牌游戏AI</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><strong>Perfect information distillation</strong>是一个使用完全信息训练，不完全信息执行（perfect-training-imperfect-execution (PTIE)）的actor-critic框架，类似于集中式训练分布式执行，critic利用完全信息（全局信息），actor利用不完全信息（局部信息）。Actor-critic是一个策略梯度(PG)方法的模板，通过一个具有价值函数的PG来最大化策略函数的预期回报：</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/formula1.png" alt></p>
<p>其中s表示RL问题中的一个状态，Q是由一个函数逼近器学习到的state-action值函数，通常称为critic。请注意，critic扮演的角色是只在训练时评估在特定情况下采取的行动的良好程度。当智能体被部署到推理中时，只能使用策略<script type="math/tex">\pi</script>(actor)来获取可行的动作。因此，对于不完全信息游戏，我们可以提供关于玩家的额外信息通过self-play来训练critic，只要actor不使用这些信息来进行决策。直观地说，就是把完全的信息提炼成不完全的策略。可以将公式1重写为：</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/formula2.png" alt></p>
<p><script type="math/tex">h</script>表示为局部信息构成的状态，<script type="math/tex">D(h)</script>（包括额外的信息比如其它玩家手中的牌）表示为全局信息构成的状态。</p>
<p>PTIE是训练不完美信息游戏代理的一种通用方法。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure1.png" alt></p>
<h2 id="PerfectDou系统设计"><a href="#PerfectDou系统设计" class="headerlink" title="PerfectDou系统设计"></a>PerfectDou系统设计</h2><h3 id="card-representation"><a href="#card-representation" class="headerlink" title="card representation"></a>card representation</h3><p>使用<script type="math/tex">12 \times 15</script>的矩阵来表示各种可行的牌的组合，如图2所示。使用<script type="math/tex">4 \times 15</script> 的矩阵来表示牌的数值及数量，列对应15种不同的牌值，第1-4行中每列对应的1的数量表示玩家手牌中该牌值对应的数量。将手牌的合法组合进行编码，合法组合见table6。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure2.png" alt></p>
<p>特征的设计如table1所示，分别给出了不完全特征设计（actor网络使用）和完全的特征设计（critic网络使用）。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table1.png" alt></p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table6.png" alt></p>
<h3 id="网络结构和动作表示"><a href="#网络结构和动作表示" class="headerlink" title="网络结构和动作表示"></a>网络结构和动作表示</h3><p>​    PerfectDou使用PPO作为学习算法。对于价值网络，使用MLP来处理编码的特性。</p>
<p>​    期望价值函数（critic）能够利用全局信息来评估玩家的现状的，可以向critic提供智能体不允许看到的额外信息。如图11所示，价值网络（critic）的不完全特征采用策略网络(actor)中的共享网络进行编码，此外还对策略在博弈过程中无法观察到的完全特征进行了编码。然后将编码的向量连接到一个简单的MLP上，以得到Value。</p>
<p>​    对于策略网络，首先使用LSTM对所有设计的特性进行编码；为了鼓励代理注意特定的卡牌类型，所提出的网络结构将把所有可用的动作编码为特征向量，如table7所示。然后利用动作和游戏特征计算合法动作概率的输出，如图3所示。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure11.png" alt></p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table7.png" alt></p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure3.png" alt></p>
<h3 id="reward-design"><a href="#reward-design" class="headerlink" title="reward design"></a>reward design</h3><p>​    如果只关心游戏结束时的结果，那么奖励相当稀疏；此外玩家只能在游戏中使用不完全的信息来估计他们赢得游戏的优势，这可能是不准确和波动的。为此，我们提出了一个在每个状态上的增强奖励函数。我们不是让玩家单独评估优势，而是使用一个公式来评估每个玩家，特别是出完所有牌所需的最小步骤，这可以被视为对获胜距离的简单估计。奖励函数被定义为由两个阵营在两个连续时间步中获胜的相对距离计算出的优势差。在时间<script type="math/tex">t</script>，奖励函数是：</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/formula4.png" alt></p>
<p>其中<script type="math/tex">l</script>是一个比例因子，<script type="math/tex">N_{t}</script>是在时间步长<script type="math/tex">t</script>上出完所有牌的最小步骤。</p>
<p>​    例如在一局中，在时间戳t，地主获胜的距离是5，两个农民的距离是3和7，这意味着农民有较大的优势，因为农民的相对距离是2，地主是-2。如果地主出牌好，所有农民都不能压制，由于地主的相对距离减少，即从2到1，地主就会得到积极的奖励。相应地，随着农民相对距离的增大，农民会得到负奖励。相反，如果距离为3的农民只是压制房东的牌手，其他农民pass，两个阵营的奖励都是0。这种奖励功能可以鼓励农民之间的合作，因为获胜距离是由双方的最小步数定义的。在实现中，奖励的计算是在一局游戏后进行的，从而提高训练效率。</p>
<h3 id="分布训练细节"><a href="#分布训练细节" class="headerlink" title="分布训练细节"></a>分布训练细节</h3><p>​    为了进一步加快训练过程，设计了一个如图4所示的分布式训练系统。具体来说，该系统包含一组rollout worker，用于收集self-play经验数据并将其发送到gpu池；这些gpu异步接收数据并将其存储到本地缓冲区中。然后，每个学习者从自己的缓冲区中随机抽取小批量样本，并分别计算梯度，然后在所有gpu上同步平均，并反向传播以更新神经网络。在每一轮更新后，新的参数将被发送给每个rollout worker。每个worker将在24步（每个玩家8步后）加载最新的模型。这种解耦的训练采样结构将使PerfectDou能够扩展到大规模的实验中。我们的分布式系统设计大量借鉴了IMPALA它还维持了一组rollout workers来接收更新的模型与环境交互，并将rollout轨迹发送给学习者。主要的区别来自学习算法，本文使用PPO而不是V-trace。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure4.png" alt></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><p>​    使用880个cpu核心和8个gpu构建了一个小型的分布式训练集群。Horovod用于同步GPU之间的梯度，总的batch size 为1024，每个GPU的批处理大小为128。在早期训练阶段，总奖励函数将是一个基本的奖励(WP或ADP)，使用公式4中的奖励来增强，帮助收敛。在后期阶段，增强的奖励将被删除。实验的超参数如table8所示。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table8.png" alt></p>
<p>给定两种算法A对B，我们计算：</p>
<ul>
<li>WP（获胜率）：在一些比赛中A获胜的比例。</li>
<li>ADP（平均得分差）：A和B之间每场比赛的平均得分差。基础得分是1，每个炸弹使得分翻倍。这是一个评估斗地主AI系统的更合理的指标。</li>
</ul>
<p>​    在DouZero中，他们更关注比赛的输赢结果，而不太关心分数。然而，在真正的比赛中，玩家必须玩大量的游戏，并根据他们获胜的分数进行排名。这就是为什么我们认为ADP是评估AI人工智能系统的更好指标，因为一个糟糕的人工智能玩家可以用很少的分数赢得游戏，但失去更多的分数。</p>
<p>​    具体来说，在每个游戏中，地主和农民的基础得分分别是2和1。当游戏中出现炸弹时，每个玩家的分数都会加倍。例如，一个农民玩家首先出一个4的炸弹，然后地主玩家用火箭压制它，然后每个农民的基础分数变成4，地主变成8。玩家在赢得比赛后将赢得所有的分数，或者输掉所有的分数。</p>
<p>​    <strong><em>在我们的实验中，我们选择ADP作为基本奖励，在早期训练阶段，用公式4中提出的奖励信号来增强。</em></strong></p>
<p>​    每两个智能体会测试10000局，忽略了叫分阶段，主要考虑出牌阶段。所有的游戏都是随机生成的，每局游戏将进行两次，即每个竞争算法被分配为地主或农民一次。我们分别使用WP和ADP作为基本奖励来比较所有评估方法的这两个指标，测试结果如table2所示。</p>
<p>​    总体而言，PerfectDou击败了所有现有的人工智能程序，无论是基于规则还是基于学习的算法，在WP和ADP上都具有显著优势。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table2.png" alt></p>
<p>​    与DouZero的训练效率比较如table3所示。可以看出PerfectDou使用1e9个样本训练出的模型强于DouZero使用1e10训练出的模型。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table3.png" alt></p>
<p>​    图12显示了PerfectDou在训练过程中与DouZero最终模型对战的WP和ADP变化。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure12.png" alt></p>
<h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>​    进一步研究PerfectDou成功的关键。具体来说是为了分析特征设计和训练框架是如何帮助PerfectDou主宰斗地竹锦标赛的。为此评估了PerfectDou的不同变体和之前的SoTA AI系统(DouZero)，包括：</p>
<ul>
<li>ImperfectDouZero:DouZero与本文提出的不完全信息特征结合</li>
<li>ImperfectDou：PerfectDou只有不完全的特征作为值函数的输入</li>
<li>RewardlessDou：PerfectDou不使用公式4中的增强reward</li>
<li>Vanilla PPO：简单的actor-critic训练，只具有不完全的特征，而且没有公式4中额外的奖励。</li>
</ul>
<p>结果如table4所示。即使只有不完全的特征，PerfectDou仍然可以用同样的训练步骤轻松击败DouZero；然而DouZero用更多的训练数据来扭转结果。此外，我们提出的特征似乎不适合DouZero获得比原始设计更好的结果。此外在没有公式4的奖励的情况下，PerfectDou仍然以更高的WP击败了DouZero(尽管牺牲了大量的ADP)，这表明了增强奖励在训练中的有效性，如果没有它，就会冒着失去分数赢得一场比赛的风险。最后，如果没有公式4中的增强奖励和完全特征设计用于价值网络，简单的PPO根本不能很好地执行。因此可以得出结论，基于actor-critic的算法和PTIE训练在我们的特征设计下提供了较高的样本效率，并且增强奖励有利于我们的人工智能的合理性。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table4.png" alt></p>
<h3 id="运行时间分析"><a href="#运行时间分析" class="headerlink" title="运行时间分析"></a>运行时间分析</h3><p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure5.png" alt></p>
<p>​    DouZero的平均推理时间为2毫秒，而PerfectDou的平均推理时间为6毫秒。而PerfectDou比DouZero慢一点的原因可能是因为更复杂的特征处理过程。以上分析表明，PerfectDou适用于高级游戏人工智能等现实世界的应用。</p>
<h3 id="深入统计分析"><a href="#深入统计分析" class="headerlink" title="深入统计分析"></a>深入统计分析</h3><p>​    我们对DouZero和PerfectDou之间的游戏进行了深入的分析和收集了统计数据。特别是我们组织了PerfectDou和DouZero之间的游戏，在每个设置的10万局游戏中扮演不同的角色。由于在我们的实验中，角色是分配而不是选择，而且地主有额外的三张牌和有更高的基础分数（例如农民基础分为1，地主基础分为2），从table5中显示的统计数据中可以看出扮演地主总是一个低劣的选择，导致负的adp。扮演地主时，PerfectDou倾向于控制游戏，甚至农民出更多的炸弹；(ii)两个农民可以更好地合作，减少地主的控制时间和玩炸弹的机会。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/table5.png" alt></p>
<h3 id="案例研究：对比DouZero与PerfectDou的行为"><a href="#案例研究：对比DouZero与PerfectDou的行为" class="headerlink" title="案例研究：对比DouZero与PerfectDou的行为"></a>案例研究：对比DouZero与PerfectDou的行为</h3><p>​    DouZero更有侵略性，但考虑更少。第一个观察结果是，DouZero在不考虑剩下手牌的情况下非常具有攻击性。 例如，如图6所示，在一开始DouZero选择了一个单打链，但留下了一对3，这可能是危险的，因为这对3是最小的牌之一，不能压制任何牌; 图7是另一个有力的例子，在图7中，DouZero选择了单顺来压制对手，而不考虑手中留下许多单牌的后果。 相反，PerfectDou则更加保守和稳健。 我们认为提出的完全信息蒸馏机制有助于PerfectDou以更合理的方式推断全局信息。  </p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure6.png" alt></p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure7.png" alt></p>
<p>​    PerfectDou更善于猜测和压制。我们观察到在PTIE框架中使用完全的信息蒸馏，通过提前压制对手，会有很大的好处。在图8中显示了当队友出一对10的情况下，DouZero选择通过；相反，PerfectDou选择一对Q压制，刚好是地主的最小对。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure8.png" alt></p>
<p>​    PerfectDou更擅长牌组合。在图9所示的战斗中，PerfectDou在组合牌策略上表现出更好的能力。具体来说，PerfectDou选择分割飞机(999，101010，因为它认为还有一个单顺(910JQK)。然而DouZero只采取了三带一，这将很容易被对手压制。这得益于牌表示的正确设计和PerfectDou的动作特征。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure9.png" alt></p>
<p>​    PerfectDou更冷静。图10描绘了一个典型而有趣的场景，PerfectDou展示了它对全局的冷静和仔细的考虑。在游戏中，上家地主出了单2，手中只剩下8张牌。DouZero似乎很害怕，并分裂了火箭炸弹；然而PerfectDou受益于优势奖励设计，考虑到保留炸弹可以以更高的分数赢得比赛。</p>
<p><img src="/2022/04/07/PerfectDou-Dominating-DouDizhu-with-Perfect-Information-Distillation/figure10.png" alt></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​    在本文中，我们提出了一个最强的斗地主AI系统。PerfectDou利用了完全信息训练-不完全信息执行的训练方式，并在分布式训练框架内进行训练。在实验中，我们广泛地研究了PefectDou如何以及为什么能够通过以合理的战略行动击败所有现有的人工智能程序来实现SoTA的性能。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul>
<li>没有谈到合法动作作为特征时如何处理每次手牌合法动作数量不一致的问题，会导致网络结构的输入不一样。在实现时是设置了最大合法动作数量？</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/" class="post-title-link" itemprop="url">Curiosity-driven Exploration by Self-supervised Prediction</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-27 20:55:03" itemprop="dateCreated datePublished" datetime="2021-11-27T20:55:03+08:00">2021-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-29 23:30:00" itemprop="dateModified" datetime="2021-11-29T23:30:00+08:00">2021-11-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    在许多现实世界中，代理外部的奖励是非常稀疏的，或者完全没有。在这种情况下，好奇心可以作为一种内在的奖励信号，使智能体能够探索其环境，并学习可能在以后的生活中有用的技能。我们将好奇心表示为一个代理在self-supervised逆动力学模型学习的视觉特征空间中预测其自身行为的结果的能力的误差。我们的公式可以扩展到图像等高维连续状态空间，绕过了直接预测像素的困难，而且忽略了环境中不能影响代理的方面。该方法在两种环境下进行了评估：VizDoom和Super Mario Bros。研究了三种广泛的情况：1)稀疏的外部奖励，好奇心允许与环境的互动更少，以达到目标；2)没有外部奖励的探索，好奇心推动智能体更有效地探索；3)推广到看不见的场景（例如，同一游戏的新关卡），从早期经验中获得的知识有助于主体探索。</p>
<h1 id="Curiosity-Driven-Exploration"><a href="#Curiosity-Driven-Exploration" class="headerlink" title="Curiosity-Driven Exploration"></a>Curiosity-Driven Exploration</h1><p>​    我们的代理由两个子系统组成：一个奖励生成器，它输出一个好奇心驱动的内在奖励信号和一个输出一系列行动来最大化奖励信号的策略。除了内在奖励外，主体还从可以选择性的从环境中获得一些外在奖励。智能体在时间<script type="math/tex">t</script>产生的内在好奇心奖励为<script type="math/tex">r_{t}^{i}</script>，外在奖励<script type="math/tex">r_{t}^{e}</script>。策略子系统被训练，使这两种奖励的总和<script type="math/tex">r_{t}= r_{t}^{i}+r_{t}^{e}</script>最大化，大多数情况<script type="math/tex">r_{t}^{e}</script>为零。</p>
<p>​    我们用一个参数为<script type="math/tex">\theta_{P}</script>的深度神经网络来表示策略<script type="math/tex">\pi \left ( s_{t}:\theta _{P} \right )</script>。给定智能体状态为<script type="math/tex">s_{t}</script>，它执行从策略中采样的动作<script type="math/tex">a_{t}\sim \pi \left ( s_{t}:\theta _{P} \right )</script>。<script type="math/tex">\theta_{P}</script>使用最大化预期的奖励总和进行优化。</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula1.png" alt></p>
<p>​    在本文讨论的实验中，我们使用asynchronous advantage actor critic策略梯度(A3C)进行策略学习。</p>
<h2 id="Prediction-error-as-curiosity-reward"><a href="#Prediction-error-as-curiosity-reward" class="headerlink" title="Prediction error as curiosity reward"></a>Prediction error as curiosity reward</h2><p>​    在原始感觉空间(例如，当<script type="math/tex">s_{t}</script>对应于图像时)进行预测是不可取的，不仅因为很难直接预测像素，而且因为不清楚预测像素是否是优化的正确目标。假如使用像素空间中的预测误差作为好奇心的奖励，想象一下这样一个场景，代理在微风中观察树叶的运动。由于微风天生很难建模，因此甚至很难预测每个叶子的像素位置。这意味着像素预测误差将保持很高，并且代理将始终对叶子保持好奇。但是叶子的运动对代理是无关紧要的，因此它对它们的持续好奇心是不可取的。潜在的问题是，代理没有意识到状态空间的某些部分根本无法被建模，因此代理可能会陷入一个人为的好奇心陷阱，阻碍其探索。以表格形式记录被访问状态的数量（或它们对连续状态空间的扩展）的寻求新奇的探索方案也存在这个问题。在过去，测量学习进步而不是预测误差已经被提出作为一种解决方案(Schmidhuber, 1991)。不幸的是，目前还没有已知的计算上可行的机制来衡量学习进展。</p>
<p>​    如果不是原始的观测空间，那么什么是正确的特征空间来进行预测，从而使预测误差提供了一个很好的好奇心测量方法呢？为了回答这个问题，让我们将所有可以修改代理观察结果的源分为三种情况：（1）可以由代理控制的东西；（2）代理无法控制但会影响代理（例如由另一个代理驾驶的车辆）（3）超出代理控制而不影响代理的东西（例如正在移动的树叶）。一个很好的好奇心特性空间应该建模（1）和（2），并且不受（3）的影响。后者是因为，如果有一个对代理来说无关紧要的变异源，那么代理就没有动机去知道它。</p>
<h2 id="Self-supervised-prediction-for-exploration"><a href="#Self-supervised-prediction-for-exploration" class="headerlink" title="Self-supervised prediction for exploration"></a>Self-supervised prediction for exploration</h2><p>​    我们的目标是不需要手工设计每个环境的特征表示，而是提出一种学习特征表示的一般机制，使学习特征空间中的预测误差提供一个很好的内在奖励信号。我们提出可以通过具有两个子模块的深度神经网络训练的特征空间：第一个子模块编码原始状态<script type="math/tex">s_{t}</script>成一个特征向量<script type="math/tex">\phi \left ( s_{t} \right )</script>和第二个子模块作为输入时间<script type="math/tex">t</script>和<script type="math/tex">t+1</script>的特征编码<script type="math/tex">\phi \left ( s_{t} \right )</script>，<script type="math/tex">\phi \left ( s_{t+1} \right )</script>，预测使状态从<script type="math/tex">s_{t}</script>到<script type="math/tex">s_{t+1}</script>的动作。训练这个神经网络相当于学习函数<script type="math/tex">g</script>，定义为：</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula2.png" alt></p>
<p>其中<script type="math/tex">\hat{a}_{t}</script>为对动作的预测估计，并对神经网络参数<script type="math/tex">\theta _{I}</script>进行优化</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula3.png" alt></p>
<p>其中<script type="math/tex">L_{I}</script>是衡量预测行动和实际行动之间差异的损失函数。在<script type="math/tex">a_{t}</script>是离散的情况下，<script type="math/tex">g</script>的输出是所有可能动作的soft-max分布，最小化<script type="math/tex">L_{I}</script>相当于多项分布下<script type="math/tex">\theta _{I}</script>的最大似然估计。学习到的函数<script type="math/tex">g</script>也被称为逆动力学模型，当代理使用其当前的策略<script type="math/tex">\pi(s)</script>与环境交互时，可以获得学习<script type="math/tex">g</script>所需的元组<script type="math/tex">(s_{t},a_{t},s_{t+1})</script>。</p>
<p>​    除了逆动力学模型，我们还训练了另一个神经网络，以<script type="math/tex">a_{t}</script>和<script type="math/tex">\phi \left ( s_{t} \right )</script>作为输入，并预测时间步<script type="math/tex">t+1</script>状态的特征编码。</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula4.png" alt></p>
<p>其中，<script type="math/tex">\hat\phi \left ( s_{t+1} \right )</script>)为<script type="math/tex">\phi \left ( s_{t+1} \right )</script>的预测估计值，通过最小化损失函数<script type="math/tex">L_{F}</script>来优化神经网络参数<script type="math/tex">\theta_{F}</script>：</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula5.png" alt></p>
<p>学习到的函数<script type="math/tex">f</script>也被称为前向动力学模型。内在奖励信号<script type="math/tex">r_{t}^{i}</script>的计算方法为</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula6.png" alt></p>
<p>其中，<script type="math/tex">\eta > 0</script>是一个缩放因子。为了生成基于好奇心的内在奖励信号，我们分别共同优化了公式3和公式5中所描述的正向动力学损失和逆动力学损失。逆模型学习一个特征空间，该特征空间只编码与预测代理行为相关的信息，而前向模型在该特征空间中进行预测。我们将好奇心公式定义为Intrinsic Curiosity Module (ICM)。由于该特征空间没有动机对任何不受代理行为影响的环境特征进行编码，因此我们的代理将不会因达到本质上不可预测的环境状态而获得奖励，其探索策略具有鲁棒性。</p>
<p>训练结构如Figure2所示</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/figure2.png" alt></p>
<p>学习智能体的总体优化可以写成</p>
<p><img src="/2021/11/27/Curiosity-driven-Exploration-by-Self-supervised-Prediction/formula7.png" alt></p>
<p>其中<script type="math/tex">0\leq \beta \leq 1</script>是一个衡量逆模型损失与正向模型损失的标量，而<script type="math/tex">\lambda > 0</script>是一个衡量策略梯度损失的重要性与学习内在奖励信号的重要性的标量。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/27/Mastering-the-game-of-Go-without-human-knowledge/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/27/Mastering-the-game-of-Go-without-human-knowledge/" class="post-title-link" itemprop="url">Mastering the game of Go without human knowledge</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-27 11:25:26" itemprop="dateCreated datePublished" datetime="2021-11-27T11:25:26+08:00">2021-11-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-13 23:36:19" itemprop="dateModified" datetime="2021-12-13T23:36:19+08:00">2021-12-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    最近，AlphaGo成为第一个在围棋比赛中击败世界冠军的项目。在AlphaGo中进行的树形搜索使用深度神经网络来评估位置和选择移动。这些神经网络是通过对人类专家动作的监督学习和对自我游戏的强化学习来训练的。这里我们介绍了一种仅基于强化学习的算法，没有游戏之外的人类数据、指导或领域知识规则。AlphaGo成为了它自己的老师：一个神经网络被训练来预测AlphaGo自己的移动选择以及AlphaGo游戏的获胜者。该神经网络提高了树状搜索的强度，从而产生了更高质量的移动选择和在下一次迭代中更强的self-play。从白板开始，我们的新程序AlphaGo Zero取得了超人的表现，以100-0战胜了之前发表的冠军AlphaGo。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    我们的程序，AlphaGoZero，在几个重要方面不同于AlphaGo。首先，它是完全通过self-play强化学习进行训练，从随机游戏开始，没有任何监督或使用人类数据。其次，它只使用棋盘上的黑白棋子作为输入特征。第三，它使用了一个单一的神经网络，而不是单独的策略和价值网络。最后，它使用了一个更简单的树形搜索，它依赖于这个单一的神经网络来评估位置和采样动作，而不执行任何Monte Carlo rollouts。为了实现这些结果，我们引入了一种新的强化学习算法，该算法在训练循环中引入了前瞻搜索，从而实现了快速改进和精确稳定的学习。Methods章节进一步描述了搜索算法、训练过程和网络结构的的技术差异。</p>
<h1 id="Reinforcement-learning-in-AlphaGo-Zero"><a href="#Reinforcement-learning-in-AlphaGo-Zero" class="headerlink" title="Reinforcement learning in AlphaGo Zero"></a>Reinforcement learning in AlphaGo Zero</h1><p>​    我们的新方法使用了一个参数为<script type="math/tex">\theta</script>的深度神经网络<script type="math/tex">f_{\theta}</script>。该神经网络将位置及其历史的棋盘表示<script type="math/tex">s</script>作为输入，并输出移动概率和一个值<script type="math/tex">\left ( p,v \right )=f_{\theta }\left ( s \right )</script>。移动概率的向量<script type="math/tex">p</script>表示选择每个move <script type="math/tex">a</script>（包括pass）,<script type="math/tex">p_{a}= Pr\left ( a|s \right )</script>的概率。值<script type="math/tex">v</script>是一个标量评估，估计当前玩家从状态<script type="math/tex">s</script>中获胜的概率。这个神经网络将策略网络和价值网络的作用结合成一个单一的体系结构。该神经网络由卷积层的许多残差块组成，具有批处理归一化和非线性激活函数（见Methods章节）。</p>
<p>​    AlphaGo Zero中的神经网络是通过一种新的强化学习算法从self-play游戏中训练出来的。在每个位置<script type="math/tex">s</script>，在神经网络<script type="math/tex">f_{\theta}</script>的引导下执行MCTS搜索。MCTS搜索输出每个move的概率<script type="math/tex">\pi</script>。神经网络<script type="math/tex">f_{\theta}</script>通常选择比原始移动概率<script type="math/tex">p</script>好得多的move；因此，MCT可被视为一个强有力的policy improvement操作。使用改进的基于MCTS的策略选择每一步，然后使用游戏赢家<script type="math/tex">z</script>作为值的样本进行搜索，可将其视为强大的policy evaluation操作。我们的强化学习算法的主要思想是在策略迭代过程中重复使用这些搜索操作。更新神经网络的参数，使移动概率和值<script type="math/tex">(p,v)=f_{\theta}(s)</script>更接近改进的搜索概率和self-play的赢家<script type="math/tex">(\pi,z)</script>；这些新参数将在下一次的self-play迭代中使用，使搜索更强。图1展示了自我游戏的训练流程。</p>
<p><img src="/2021/11/27/Mastering-the-game-of-Go-without-human-knowledge/figure1.png" alt></p>
<p>​    MCTS使用神经网络<script type="math/tex">f_{\theta}</script>来指导其模拟(见图2)。搜索树中的每个边<script type="math/tex">(s,a)</script>都存储先验概率<script type="math/tex">P(s,a)</script>、访问计数<script type="math/tex">N(s,a)</script>和动作值<script type="math/tex">Q(s,a)</script>。每次模拟都从根状态开始，并迭代地选择使置信上限<script type="math/tex">Q(s,a)+U(s,a)</script>能够最大化的移动，其中<script type="math/tex">U(s,a)\propto P(s,a)/(1+N(s,a))</script>，直到遇到一个叶子节点<script type="math/tex">s^{'}</script>。这个叶子的位置只被网络expanded和evaluated一次，以生成先验概率和评估<script type="math/tex">(P(s^{'},\cdot ),V(s^{'})=f_{\theta}(s^{'})</script>。模拟经过每条边<script type="math/tex">(s,a)</script>更新增加其访问数<script type="math/tex">N(s,a)</script>， 并将其动作值更新为这些模拟的平均值评估<script type="math/tex">Q(s,a)=1/N(s,a)\sum _{s^{'}|s,a\rightarrow s^{'}}V(s^{'})</script>，其中<script type="math/tex">s,a\rightarrow s^{'}</script>表明模拟从<script type="math/tex">s</script>采取动作<script type="math/tex">a</script>后最终达到<script type="math/tex">s^{'}</script>。</p>
<p><img src="/2021/11/27/Mastering-the-game-of-Go-without-human-knowledge/figure2.png" alt></p>
<p>​    MCTS可以被视为一种self-play算法，给定神经网络参数<script type="math/tex">\theta</script>和根位置<script type="math/tex">s</script>，计算搜索概率向量<script type="math/tex">\pi=\alpha_{\theta}(s)</script>，与每次移动的访问次数指数成比例，<script type="math/tex">\pi _{a}\propto N\left ( s,a \right )^{1/\tau }</script>，其中τ是一个温度参数。    </p>
<p>​    神经网络由一个self-play强化学习算法来训练，该算法使用MCTS来选择每个动作。首先将神经网络初始化为随机权值<script type="math/tex">\theta_{0}</script>。在随后的每次迭代<script type="math/tex">i\geq 1</script>中，都生成self-play(图1a)。在每个时间步长<script type="math/tex">t</script>中，使用神经网络fθi−1的上一次迭代来执行一个MCTS搜索π=αθ−()tti1，并通过对搜索概率πt进行采样来执行一个动作。当两个玩家都通过游戏，当搜索值低于辞职阈值或游戏超过最大长度时，游戏在步骤T步结束；然后对游戏进行评分，给予rT∈{−1，+1}的最终奖励（详见方法）。每个时间步t的数据存储为(st，πt，zt)，其中zt=±从t步当前玩家的角度来看是游戏赢家。并行地(图。1b)，从自游戏(s)最后一次迭代的所有时间步长中均匀采样的数据(s、π、z)中训练新的网络参数θi。对神经网络=θpvfs（，）(i)进行调整，使预测值v与自玩赢家z之间的误差最小化，并使神经网络移动概率p与搜索概率的相似性最大化</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/" class="post-title-link" itemprop="url">Mastering the Game of Go with Deep Neural Networks and Tree Search</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-26 21:58:21" itemprop="dateCreated datePublished" datetime="2021-11-26T21:58:21+08:00">2021-11-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-27 11:23:18" itemprop="dateModified" datetime="2021-11-27T11:23:18+08:00">2021-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/model-based/" itemprop="url" rel="index"><span itemprop="name">model based</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.nature.com/articles/nature16961?MRK_CMPG_SOURCE=sm_tw_pp">paper</a></p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>本文提出了将监督学习与self-play强化学习相结合，首次在围棋中击败职业选手。</p>
<p>将棋盘位置作为<script type="math/tex">19\times19</script>的图像传入，使用卷积层来构建位置的表示。我们使用value network和policy network来减少搜索树的有效深度和广度：使用value network评估位置，并使用policy对动作进行采样。神经网络的训练流程和架构如Figure1所示。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/figure1.png" alt></p>
<p>首先使用人类专家的走步数据通过监督学习训练policy network<script type="math/tex">(p_{\sigma })</script>,同时采用<strong>Computing Elo ratings of move patterns in the game of Go</strong>和<strong>Fuego — an open-source framework for board games and Go engine based on Monte-Carlo tree search</strong>两篇论文类似的方法训练了<script type="math/tex">p_{\pi}</script>用于rollout时快速采样动作。接下来训练RL policy network <script type="math/tex">p_{\rho }</script>，它通过优化self-play的最终结果来改进监督学习策略网络。这将调整策略，以达到赢得游戏的正确目标，而不是最大化预测的准确性。最后训练价值网络<script type="math/tex">v_{\theta}</script>用于预测RL policy network对抗自己时游戏的获胜者。</p>
<h1 id="监督学习策略网络"><a href="#监督学习策略网络" class="headerlink" title="监督学习策略网络"></a>监督学习策略网络</h1><p>通过已有的专家数据进行训练，网络由卷积层和非线性激活函数组成，最后一层为softmax，输出合法动作的概率。训练数据为随机采样的state-action对<script type="math/tex">(s,a)</script>，网络的输入<script type="math/tex">s</script>表示如表extend data table2所示。使用随机梯度上升来最大限度地提高人类在状态s中移动a的可能性。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula1.png" alt></p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/extend_data_table2.png" alt></p>
<p>训练结果extend data table 3所示,Symmetrics含义可以看paper的Method章节。准确度的微小提高可以使比赛强度巨大提高，如Figure2 a所示。更大的网络可以获得更好的精度，但在搜索过程中评估速度较慢。同时还训练了一个更快但更不准确的rollout policy <script type="math/tex">p_{\pi}(a|s)</script>，使用权重为<script type="math/tex">\pi</script>的一个linear softmax of small pattern features（如extend data table 4所示），达到了24.2%的准确率，只使用<script type="math/tex">\mu s</script>来选择一个动作，而不是策略网络的<script type="math/tex">3ms</script>。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/extend_data_table3.png" alt></p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/extend_data_table4.png" alt></p>
<h1 id="强化学习策略网络"><a href="#强化学习策略网络" class="headerlink" title="强化学习策略网络"></a>强化学习策略网络</h1><p>​    使用policy gradient 强化学习方法提升策略网络。强化学习策略网络和监督学习策略网络使用相同的结构，并使用监督学习网络参数初始化强化学习网络。为了防止过拟合，训练过程中会把迭代的网络模型放入对手池中，每次从池子中随机选择一个模型与当前策略对战。使用的reward function为当游戏未结束时reward为0，如果最终胜利reward为1，最终失败为-1。在每个时间步<script type="math/tex">t</script>，通过随机梯度上升，向预期结果最大化的方向更新权重。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula2.png" alt></p>
<p>其中<script type="math/tex">z_{t}</script>表示游戏reward。最终RL policy network对战监督学习策略网络达到了80%的胜率，对战Pachi（open-source Go program）达到了85%的胜率，以前最先进的监督学习网络只有11%的胜率。</p>
<h1 id="强化学习值网络"><a href="#强化学习值网络" class="headerlink" title="强化学习值网络"></a>强化学习值网络</h1><p>该网络用于位置评估，估计一个值函数<script type="math/tex">v^p(s)</script>，该函数预测在位置<script type="math/tex">s</script>处开始使用策略<script type="math/tex">p</script>对战到游戏结束的结果。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula3.png" alt></p>
<p>理想情况下，我们想知道完美对战下的最优值函数<script type="math/tex">v^{\ast }\left ( s \right )</script>；实际上使用RL策略网络<script type="math/tex">p_{\rho}</script>来估计我们最强策略的值函数<script type="math/tex">v^{p_{\rho}}</script>。我们使用一个值网络<script type="math/tex">v_{\theta}(s)</script>来近似值函数,<script type="math/tex">v_{\theta }\left ( s \right )\approx v^{p_{\rho }}\left ( s \right )\approx v^{\ast }\left ( s \right )</script>。该神经网络具有与策略网络相似的结构，但输出一个预测值，而不是一个概率分布。我们用state-outcome (s，z)作为训练集，使用随机梯度下降来最小化预测值<script type="math/tex">v_{\theta }\left ( s \right )</script>和相应的结果<script type="math/tex">z</script>之间的均方误差(MSE)。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula4.png" alt></p>
<p>从由完整游戏组成的数据中预测游戏结果的方法会导致过拟合。问题是连续的位置是密切相关，只差一步，但回归目标是共享的。当以这种方式在数据集上进行训练时，价值网络记忆了游戏结果，而不是推广到新的位置。为了缓解这个问题，生成了一个新的self-play数据集，由3000万个不同的position组成，每个position都从一个单独的游戏中采样。每个游戏都在RL策略网络和它自身之间进行，直到游戏结束。图2b显示了值网络的位置评价精度，与MCTS使用policy <script type="math/tex">p_{\pi}</script> rollout 相比，值函数始终更准确。</p>
<h1 id="使用策略网络和价值网络搜索"><a href="#使用策略网络和价值网络搜索" class="headerlink" title="使用策略网络和价值网络搜索"></a>使用策略网络和价值网络搜索</h1><p>AlphaGo将策略和价值网络结合在一个MCTS算法中，如Figure3所示。Mcts具有select、expansion、evaluation、bacckup四部分。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/figure3.png" alt></p>
<p>从当前状态<script type="math/tex">s_{t}</script>通过公式5选择<script type="math/tex">a_{t}</script></p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula5.png" alt></p>
<p>其中的<script type="math/tex">u\left ( s,a \right )\propto \frac{P\left ( s,a \right )}{1+N\left ( s,a \right )}</script>是为了鼓励探索，最终采用的<script type="math/tex">u(s,a)</script>见Method章节，<script type="math/tex">P(s,a)</script>为先验概率（可以由策略网络输出），<script type="math/tex">N(s,a)</script>为当前<script type="math/tex">s</script>的访问次数。</p>
<p>​    当遍历到达step <script type="math/tex">L</script>的叶节点<script type="math/tex">S_{L}</script>时，可以expand该叶节点。叶节点<script type="math/tex">S_{L}</script>仅由SL策略网络<script type="math/tex">p_{\sigma }</script>处理一次输出每个合法动作的概率<script type="math/tex">P</script>,<script type="math/tex">P(s,a)=p_{\sigma}(a|s)</script>。叶节点以两种非常不同的方式进行评估：第一种通过值网络<script type="math/tex">v_{\theta}(S_{L})</script>输出；第二种使用rollout policy <script type="math/tex">p_{\pi}</script>对战到游戏终局获得结果。使用混合参数<script type="math/tex">\lambda</script>将这些评估组合形成新的评估方式<script type="math/tex">V(S_{L})</script></p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula6.png" alt></p>
<p>在每次模拟结束时，更新所有遍历边的action value (<script type="math/tex">Q(s,a)</script>)和visit count (<script type="math/tex">N(s,a)</script>)。</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula7.png" alt></p>
<p><script type="math/tex">S_{L}^{i}</script>表示第<script type="math/tex">i</script>次模拟时的叶子节点，<script type="math/tex">1(s,a,i)</script>表示第<script type="math/tex">i</script>次模拟是否经过边<script type="math/tex">(s,a)</script>。当搜索完成，算法将从root position选择访问量最多的动作。在AlphaGo中使用值函数<script type="math/tex">v_{\theta }\left ( s \right )\approx v^{p_{\rho }}\left ( s \right )</script> （RL policy network）比<script type="math/tex">v_{\theta }\left ( s \right )\approx v^{p_{\sigma  }}\left ( s \right )</script> (SL policy network 监督学习)更好。</p>
<p>为了有效地将MCTS与深度神经网络结合起来，AlphaGo使用异步多线程搜索，在cpu上执行模拟，并在gpu上并行计算策略和值网络。AlphaGo的最终版本使用了40个搜索线程、48个cpu和8个gpu。还实现了一个AlphaGo的分布式版本，在Method章节会详细讲。</p>
<h1 id="AlphaGo评估"><a href="#AlphaGo评估" class="headerlink" title="AlphaGo评估"></a>AlphaGo评估</h1><p>对战结果Figure4 a</p>
<p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/figure4.png" alt></p>
<p>评估了AlphaGo的变体，这些变体仅使用价值网络<script type="math/tex">\lambda=0</script>或仅使用rollout（<script type="math/tex">\lambda=1</script>）来评估位置(见图4b)。即使没有rollout，AlphaGo的性能也超过了所有其他Go程序，这表明价值网络为蒙特卡罗评估提供了一个可行的替代方案。然而，混合评价（<script type="math/tex">\lambda=0.5</script>）表现最好，与其他变体对战达到了95%以上的胜率。这说明这两种位置评估机制是互补的：价值网络近似于强者<script type="math/tex">p_{\rho}</script>所玩的游戏的结果，而rollout可以精确地评分和评估较弱但较快的rollout <script type="math/tex">p_{\pi}</script>所玩游戏的结果。</p>
<h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p><img src="/2021/11/26/Mastering-the-Game-of-Go-with-Deep-Neural-Networks-and-Tree-Search/formula9.png" alt></p>
<p>具体方法的实现细节可以看paper</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/" class="post-title-link" itemprop="url">Mastering Complex Control in MOBA Games with Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-24 21:34:37" itemprop="dateCreated datePublished" datetime="2021-11-24T21:34:37+08:00">2021-11-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-25 22:28:14" itemprop="dateModified" datetime="2021-11-25T22:28:14+08:00">2021-11-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E6%88%98%E6%96%97%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">战斗类</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    本文从系统和算法角度提出了一个深度强化学习框架用于解决1v1战斗场景中复杂动作控制的问题。系统具有低耦合和高可伸缩性，这使大规模的高效探索成为可能。算法包括几种新的策略，包括 control dependency decoupling, action mask, target attention, and dual-clip PPO，提出的actor-critic网络可以在系统中得到有效的训练。在王者荣耀中测试，训练的智能体可以在1v1游戏中击败顶级职业人类玩家。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    本文对构建MOBA 1v1游戏AI(需要高度复杂的动作控制)进行了系统而深入的研究。在系统方面，开发了一个深度强化学习框架，提供可扩展的off-policy训练。在算法方面，actor-critic神经网络用于动作控制。我们的网络采用多标签近端策略算法(PPO)目标进行优化，并具有控制依赖的解耦、注意机制用于目标选择、action mask用于有效探索、用于学习技能组合的LSTM，以及一个改进版本的PPO，称为dual-clip PPO，以确保训练收敛。</p>
<p>​    在王者荣耀游戏中进行大量的测试，表明训练过的AI可以使用不同的英雄类型击败顶级职业人类玩家。</p>
<h1 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h1><p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/figure1.png" alt></p>
<p>系统包含如下四个模块：</p>
<ul>
<li><p>AI Server</p>
<p>AI Server涵盖了游戏环境与AI模型之间的交互逻辑。AI Server通过使用镜像策略self-play生成游戏片段(Silver et al. 2017)。对手的策略采样类似于 (Bansal et al. 2017)。基于从游戏状态中提取的特征，利用玻尔兹曼探索法预测英雄动作(Cesa-Bianchi et al. 2017)，即基于softmax分布的抽样。然后将采样的动作转发到游戏核心进行执行。执行后，游戏核心连续返回相应的奖励值和下一个状态。在使用中，一台AI Server将绑定一个CPU核心。由于游戏逻辑是在cpu上运行的，所以我们也在cpu上运行模型推理，以节省IO成本。为了有效地生成片段，我们构建了一个快速推理库<a target="_blank" rel="noopener" href="https://github.com/Tencent/FeatherCNN">FeatherCNN</a>的CPU版本。FeatherCNN可以自动将从Tensorflow和Caffe等主流工具训练出来的AI模型转换为定制的推理格式。</p>
</li>
<li><p>Dispatch Module</p>
<p> 每个Dispatch Module都与同一台机器上的几个AI服务器绑定。它从AI Server中收集数据样本，包括奖励、特征、行动概率等。这些样本首先被压缩和打包，然后发送到Memory Pool。</p>
</li>
<li><p>Memory Pool</p>
<p>Memory Pool也是一个服务器。它的内部被实现为数据存储高效的循环队列。它支持不同长度的样本，并基于生成的时间进行数据采样。</p>
</li>
<li><p>RL Learner</p>
<p>RL Learner 是一个分布式训练环境。 为了加速策略更新使用large batch size，集成多个 RL Learner 从相同数量的内存池中并行获取数据。 RL Learner 中的梯度通过 ring allreduce 算法 (Sergeev and Balso 2018)进行平均。 为了降低 IO 成本，RL Learner 使用共享内存而不是 socket 与 Memory Pools 通信，这可以提供 2-3 倍的速度提升。 来自 RL Learner的训练模型以点对点的方式快速同步到AI Servers。</p>
</li>
</ul>
<p>在我们的系统中，经验的生成与参数的学习相解耦。这种灵活的机制使AI Server和RL Learner具有高吞吐量的可扩展性。为了避免learners和actors之间的通信成本瓶颈，我们训练过的模型通过我们的主RL Learner的点对点与AI Server同步。为了平滑数据的存储和传输，我们设计了两个中介服务器，即Dispatch服务器和Memory Pool服务器。在实践中，我们可以毫不费力地扩展到数百万个CPU内核和数千个gpu。请注意，这种设计不同于现有的系统设计，如IMPALA (Espeholt et al. 2018)。在IMPALA中，参数分布在整个学习者中，参与者同时从所有学习者中检索参数。</p>
<h1 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h1><p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/figure2.png" alt></p>
<p>creeps表示野怪，turrets表示防御塔</p>
<p>网络将<script type="math/tex">f_{i}</script>(本地图像信息，比如2D障碍信息),<script type="math/tex">f_{8}</script>（可观测的单元属性，比如英雄类型、血量等）,<script type="math/tex">f_{g}</script>（游戏状态信息，比如游戏时长等）分别编码为<script type="math/tex">h_{i}</script>,<script type="math/tex">h_{u}</script>,<script type="math/tex">h_{g}</script>，使用的网络层如Figure2所示。<script type="math/tex">f_{u}</script>经过几层后会被分为单元的表示和target的attention keys两部分。为了处理不同数量的单元，通过max-pooling将相同类型的单元映射到固定长度的特征向量。然后将所有类型的<script type="math/tex">h_i</script>、<script type="math/tex">h_{u}</script>和 <script type="math/tex">h_{g}</script> 连接表示为一个可观察状态的编码向量。然后通过一个LSTM层将状态编码映射到最终的表示<script type="math/tex">h_{LSTM}</script>上，从而进一步考虑了时间信息。<script type="math/tex">h_{LSTM}</script>被发送到一个FC层来预测动作<script type="math/tex">a</script>。动作<script type="math/tex">a</script>的目标单元<script type="math/tex">t</script>可由每个单元上的目标注意机制进行预测。该机制将<script type="math/tex">h_{LSTM}</script>的FC输出视为query，将所有单元编码的stack视为keys <script type="math/tex">h_{keys}</script>，并计算目标注意力:</p>
<p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/formula1.png" alt></p>
<p><script type="math/tex">p(t|a)</script>是当前状态下所有单元的注意力分布，<script type="math/tex">p(t|a)</script>的维度是当前状态下的单元数。</p>
<p>​    其次，在multi-label policy network中，很难明确地建模不同标签之间的相互关联，例如技能方向(Offset X and Offset Y)与技能类型（Button）之间的相关性。为了解决这个问题，我们在一个操作中独立地处理每个标签，以解耦它们的相互关联，即控制依赖关系的解耦。在解耦相互互关联之前，without clipping PPO目标是：</p>
<p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/formula2.png" alt></p>
<p>假设动作被解耦为<script type="math/tex">a=(a^{0},...,a^{N_{a}-1})</script>,目标函数变为</p>
<p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/formula3.png" alt></p>
<p>​    这个解耦的目标带来了两个优点。首先，它简化了策略结构。具体来说，策略网络可以不考虑相互的相关性，因为这种依赖关系可以进行后处理。第二，它增加了行动的多样性。由于每个组件都有自己独立的价值输出通道，因此行动可以显著多样化，从而在训练过程中诱导更多的探索。此外，<strong>为了探索的多样性，我们在游戏开始的训练中随机两个代理的位置</strong>。</p>
<p>​    为了提高训练效率，提出了action mask，基于经验丰富的人类玩家的先验知识，在策略的最终输出层中mask不合理的动作。</p>
<p><strong>Dual-clip PPO</strong>  设<script type="math/tex">r_{t}(\theta)</script>表示概率比<script type="math/tex">\frac{\pi _{\theta }\left ( a_{t}|s_{t} \right )}{\pi _{\theta _{old}}\left ( a_{t}|s_{t} \right )}</script>。由于<script type="math/tex">r_{t}(\theta)</script>的比值可能非常大，因此RL目标的最大化可能会导致过大的策略偏差。为了缓解这个问题，标准的PPO算法 (Schulman et al. 2017) 包含如下ratio clip来惩罚policy的极端变化</p>
<p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/formula4.png" alt></p>
<p>然而，在大规模off-policy训练环境中，轨迹从各种policy来源中采样，这可能与当前的policy <script type="math/tex">\pi_{\theta}</script>有很大的不同。在这种情况下，标准PPO将无法处理这些偏差，因为它最初是针对on-policy提出的 (Schulman et al. 2017)。例如，当<script type="math/tex">\pi _{\theta }\left ( a_{t}^{\left ( i \right )}|s_{t} \right )> >\pi  _{\theta _{old}}\left ( a_{t}^{\left ( i \right )} |s_{t}\right )</script>时，比率<script type="math/tex">r_{t}(\theta)</script>是一个巨大的数字。当<script type="math/tex">\hat{A}_{t}<0</script>时，如此大的比率<script type="math/tex">r_{t}(\theta)</script>将引入一个大的无界方差,因为<script type="math/tex">r_{t}\left ( \theta  \right )\hat{A}_{t}< < 0</script>。因此，即使采用PPO的目标，新policy也与旧policy有了明显的偏差，这很难确保策略可以收敛。因此我们提出了一种 dual-clipped PPO算法来支持大规模分布训练，进一步剪<script type="math/tex">r_{t}\left ( \theta  \right )\hat{A}_{t}</script>下界，如图3所示。当<script type="math/tex">\hat{A}_{t}<0</script>时，dual-clipped PPO的新目标是：</p>
<p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/formula5.png" alt></p>
<p>其中c&gt;1是一个常数，表示下界。</p>
<p><img src="/2021/11/24/Mastering-Complex-Control-in-MOBA-Games-with-Deep-Reinforcement-Learning/figure3.png" alt></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/11/Attention-Is-All-You-Need/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/11/Attention-Is-All-You-Need/" class="post-title-link" itemprop="url">Attention Is All You Need</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-08-11 21:04:01" itemprop="dateCreated datePublished" datetime="2021-08-11T21:04:01+08:00">2021-08-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-08-21 10:21:56" itemprop="dateModified" datetime="2021-08-21T10:21:56+08:00">2021-08-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​     主要的序列转换模型基于复杂的循环或卷积神经网络，包括编码器和解码器。 性能最好的模型还通过注意力机制连接编码器和解码器。 我们提出了一种新的简单网络架构，即 Transformer，它完全基于注意力机制，完全消除了递归和卷积。 在两个机器翻译任务上的实验表明，这些模型在质量上更胜一筹，同时更可并行化并且需要更少的训练时间。 我们的模型在 WMT 2014 英德翻译任务上达到了 28.4 BLEU，比现有的最佳结果（包括集成）提高了 2 BLEU。 在 WMT 2014 英语到法语翻译任务中，我们的模型在 8 个 GPU 上训练 3.5 天后建立了一个新的单模型最先进的 BLEU 分数 41.0，这是最好的训练成本的一小部分文献中的模型。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    RNN输入需要上一个隐藏状态，这种固有的顺序特性导致在训练中不能使用并行化。在除少数情况下，注意机制与循环网络一起使用。在这项工作中，我们提出了Transformer，一个避免递归的模型架构，而是完全依赖于一个注意机制来绘制输入和输出之间的全局依赖关系。Transformer允许更多的并行化，在8个P100 gpu上训练了短短12个小时后，可以达到翻译质量的新水平。</p>
<h1 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h1><p>​    大多数具有竞争力的神经序列转换模型都具有encoder-decoder结构。 在这里，encoder将符号表示的输入序列 <script type="math/tex">\left ( x_{1},...,x_{n} \right )</script> 映射到连续表示的序列<script type="math/tex">z= \left ( z_{1},...,z_{n} \right )</script>。 给定<script type="math/tex">z</script>，decoder然后生成一个符号的输出序列<script type="math/tex">\left ( y_{1},...,y_{n} \right )</script>，一次一个元素。 在每一步，模型都是自回归的，在生成下一个时，将先前生成的符号作为附加输入使用。</p>
<p>​    Transformer 遵循这种整体架构，使用堆叠的self-attention和point-wise、完全连接的encoder和decoder层，分别如图 1 的左半部分和右半部分所示。</p>
<p><img src="/2021/08/11/Attention-Is-All-You-Need/figure1.png" alt></p>
<h2 id="Encoder-and-Decoder-Stacks"><a href="#Encoder-and-Decoder-Stacks" class="headerlink" title="Encoder and Decoder Stacks"></a>Encoder and Decoder Stacks</h2><ul>
<li><p>Encoder:编码器由<script type="math/tex">N=6</script>个相同层组成。每一层都有两个子层。第一个是multi-head self-attention mechanism，第二个是一个简单的position-wise全连接的前馈网络。我们在两个子层周围使用一个残差连接，然后是层归一化。即每个子层的输出都是<script type="math/tex">LayerNorm(x+Sublayer(x))</script>，其中<script type="math/tex">Sublayer(x)</script>是由子层本身实现的函数。为了方便这些残差连接，模型中的所有子层以及嵌入层都会产生了维度<script type="math/tex">d_{model}=512</script>的输出。</p>
</li>
<li><p>Decoder:该解码器还由<script type="math/tex">N=6</script>个相同层的堆栈组成。除了每个编码器层中的两个子层外，解码器还插入第三子层，该子层对编码器堆栈的输出执行multi-head attention。与编码器类似，我们在每个子层周围使用残差连接，然后进行层归一化。我们还修改了解码器堆栈中的self-attention 子层，以防止位置关注后续位置。这种掩蔽，再加上输出嵌入被一个位置偏移的事实，确保了对位置<script type="math/tex">i</script>的预测只能依赖于小于<script type="math/tex">i</script>的位置的已知输出。</p>
</li>
</ul>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>​    注意函数可以描述为将query和一组key-value对映射到输出，其中query、keys、values和output都是向量。输出计算为values的加权和，其中分配给每个value的权重由query与相应key的兼容性函数计算。</p>
<h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p>​    我们将我们的特别attention称为“Scaled Dot-Product Attention”（图 2）。 输入包括维度<script type="math/tex">d_{k}</script>的queries和keys，以及维度<script type="math/tex">d_{v}</script>的values。 我们使用query和所有keys的点积，将每个除以<script type="math/tex">\sqrt{d_{k}}</script>，然后应用 softmax 函数来获得value上的权重。</p>
<p><img src="/2021/08/11/Attention-Is-All-You-Need/figure2.png" alt></p>
<p>​    在实践中，我们同时计算一组queries上的attention function，并打包到一个矩阵<script type="math/tex">Q</script>中。keys和values也被打包在矩阵<script type="math/tex">k</script>和<script type="math/tex">V</script>中。我们计算的输出矩阵如下：</p>
<p><img src="/2021/08/11/Attention-Is-All-You-Need/formula1.png" alt></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">QilongPan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QilongPan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
