<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="潘其龙">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="潘其龙">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="QilongPan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>潘其龙</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">潘其龙</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/08/The-Surprising-Effectiveness-of-PPO-in-Cooperative-Multi-Agent-Games/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/08/The-Surprising-Effectiveness-of-PPO-in-Cooperative-Multi-Agent-Games/" class="post-title-link" itemprop="url">The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-08-08 11:39:12" itemprop="dateCreated datePublished" datetime="2021-08-08T11:39:12+08:00">2021-08-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-08-09 20:20:31" itemprop="dateModified" datetime="2021-08-09T20:20:31+08:00">2021-08-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93/" itemprop="url" rel="index"><span itemprop="name">多智能体</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    近端策略优化(PPO)是一种流行的on-policy强化学习算法，但在多代理环境中的利用率明显低于off-policy学习算法。这通常是由于认为在多代理环境中，on-policy的样本效率明显低于off-policy的方法。在这项工作中，我们研究了Multi-Agent PPO(MAPPO)，这是PPO的一种变体，专门用于多代理环境。使用1-GPU台式电脑，我们展示了MAPPO在三种流行的多代理测试台上（the particle-world environments, the Starcraft multi-agent challenge, and the Hanabi challenge）以极少的超参数调整，并且没有任何特定领域的算法修改或架构实现了惊人的强大性能。在大多数环境中，我们发现与off-policy 基线相比，MAPPO取得了很强的结果，同时显示出相当的样本效率。最后通过消融研究，我们提出了对MAPPO实际性能影响最大的实现和算法因素。</p>
<p><img src="/2021/08/08/The-Surprising-Effectiveness-of-PPO-in-Cooperative-Multi-Agent-Games/figure1.png" alt></p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    我们专注于完全合作的任务，以便直接比较我们的结果与最先进的代理的性能。此外我们只使用一台具有1个GPU和1台多核CPU的台式机进行训练。</p>
<p>​    本文的贡献总结如下：</p>
<ul>
<li>我们证明，Multi-Agent PPO(MAPPO)，通过极小的超参数调整，并且没有任何特定领域的算法更改或架构，在三个普遍存在的基准：Particle World, Starcraft and Hanabi上实现了与各种off-policy方法相当的最终性能。</li>
<li>我们证明了MAPPO获得了这些强大的结果，同时经常使用与许多off-policy方法相当数量的样本。</li>
<li>我们分析了控制MAPPO实际性能的5个实现和算法因素，并在<a target="_blank" rel="noopener" href="https://sites.google.com/view/mappo-neurips">https://sites.google.com/view/mappo-neurips</a>上发布了我们的源代码。</li>
</ul>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>​    我们考虑多智能体游戏，并调查那些在现有文献中很大程度上被忽略，或对多智能体设置完全唯一的因素。尽管我们的多代理基准测试与MuJoCo等单代理环境有显著不同的动态，但我们确实发现了一些以前的建议，如输入归一化、层归一化、值剪辑、正交初始化和梯度剪切，这很有帮助，并将它们包含在我们的实现中。</p>
<h1 id="Multi-Agent-PPO-MAPPO"><a href="#Multi-Agent-PPO-MAPPO" class="headerlink" title="Multi-Agent PPO(MAPPO)"></a>Multi-Agent PPO(MAPPO)</h1><p><img src="/2021/08/08/The-Surprising-Effectiveness-of-PPO-in-Cooperative-Multi-Agent-Games/algorithm1.png" alt></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/05/QMIX-Monotonic-Value-Function-Factorisation-for-Deep-Multi-Agent-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/05/QMIX-Monotonic-Value-Function-Factorisation-for-Deep-Multi-Agent-Reinforcement-Learning/" class="post-title-link" itemprop="url">QMIX:Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-08-05 20:55:47" itemprop="dateCreated datePublished" datetime="2021-08-05T20:55:47+08:00">2021-08-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-08-09 20:18:59" itemprop="dateModified" datetime="2021-08-09T20:18:59+08:00">2021-08-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93/" itemprop="url" rel="index"><span itemprop="name">多智能体</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    在许多现实世界中，一组代理必须以分散的方式行事的同时协调他们的行为。同时，通常可以在模拟或实验室环境中以集中方式训练代理，其中全局状态信息可用并且通信限制被解除。学习以额外状态信息为条件的联合行动值是一种利用集中学习的有吸引力的方式，但提取分散策略的最佳策略尚不清楚。我们的解决方案是 QMIX，这是一种新颖的基于价值的方法，可以在集中的端到端时尚中训练分散的策略。 QMIX 采用了一个网络，该网络将联合动作值估计为仅以局部观察为条件的每个代理值的复杂非线性组合。我们在结构上强制每个代理值中的联合动作值是单调的，这允许在离策略学习中实现联合动作值的最大化，并保证集中和分散策略之间的一致性。我们在一组具有挑战性的星际争霸 II 微观管理任务上评估 QMIX，并表明 QMIX 显着优于现有的基于价值的多智能体强化学习方法。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    强化学习(RL)对于帮助解决各种合作的多主体问题具有相当大的前景，如机器人群的协调 (Huttenrauch et al.,2017)和自动驾驶汽车(Cao et al., 2012)。</p>
<p>​    在许多这样的设置中，部分可观察性和/或通信限制需要学习分散的策略，这只以每个代理的本地操作观察历史为条件。分散化的策略也自然地减弱了联合动作空间随着代理数量呈指数级增长的问题，这往往使传统的单代理RL方法的应用不切实际。</p>
<p>​    幸运的是，分散的政策通常可以在模拟或实验室设置中以集中的方式学习。这通常授予对其他状态信息的访问，否则对代理隐藏，并删除代理间通信约束。具有分散执行的集中培训范式 (Oliehoek et al., 2008; Kraemer &amp; Banerjee, 2016)最近引起了RL社区的关注(Jorge et al., 2016; Foerster et al., 2018)。然而，围绕如何最好地利用集中式训练的许多挑战仍然存在。</p>
<p>​    其中一个挑战是如何表示和使用大多数RL方法所学习的动作值函数。一方面，正确捕获代理操作的影响需要一个集中的动作值函数Qtot，它以全局状态和联合动作为条件。另一方面，当有许多代理时，这种函数很难学习，即使可以学习，也没有提供明显的方法来提取分散的策略，允许每个代理只基于单个观察选择单个操作。</p>
<p>​    最简单的选择是放弃一个集中的动作值函数，让每个代理独立学习单个动作值函数Qa，如独立的Q学习(IQL) (Tan, 1993)。然而，这种方法不能明确地表示代理之间的相互作用，也可能不会收敛，因为每个代理的学习都被其他代理的学习和探索所混淆。</p>
<p>​    在另一个极端，我们可以学习一个完全集中的状态操作值函数<script type="math/tex">Q_{tot}</script>，然后使用它来指导参与者-批评框架中的分散策略的优化，反事实多代理(COMA)策略梯度(Foerster et al., 2018)，以及Gupta et al. (2017)等人的工作。然而，这需要关于政策学习，这可能是样本效率低下的，当有多个代理时，训练完全集中的批评者就变得不切实际。</p>
<p>​    在这两个极端之间，我们可以学习一个集中但被分解的<script type="math/tex">Q_{tot}</script>，这是一种由值分解网络(VDN)采用的方法(Sunehag et al., 2017)。通过将<script type="math/tex">Q_{tot}</script>表示为单个值函数<script type="math/tex">Q_{a}</script>的和，只影响单个观察和动作，一个分散的策略仅仅从每个代理贪婪地选择动作而产生。然而，VDN严重限制了可以表示的集中动作值函数的复杂性，并忽略了训练期间可用的任何额外的状态信息。</p>
<p>​    在本文中，我们提出了一种名为QMIX的新方法，它像VDN一样，位于IQL和COMA的极端之间，但可以代表更丰富的操作值函数。我们的方法的关键是，VDN的完全因式分解不必要提取去中心策略。相反，我们只需要确保在<script type="math/tex">Q_{tot}</script>上执行的全局argmax就会产生与在每个<script type="math/tex">Q_{a}</script>上执行的一组单个argmax操作相同的结果。为此，对<script type="math/tex">Q_{tot}</script>和每个<script type="math/tex">Q_{a}</script>之间的关系实施单调性约束就足够了：</p>
<p><img src="/2021/08/05/QMIX-Monotonic-Value-Function-Factorisation-for-Deep-Multi-Agent-Reinforcement-Learning/formula1.png" alt></p>
<p>​    QMIX由代表每个<script type="math/tex">Q_{a}</script>的代理网络和一个将它们组合成<script type="math/tex">Q_{tot}</script>的混合网络组成，不像在VDN中那样是一个简单的和，而是以一种复杂的非线性方式，确保集中和分散策略之间的一致性。同时，它通过限制混合网络具有正权值来加强（1）的约束。因此，QMIX可以用一个分解的表示来表示复杂的集中动作值函数，该函数可以扩展代理数量，并允许通过线性时间单个argmax操作轻松提取分散的策略。</p>
<p>​    我们在星际争霸 II中构建的一系列单元微管理任务上评估 QMIX (Vinyals et al., 2017)。 我们的实验表明，QMIX 在绝对性能和学习速度方面都优于 IQL 和 VDN。 特别是，我们的方法在具有异构代理（系统内智能体的动力学不完全一致）<a target="_blank" rel="noopener" href="http://cdmd.cnki.com.cn/Article/CDMD-10487-1019616494.htm#:~:text=%E5%BC%82%E6%9E%84%E6%8C%87%E7%9A%84%E6%98%AF%E7%B3%BB%E7%BB%9F%E5%86%85%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E5%8A%A8%E5%8A%9B%E5%AD%A6%E4%B8%8D%E5%AE%8C%E5%85%A8%E4%B8%80%E8%87%B4%2C%E6%98%BE%E7%84%B6%E8%BF%99%E4%B8%80%E7%89%B9%E6%80%A7%E5%9C%A8%E4%BC%97%E5%A4%9A%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E7%B3%BB%E7%BB%9F%E4%B8%AD%E9%83%BD%E6%98%AF%E5%AD%98%E5%9C%A8%E7%9A%84%2C%E4%BD%86%E6%97%B6%E5%B8%B8%E5%BF%BD%E7%95%A5%E4%BA%86%E6%99%BA%E8%83%BD%E4%BD%93%E9%97%B4%E7%9A%84%E5%B7%AE%E5%BC%82%E6%80%A7%E3%80%82,%E5%BC%82%E6%9E%84%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%B8%80%E4%B8%AA%E5%85%B8%E5%9E%8B%E4%BE%8B%E5%AD%90%E6%98%AF%E7%94%B1%E6%97%A0%E4%BA%BA%E6%9C%BA%E5%92%8C%E6%97%A0%E4%BA%BA%E8%89%87%E7%BB%84%E6%88%90%E7%9A%84%E6%B7%B7%E5%90%88%E7%BC%96%E9%98%9F%E3%80%82%20%E6%9C%AC%E6%96%87%E7%A0%94%E7%A9%B6%E4%BA%86%E4%B8%80%E7%B1%BB%E5%BC%82%E6%9E%84%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%90%8C%E6%AD%A5%E9%97%AE%E9%A2%98%2C%E6%8F%90%E5%87%BA%E4%BA%86%E6%9C%89%E6%95%88%E7%9A%84%E5%90%8C%E6%AD%A5%E7%AD%96%E7%95%A5%2C%E5%BE%97%E5%87%BA%E4%BA%86%E5%AE%9E%E7%8E%B0%E5%90%8C%E6%AD%A5%E7%9A%84%E5%85%85%E5%88%86%E5%92%8C%E5%BF%85%E8%A6%81%E6%9D%A1%E4%BB%B6%2C%E5%B9%B6%E9%87%8F%E5%8C%96%E4%BA%86%E5%90%8C%E6%AD%A5%E9%80%9F%E7%8E%87%E3%80%82">异构代理</a>的任务上显示出可观的性能提升。 此外，我们的消融显示了对状态信息进行调节的必要性和代理 Q 值的非线性混合，以便在任务之间实现一致的性能。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>​    最近在多代理RL方面的工作已经开始从表格方法(Yang &amp; Gu, 2004; Busoniu et al., 2008)转向能够处理高维状态和动作空间的深度学习方法(Tampuu et al., 2017; Foerster et al.,2018; Peng et al., 2017)。在本文中，我们重点关注合作设置。</p>
<p>​    一方面，为多智能体系统查找策略的一种自然方法是直接学习分散的值函数或策略。独立的Q-学习 (Tan, 1993)使用Q-学习 (Watkins, 1989)为每个代理训练独立的动作价值函数。(Tammuu等人，2017)使用DQN将该方法扩展到深度神经网络(Mnih et al., 2015)。虽然这些方法只是实现了分散化，但它们容易出现由于同时学习和探索代理所引起的环境的非平稳性而引起的不稳定性。Omidshafiei et al. (2017) and Foerster et al. (2017)在一定程度上解决了学习稳定问题，但仍然学习了分散的值函数，不允许在培训期间包含额外的状态信息。</p>
<p>​    另一方面，联合行动的集中学习可以自然地处理协调问题，避免非平稳性，但随着联合行动空间的扩大，代理数量呈指数增长。可扩展集中学习的经典方法包括coordination graphs (Guestrin et al., 2002)，它通过将全局奖励函数分解为 agent-local项的和来利用代理之间的条件独立性。Sparse cooperative Q-learning (Kok &amp; Vlassis, 2006)是一种表格q学习算法，它只在需要这种协调的状态下学习协调一组协作代理的动作，将这些依赖关系编码在coordination graph中。这些方法要求预先提供代理之间的依赖关系，而我们不需要这样的先验知识。相反，我们假设每个代理总是对全局奖励有贡献，并学习其在每个状态下的贡献的大小。</p>
<p>​    最近的集中学习方法在执行过程中需要更多的通信： CommNet(Sukhbaatar et al., 2016)使用集中的网络体系结构在代理之间交换信息。BicNet(Pengetal.，2017)使用双向rnn在行为者-批评者环境中在代理之间交换信息。这种方法还需要估计个体代理的奖励。</p>
<p>​    一些工作已经开发了混合方法，利用完全分散执行的集中学习设置。COMA(Foersteretal.，2018)使用一个集中的批评者来培训分散的参与者，估计每个代理的反事实优势函数，以解决多代理的信用分配。同样地，Gupta et al. (2017)提出了一个集中的参与者批评算法，该算法随代理的数量而扩展，但减轻了集中化的优势。Lowe et al. (2017)为每个代理学习一个集中的批评者，并将其应用于具有连续动作空间的竞争游戏。这些方法采用策略上的策略梯度学习，其样本效率较低，容易陷入次优局部最小值。</p>
<p>​    Sunehag et al. (2017)提出了价值分解网络(VDN)，它允许具有分散执行的集中价值函数学习。他们的算法将一个中心状态动作值函数分解为单个代理项的和。这对应于使用一个退化的完全不连通的协调图。VDN在训练期间不使用额外的状态信息，只能代表有限的集中动作值函数。</p>
<p>​    许多论文已经在星际争霸中建立了单位微管理，作为深度多代理RL的基准。Usunier et al. (2017)提出了一种使用集中贪婪MDP和一阶优化的算法。Peng et al. (2017)也评估了他们在星际争霸上的方法。然而，两者都不需要分散的执行。与我们的设置类似的是Foerster et al. (2017)的工作，他评估了IQL对最多5名特工的战斗场景的重放稳定方法。Foerster et al. (2018)也会使用此设置。在本文中，我们构建了StarCraft II Learning Environment(SC2LE)中(Vinyalsetal.，2017)的单元微管理任务，因为它得到了游戏开发者的积极支持，SC2LE提供了一个更稳定的测试环境。</p>
<p>​    QMIX依赖于一个神经网络来将集中状态转换为另一个神经网络的权值，以一种让人想起hypernetworks (Ha et al., 2017)的方式。第二个神经网络通过保持其权重为正，限制其输入是单调的。Dugas et al. (2009)研究了神经网络的这种功能限制。</p>
<h1 id="QMIX"><a href="#QMIX" class="headerlink" title="QMIX"></a>QMIX</h1><p>​    在本节中，我们提出了一种名为QMIX的新方法，它和VDN一样，位于IQL和集中q学习的极端之间，但可以代表更丰富的动作值函数。</p>
<p>​    我们的方法的关键是，为了能够提取与集中对应策略完全一致的分散策略，VDN是不必完全分解的。相反，为了保持一致性，我们只需要确保在<script type="math/tex">Q_{tot}</script>上执行的全局argmax就会产生与在每个<script type="math/tex">Q_{a}</script>上执行的一组单个argmax操作相同的结果：</p>
<p><img src="/2021/08/05/QMIX-Monotonic-Value-Function-Factorisation-for-Deep-Multi-Agent-Reinforcement-Learning/formula4.png" alt></p>
<p>​    这允许每个代理a只通过对其<script type="math/tex">Q_{a}</script>选择贪婪的行动来参与一个分散的执行。作为一个副作用，如果满足（4），那么获取策略外学习更新所需的<script type="math/tex">Q_{tot}</script>的argmax是非常易于处理的。</p>
<p>​    VDN 的表示足以满足（4）。 然而，QMIX 基于这样的观察，即这种表示可以推广到更大的单调函数族，这些单调函数也足以满足（4）但不是必需的。 可以通过对 Qtot 和每个 Qa 之间关系的约束来强制执行单调性</p>
<p><img src="/2021/08/05/QMIX-Monotonic-Value-Function-Factorisation-for-Deep-Multi-Agent-Reinforcement-Learning/formula5.png" alt></p>
<p>​    为了强制执行（5），QMIX使用由agent networks, a mixing network, and a setof hypernetworks (Ha et al., 2017)。图2说明了整个设置。</p>
<p>​    对于每个代理a，都有一个代理网络代表其单个值函数<script type="math/tex">Q_{a}\left ( \tau ^{a},u^{a} \right )</script>。我们将代理网络表示为drqn，在每个时间步长接收当前的个体观察<script type="math/tex">o_{t}^{a}</script>和最后一个动作<script type="math/tex">u_{t-1}^{a}</script>作为输入，如图2c所示。</p>
<p>​    混合网络是一个前馈神经网络，它将代理网络的输出作为输入，并单调地混合，产生<script type="math/tex">Q_{tot}</script>的值，如图2a所示。为了加强（5)的单调性约束，混合网络的权值(而不是偏差）被限制为非负的。这允许混合网络任意紧密地逼近任何单调函数(Dugas et al., 2009)。</p>
<p>​        混合网络的权值由单独的超网络产生的。每个超网络以状态s作为输入，生成一层混合网络的权值。每个超网络由一个线性层组成，然后是一个绝对激活函数，以确保混合网络的权值是非负的。超网络的输出是一个向量，它被重新塑造成一个适当大小的矩阵。bias以同样的方式产生，但不限于非负性。最终的偏差是由一个具有ReLU非线性的2层超网络产生的。图2a说明了混合网络和超网络。</p>
<p><img src="/2021/08/05/QMIX-Monotonic-Value-Function-Factorisation-for-Deep-Multi-Agent-Reinforcement-Learning/figure2.png" alt></p>
<p>​    状态由超网络使用，而不是直接传递到混合网络，因为<script type="math/tex">Q_{tot}</script>可以以非单调方式依赖于额外的状态信息。 因此，将 s 的某些函数与每个代理的值一起通过单调网络传递会受到过度约束。 相反，超网络的使用可以以任意方式调节单调网络在 s 上的权重，从而尽可能灵活地将完整状态 s 集成到联合动作值估计中。</p>
<p>​    QMIX经过端到端培训，以尽量减少以下损失：</p>
<p><img src="/2021/08/05/QMIX-Monotonic-Value-Function-Factorisation-for-Deep-Multi-Agent-Reinforcement-Learning/formula6.png" alt></p>
<p>​    其中b是从重放缓冲区采样的转换的批大小,<script type="math/tex">y^{tot}=r+\gamma max_{u^{'}}Q_{tot}\left ( \tau ^{'},u^{'},s^{'};\theta ^{-} \right )</script>,<script type="math/tex">\theta ^{-}</script>是目标网络的参数，如DQN中（6)类似于(2）的标准DQN损失。由于（4）成立，我们可以在代理数量上以时间线性地执行<script type="math/tex">Q_{tot}</script>的最大化（而不是在最坏情况下按指数缩放）。</p>
<h1 id="表示复杂性"><a href="#表示复杂性" class="headerlink" title="表示复杂性"></a>表示复杂性</h1><p>​    可用QMIX表示的值函数类包括任何值函数，在完全可观测的设置中，它们可以分解为代理的单个值函数的非线性单调组合。这扩展了由VDN可表示的线性单调值函数。然而，（5）中的约束阻止了QMIX表示不以这种方式分解的值函数。</p>
<p>​    直观地说，一个代理的最佳操作依赖于其他代理在同一时间步长的动作的任何值函数都不会适当地因式分解，因此不能用QMIX完美地表示。然而，QMIX可以比VDN更准确地近似这些值函数。此外，它可以利用训练过程中可用的额外状态信息，我们通过经验证明。关于表示复杂性的更详细的讨论可在补充材料中找到。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/31/Value-Decomposition-Networks-For-Cooperative-Multi-Agent-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/31/Value-Decomposition-Networks-For-Cooperative-Multi-Agent-Learning/" class="post-title-link" itemprop="url">Value-Decomposition Networks For Cooperative Multi-Agent Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-31 11:16:55" itemprop="dateCreated datePublished" datetime="2021-07-31T11:16:55+08:00">2021-07-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-08-09 20:19:14" itemprop="dateModified" datetime="2021-08-09T20:19:14+08:00">2021-08-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93/" itemprop="url" rel="index"><span itemprop="name">多智能体</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    我们研究了具有单个联合奖励信号的协作多智能体强化学习问题。 这类学习问题很困难，因为通常有很大的组合动作和观察空间。 在完全中心化和去中心化的方法中，我们发现了虚假奖励的问题和一种我们称之为“lazy agent”问题的现象，这是由于部分可观察性引起的。 我们通过使用新颖的价值分解网络架构训练单个代理来解决这些问题，该架构学习将团队价值函数分解为agent-wise价值函数。 我们对一系列部分可观察的多智能体领域进行了实验评估，并表明学习这种价值分解会带来卓越的结果，尤其是在与权重共享、角色信息和信息渠道相结合时。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    我们提出了一个简单的实验，在这个实验中，集中的方法通过学习低效的策略，另一个代理是“lazy”的策略而失败。当一个代理学习了一个有用的策略时，但另一个代理不愿学习，因为它的探索会阻碍第一个代理，并导致更糟糕的团队奖励（例如，想象一个使用目标数作为球队奖励信号的RL训练2人足球队。假设一个球员成为了比另一个球员更好的得分手。当较差的玩家投篮时，结果平均要糟糕得多，而较弱的玩家学会了避免投篮）。</p>
<p>​    另一种方法是训练独立的学习者，以优化团队奖励。一般来说，每个代理都面临一个非平稳的学习问题，因为其环境的动态随着队友改变学习的行为而有效地发生变化。</p>
<p>​    提高独立学习者表现的一种方法是设计个体奖励函数，更直接地与个体主体观察相关。然而，即使在单代理的情况下，奖励塑造也是困难的，只有一小类形状的奖励函数被保证保持最优的w.r.t。真正的目标(Ng等人，1999年；Devlin等人，2014年；Eck等人，2016年)。本文的目标是更一般的自治解，其中学习团队值函数的分解。</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/66571753">Dec-POMDP</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Louiii/ValueDecomposition">案例code</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/21/Multi-Agent-Actor-Critic-for-Mixed-Cooperative-Competitive-Environments/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/21/Multi-Agent-Actor-Critic-for-Mixed-Cooperative-Competitive-Environments/" class="post-title-link" itemprop="url">Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-07-21 20:08:57 / Modified: 20:10:41" itemprop="dateCreated datePublished" datetime="2021-07-21T20:08:57+08:00">2021-07-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/" class="post-title-link" itemprop="url">强化学习常用游戏模拟环境</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-15 21:19:45" itemprop="dateCreated datePublished" datetime="2021-07-15T21:19:45+08:00">2021-07-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-07-20 23:47:37" itemprop="dateModified" datetime="2021-07-20T23:47:37+08:00">2021-07-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1GN411o7uy">网易伏羲视频地址</a></p>
<h1 id="Grid-world"><a href="#Grid-world" class="headerlink" title="Grid world"></a>Grid world</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic1.png" alt></p>
<h1 id="Multi-agent-Grid-world"><a href="#Multi-agent-Grid-world" class="headerlink" title="Multi-agent Grid world"></a>Multi-agent Grid world</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic2.png" alt></p>
<h1 id="Particle"><a href="#Particle" class="headerlink" title="Particle"></a>Particle</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic3.png" alt></p>
<h1 id="Magent"><a href="#Magent" class="headerlink" title="Magent"></a>Magent</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic4.png" alt></p>
<h1 id="OpenAI-Gym"><a href="#OpenAI-Gym" class="headerlink" title="OpenAI Gym"></a>OpenAI Gym</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic5.png" alt></p>
<h1 id="OpenAI-Gym-Retro"><a href="#OpenAI-Gym-Retro" class="headerlink" title="OpenAI Gym Retro"></a>OpenAI Gym Retro</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic6.png" alt></p>
<h1 id="ProGen"><a href="#ProGen" class="headerlink" title="ProGen"></a>ProGen</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic7.png" alt></p>
<h1 id="Malmo"><a href="#Malmo" class="headerlink" title="Malmo"></a>Malmo</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic8.png" alt></p>
<h1 id="Obstacle-Tower"><a href="#Obstacle-Tower" class="headerlink" title="Obstacle Tower"></a>Obstacle Tower</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic9.png" alt></p>
<h1 id="Torcs赛车"><a href="#Torcs赛车" class="headerlink" title="Torcs赛车"></a>Torcs赛车</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic10.png" alt></p>
<h1 id="DeepMind-Lab"><a href="#DeepMind-Lab" class="headerlink" title="DeepMind Lab"></a>DeepMind Lab</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic11.png" alt></p>
<h1 id="Hard-Eight"><a href="#Hard-Eight" class="headerlink" title="Hard Eight"></a>Hard Eight</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic12.png" alt></p>
<h1 id="DeepMind-Control"><a href="#DeepMind-Control" class="headerlink" title="DeepMind Control"></a>DeepMind Control</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic13.png" alt></p>
<h1 id="VizDoom"><a href="#VizDoom" class="headerlink" title="VizDoom"></a>VizDoom</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic14.png" alt></p>
<h1 id="Pommerman"><a href="#Pommerman" class="headerlink" title="Pommerman"></a>Pommerman</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic15.png" alt></p>
<h1 id="Multiagent-emergence"><a href="#Multiagent-emergence" class="headerlink" title="Multiagent emergence"></a>Multiagent emergence</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic16.png" alt></p>
<h1 id="Quake-III-Arena-Capture-the-Flag"><a href="#Quake-III-Arena-Capture-the-Flag" class="headerlink" title="Quake III Arena Capture the Flag"></a>Quake III Arena Capture the Flag</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic17.png" alt></p>
<h1 id="Google-Research-Football"><a href="#Google-Research-Football" class="headerlink" title="Google Research Football"></a>Google Research Football</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic18.png" alt></p>
<h1 id="Neural-MMOs"><a href="#Neural-MMOs" class="headerlink" title="Neural MMOs"></a>Neural MMOs</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic19.png" alt></p>
<h1 id="StarCraft-II"><a href="#StarCraft-II" class="headerlink" title="StarCraft II"></a>StarCraft II</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic20.png" alt></p>
<h1 id="Unity-ML-Agents-Tooklkit"><a href="#Unity-ML-Agents-Tooklkit" class="headerlink" title="Unity ML-Agents Tooklkit"></a>Unity ML-Agents Tooklkit</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic21.png" alt></p>
<h1 id="Al2Thor"><a href="#Al2Thor" class="headerlink" title="Al2Thor"></a>Al2Thor</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic22.png" alt></p>
<h1 id="潮人篮球"><a href="#潮人篮球" class="headerlink" title="潮人篮球"></a>潮人篮球</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/pic23.png" alt></p>
<h1 id="Arcade-Learning-Environment"><a href="#Arcade-Learning-Environment" class="headerlink" title="Arcade-Learning-Environment"></a>Arcade-Learning-Environment</h1><p><a target="_blank" rel="noopener" href="https://github.com/mgbellemare/Arcade-Learning-Environment">github</a></p>
<h1 id="Fighting-ICE"><a href="#Fighting-ICE" class="headerlink" title="Fighting ICE"></a>Fighting ICE</h1><p><a target="_blank" rel="noopener" href="https://github.com/myt1996/gym-fightingice">github</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/14/TD3-Addressing-Function-Approximation-Error-in-Actor-Critic-Methods/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/14/TD3-Addressing-Function-Approximation-Error-in-Actor-Critic-Methods/" class="post-title-link" itemprop="url">TD3_Addressing Function Approximation Error in Actor-Critic Methods</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-07-14 22:15:33 / Modified: 23:44:40" itemprop="dateCreated datePublished" datetime="2021-07-14T22:15:33+08:00">2021-07-14</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>2018年<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1802.09477.pdf">paper</a></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    在deep Q-learning等基于价值的强化学习方法中，函数近似误差会导致高估的值估计和次优策略。 我们表明这个问题在actor-critic环境中仍然存在，并提出了新的机制来最小化其对actor和评论critic的影响。 我们的算法建立在Double Q-learning的基础上，通过取一对critic之间的最小值来限制高估。 我们在目标网络和高估偏差之间建立了联系，并建议延迟策略更新以减少每次更新的错误并进一步提高性能。 我们在 OpenAI gym任务上评估我们的方法，在测试的每个环境中都优于最先进的方法。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    在具有离散动作空间的强化学习问题中，由于函数逼近误差而导致的价值高估问题得到了很好的研究。 然而在连续控制域中，actor-critic 方法的类似问题在很大程度上没有被触及。 在本文中，我们展示了在actor-critic环境中存在高估偏差和时间差分方法中的误差累积。 我们提出的方法解决了这些问题，并且大大优于当前的技术水平。</p>
<p>​    本文首先建立了在连续控制环境中，确定性策略梯度(Silver et al., 2014)也存在这种高估特性。此外，我们发现离散动作设置中无处不在的解决方案 Double DQN (Van Hasselt et al., 2016) 在actor-critic环境中无效。在训练期间，Double DQN 使用单独的目标值函数估计当前策略的值，允许在没有最大化偏差的情况下评估动作。不幸的是，由于actor-critic环境中的策略变化缓慢，当前和目标值的估计仍然过于相似，无法避免最大化偏差。这可以通过使用一对独立训练的critic,将较旧的变体 Double Q-learning (Van Hasselt, 2010) 改编为actor-critic格式来解决。虽然这允许较少偏值估计，但即使是无偏的具有高方差的估计仍然可能导致未来对状态空间局部区域的高估，这反过来又会对全球策略产生负面影响。为了解决这个问题，我们提出了一种裁剪Double Q-learning变体，它利用了一个概念，即遭受高估偏差的价值估计可以用作真实价值估计的近似上限。这有利于低估，这在学习过程中不会传播，因为策略避免了低值估计的动作。</p>
<p>​    鉴于噪声与高估偏差的联系，本文包含许多解决方差减少的组件。 首先我们表明目标网络是deep Q-learning方法中的一种常用方法，它通过减少错误的积累对于减少方差至关重要。 其次，为了解决价值和策略的耦合问题，我们建议延迟策略更新，直到价值估计收敛。 最后，我们引入了一种新的正则化策略，其中 SARSA-style update bootstraps类似的动作估计以进一步减少方差。</p>
<p>​    我们的修改应用于连续控制的最先进的 actor-critic 方法，即深度确定性策略梯度算法 (DDPG) (Lillicrap et al., 2015)，以形成Twin Delayed Deep Deterministic policy gradient算法（TD3）， actor-critic 算法考虑了策略和值更新中函数逼近误差之间的相互作用。 我们在来自 OpenAI gym(Brockman et al., 2016）的七个连续控制领域上评估我们的算法，在那里我们大大超过了最先进的技术。</p>
<p>​    鉴于最近对重现性的关注 (Henderson et al., 2017)，我们在大量seeds上进行实验，对每个贡献进行消融研究，我们将代码和学习曲线进行开源(<a target="_blank" rel="noopener" href="https://github.com/sfujim/TD3)。">https://github.com/sfujim/TD3)。</a></p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>Multi-step的方法在累积估计偏差和策略和环境引起的方差之间进行权衡。当step增加，会增加方差，减小偏差。<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/299738989">解释</a></p>
<h1 id="涉及paper"><a href="#涉及paper" class="headerlink" title="涉及paper"></a>涉及paper</h1><ul>
<li>Double Q-learning <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/96100933">概述</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">多智能体强化学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-07-13 20:49:57 / Modified: 22:31:39" itemprop="dateCreated datePublished" datetime="2021-07-13T20:49:57+08:00">2021-07-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV18z411q7Kc">网易伏羲多智能体强化学习</a></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic1.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic2.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic3.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic4.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic5.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic6.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic7.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic8.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic9.png" alt></p>
<p><strong>Distributed AI Agenda</strong>:一个系统里有多个agent，给每个agent分配一个算法，设定学习过程。让agent根据学习算法学习到一些策略，这些策略组合起来就是整个系统的最优策略。即一个系统控制多个agent。</p>
<p><strong>AI Agenda</strong>：单个agent处在一个系统中，不知道环境怎样，对手怎样，需要根据不同的情况作出不同的反应，获取最大收益。</p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic10.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic11.png" alt></p>
<p><strong>Deep MARL:</strong></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic12.png" alt></p>
<p><strong>集中式学习分布式执行</strong></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic13.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic14.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic15.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic16.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic17.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic18.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic19.png" alt></p>
<p><strong>coordination 一致</strong></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic20.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic21.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic22.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic23.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic24.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic25.png" alt></p>
<p><strong>Learning to Communicate</strong></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic26.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic27.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic28.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic29.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic30.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic31.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic32.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic33.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic34.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic35.png" alt></p>
<p><strong>Neural Network Design</strong></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic36.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic37.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic38.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic39.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic40.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic41.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic43.png" alt></p>
<p><strong>Opponent Exploitation</strong></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic42.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic44.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic45.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic46.png" alt></p>
<p><strong>Mmulti-Agent Exploration</strong></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic47.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic48.png" alt></p>
<ul>
<li>VDN</li>
<li>Q-mix</li>
<li>MADDPG</li>
<li>COMA</li>
<li>QTRAN</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/10/Accelerated-Methods-For-Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/10/Accelerated-Methods-For-Deep-Reinforcement-Learning/" class="post-title-link" itemprop="url">Accelerated Methods For Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-10 19:01:30" itemprop="dateCreated datePublished" datetime="2021-07-10T19:01:30+08:00">2021-07-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-07-11 14:03:13" itemprop="dateModified" datetime="2021-07-11T14:03:13+08:00">2021-07-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    深度强化学习(RL)最近取得了许多新的成功，但实验周转时间仍然是研究和实践中的一个关键瓶颈。我们研究如何优化现代计算机的深度RL算法，特别是CPU和GPU的组合。我们证实了策略梯度和Q-value学习算法都可以适应于使用许多并行模拟器实例的学习。我们进一步发现，可以使用比标准尺寸大得多的批量大小进行训练，而不会对样本复杂度或最终性能产生负面影响。我们利用这些事实来建立一个统一的并行化框架，这极大地加速了这两种算法的实验。所有的神经网络计算都使用GPU，加速了数据的收集和训练。我们的结果包括使用整个DGX-1在雅达利游戏上在几分钟内的学习成功策略，使用同步和异步算法。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    本文的贡献是一个并行深度RL的框架，包括推理和训练的GPU加速的新技术。</p>
<h1 id="Parallel-Accelerated-RL-Fraework"><a href="#Parallel-Accelerated-RL-Fraework" class="headerlink" title="Parallel,Accelerated RL Fraework"></a>Parallel,Accelerated RL Fraework</h1><p>​    我们考虑使用深度神经网络的基于 CPU 的模拟器环境和策略。 我们在这里描述了一套完整的深度强化学习并行化技术，这些技术在采样和优化期间都实现了高吞吐量。 我们对 GPU 一视同仁； 每个都执行相同的采样学习过程。 此策略可直接扩展到各种数量的 GPU。</p>
<h2 id="Synchronized-Sampling同步采样"><a href="#Synchronized-Sampling同步采样" class="headerlink" title="Synchronized Sampling同步采样"></a>Synchronized Sampling同步采样</h2><p>​    我们首先将多个 CPU 内核与单个 GPU 相关联。 多个模拟器在多个CPU核上并行运行，这些进程以同步方式执行环境步骤。 在每一步，所有单独的observation都被收集到一个批处理中进行推理，在提交最后一个观察后在 GPU 上调用。 一旦动作返回，模拟器就会再次步进，依此类推。 系统共享内存数组提供actor-server和模拟器进程之间的快速通信。</p>
<p>​    由于落后者效应，同步采样可能会变慢—在每一步等待最慢的进程。 step时间的差异源于不同模拟器状态的不同计算负载和其他随机波动。 随着并行进程数量的增加，落后者效应会恶化，但我们通过为每个进程堆叠多个独立的模拟器实例来减轻它。 对于每个推理批次，每个进程都会（按顺序）执行其所有模拟器。 这种安排还允许推理的批量大小增加到超过进程数（即 CPU 内核）。 示意图如图 1(a) 所示。 可以通过仅在优化暂停期间重置来避免长时间环境重置导致的减速。</p>
<p><img src="/2021/07/10/Accelerated-Methods-For-Deep-Reinforcement-Learning/figurea.png" alt></p>
<p>​    如果模拟和推理负载达到平衡，每个组件将有一半的时间空闲，所以我们组成两个交替的模拟器进程组。当一个组等待下一个操作时，另一个步骤和GPU在为每个组之间交替服务。交替保持了高利用率，并进一步隐藏了两者中最快的计算量的执行时间。</p>
<p>​    我们通过重复模板来组织多个GPU，均匀地分配可用的CPU核心。我们发现固定每个模拟器进程的CPU分配是有益的，并保留一个核心来运行每个GPU。实验部分包含了采样速度的测量值，采样速度随着环境实例数量的增加而增加。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/10/Distributed-Prioritized-Experience-Replay/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/10/Distributed-Prioritized-Experience-Replay/" class="post-title-link" itemprop="url">Distributed Prioritized Experience Replay</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-07-10 13:11:31 / Modified: 18:49:56" itemprop="dateCreated datePublished" datetime="2021-07-10T13:11:31+08:00">2021-07-10</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    我们提出了一种用于大规模深度强化学习的分布式架构，它使代理能够从比以前更多的数量级的数据中有效地学习。 该算法将actor与learner解耦：actor通过根据共享神经网络选择动作与自己的环境实例进行交互，并将由此产生的经验积累在共享经验回放记忆中； learner重放经验样本并更新神经网络。 该架构依赖于优先经验重放，以只关注actor生成的最重要的数据。 我们的架构大大提高了在Arcade Learning Environment的水平，通过短时间训练实现了更好的最终性能。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    深度学习的广泛趋势是将更多计算与更强大的模型和大型数据集相结合可产生更令人印象深刻的结果，强化学习也类似。在本文中描述了一种通过生成更多数据并以优先级方式进行选择来扩大深度强化学习的方法。我们使用这种分布式体系结构来扩展DQN和DDPG的变体。通过允许代理从策略以前版本生成的数据中学习，经验重放也有助于防止过度拟合。</p>
<h1 id="Ape-X"><a href="#Ape-X" class="headerlink" title="Ape-X"></a>Ape-X</h1><p>​    在本文中，我们将优先级的经验重放prioritized experience replay扩展到分布式中，并表明这是一种高度可伸缩的深度强化学习方法，并且我们将我们的方法称为Ape-X。</p>
<p><img src="/2021/07/10/Distributed-Prioritized-Experience-Replay/architecture.png" alt></p>
<p><img src="/2021/07/10/Distributed-Prioritized-Experience-Replay/algorithm1.png" alt></p>
<p><img src="/2021/07/10/Distributed-Prioritized-Experience-Replay/algorithm2.png" alt></p>
<p>​    多个actor，每个actor都有自己的环境实例生成经验，将其添加到共享的经验重放内存中，并计算数据的初始优先级。（单个）learner从此内存中获取样本，并更新网络和内存中经验的优先级。使用learner提供的最新网络参数定期更新actor的网络。</p>
<p>​    原则上，actor和learner都可以分布在多个worker之间。在我们的实验中，数百个actor在CPU上运行以生成数据，而一个learner采样最有用的经验在GPU上运行。更新的网络参数会定期从learner那里传达给actor。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">李宏毅机器学习笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-06 22:46:32" itemprop="dateCreated datePublished" datetime="2021-07-06T22:46:32+08:00">2021-07-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-07-11 22:41:06" itemprop="dateModified" datetime="2021-07-11T22:41:06+08:00">2021-07-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Wv411h7kN?p=40">视频地址</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">QilongPan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">36</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QilongPan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
