<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="潘其龙">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="潘其龙">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="QilongPan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>潘其龙</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">潘其龙</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/" class="post-title-link" itemprop="url">强化学习常用游戏模拟环境</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-15 21:19:45" itemprop="dateCreated datePublished" datetime="2021-07-15T21:19:45+08:00">2021-07-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-07-20 23:39:33" itemprop="dateModified" datetime="2021-07-20T23:39:33+08:00">2021-07-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1GN411o7uy">网易伏羲视频地址</a></p>
<h1 id="Grid-world"><a href="#Grid-world" class="headerlink" title="Grid world"></a>Grid world</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic1.png" alt></p>
<h1 id="Multi-agent-Grid-world"><a href="#Multi-agent-Grid-world" class="headerlink" title="Multi-agent Grid world"></a>Multi-agent Grid world</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic2.png" alt></p>
<h1 id="Particle"><a href="#Particle" class="headerlink" title="Particle"></a>Particle</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic3.png" alt></p>
<h1 id="Magent"><a href="#Magent" class="headerlink" title="Magent"></a>Magent</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic4.png" alt></p>
<h1 id="OpenAI-Gym"><a href="#OpenAI-Gym" class="headerlink" title="OpenAI Gym"></a>OpenAI Gym</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic5.png" alt></p>
<h1 id="OpenAI-Gym-Retro"><a href="#OpenAI-Gym-Retro" class="headerlink" title="OpenAI Gym Retro"></a>OpenAI Gym Retro</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic6.png" alt></p>
<h1 id="ProGen"><a href="#ProGen" class="headerlink" title="ProGen"></a>ProGen</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic7.png" alt></p>
<h1 id="Malmo"><a href="#Malmo" class="headerlink" title="Malmo"></a>Malmo</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic8.png" alt></p>
<h1 id="Obstacle-Tower"><a href="#Obstacle-Tower" class="headerlink" title="Obstacle Tower"></a>Obstacle Tower</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic9.png" alt></p>
<h1 id="Torcs赛车"><a href="#Torcs赛车" class="headerlink" title="Torcs赛车"></a>Torcs赛车</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic10.png" alt></p>
<h1 id="DeepMind-Lab"><a href="#DeepMind-Lab" class="headerlink" title="DeepMind Lab"></a>DeepMind Lab</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic11.png" alt></p>
<h1 id="Hard-Eight"><a href="#Hard-Eight" class="headerlink" title="Hard Eight"></a>Hard Eight</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic12.png" alt></p>
<h1 id="DeepMind-Control"><a href="#DeepMind-Control" class="headerlink" title="DeepMind Control"></a>DeepMind Control</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic13.png" alt></p>
<h1 id="VizDoom"><a href="#VizDoom" class="headerlink" title="VizDoom"></a>VizDoom</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic14.png" alt></p>
<h1 id="Pommerman"><a href="#Pommerman" class="headerlink" title="Pommerman"></a>Pommerman</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic15.png" alt></p>
<h1 id="Multiagent-emergence"><a href="#Multiagent-emergence" class="headerlink" title="Multiagent emergence"></a>Multiagent emergence</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic16.png" alt></p>
<h1 id="Quake-III-Arena-Capture-the-Flag"><a href="#Quake-III-Arena-Capture-the-Flag" class="headerlink" title="Quake III Arena Capture the Flag"></a>Quake III Arena Capture the Flag</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic17.png" alt></p>
<h1 id="Google-Research-Football"><a href="#Google-Research-Football" class="headerlink" title="Google Research Football"></a>Google Research Football</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic18.png" alt></p>
<h1 id="Neural-MMOs"><a href="#Neural-MMOs" class="headerlink" title="Neural MMOs"></a>Neural MMOs</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic19.png" alt></p>
<h1 id="StarCraft-II"><a href="#StarCraft-II" class="headerlink" title="StarCraft II"></a>StarCraft II</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic20.png" alt></p>
<h1 id="Unity-ML-Agents-Tooklkit"><a href="#Unity-ML-Agents-Tooklkit" class="headerlink" title="Unity ML-Agents Tooklkit"></a>Unity ML-Agents Tooklkit</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic21.png" alt></p>
<h1 id="Al2Thor"><a href="#Al2Thor" class="headerlink" title="Al2Thor"></a>Al2Thor</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic22.png" alt></p>
<h1 id="潮人篮球"><a href="#潮人篮球" class="headerlink" title="潮人篮球"></a>潮人篮球</h1><p><img src="/2021/07/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%B8%B8%E6%88%8F%E6%A8%A1%E6%8B%9F%E7%8E%AF%E5%A2%83/nodejs/myBlog/source/_posts/强化学习常用游戏模拟环境/pic23.png" alt></p>
<h1 id="Arcade-Learning-Environment"><a href="#Arcade-Learning-Environment" class="headerlink" title="Arcade-Learning-Environment"></a>Arcade-Learning-Environment</h1><p><a target="_blank" rel="noopener" href="https://github.com/mgbellemare/Arcade-Learning-Environment">github</a></p>
<h1 id="Fighting-ICE"><a href="#Fighting-ICE" class="headerlink" title="Fighting ICE"></a>Fighting ICE</h1><p><a target="_blank" rel="noopener" href="https://github.com/myt1996/gym-fightingice">github</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/14/TD3-Addressing-Function-Approximation-Error-in-Actor-Critic-Methods/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/14/TD3-Addressing-Function-Approximation-Error-in-Actor-Critic-Methods/" class="post-title-link" itemprop="url">TD3_Addressing Function Approximation Error in Actor-Critic Methods</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-07-14 22:15:33 / Modified: 23:44:40" itemprop="dateCreated datePublished" datetime="2021-07-14T22:15:33+08:00">2021-07-14</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>2018年<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1802.09477.pdf">paper</a></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    在deep Q-learning等基于价值的强化学习方法中，函数近似误差会导致高估的值估计和次优策略。 我们表明这个问题在actor-critic环境中仍然存在，并提出了新的机制来最小化其对actor和评论critic的影响。 我们的算法建立在Double Q-learning的基础上，通过取一对critic之间的最小值来限制高估。 我们在目标网络和高估偏差之间建立了联系，并建议延迟策略更新以减少每次更新的错误并进一步提高性能。 我们在 OpenAI gym任务上评估我们的方法，在测试的每个环境中都优于最先进的方法。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    在具有离散动作空间的强化学习问题中，由于函数逼近误差而导致的价值高估问题得到了很好的研究。 然而在连续控制域中，actor-critic 方法的类似问题在很大程度上没有被触及。 在本文中，我们展示了在actor-critic环境中存在高估偏差和时间差分方法中的误差累积。 我们提出的方法解决了这些问题，并且大大优于当前的技术水平。</p>
<p>​    本文首先建立了在连续控制环境中，确定性策略梯度(Silver et al., 2014)也存在这种高估特性。此外，我们发现离散动作设置中无处不在的解决方案 Double DQN (Van Hasselt et al., 2016) 在actor-critic环境中无效。在训练期间，Double DQN 使用单独的目标值函数估计当前策略的值，允许在没有最大化偏差的情况下评估动作。不幸的是，由于actor-critic环境中的策略变化缓慢，当前和目标值的估计仍然过于相似，无法避免最大化偏差。这可以通过使用一对独立训练的critic,将较旧的变体 Double Q-learning (Van Hasselt, 2010) 改编为actor-critic格式来解决。虽然这允许较少偏值估计，但即使是无偏的具有高方差的估计仍然可能导致未来对状态空间局部区域的高估，这反过来又会对全球策略产生负面影响。为了解决这个问题，我们提出了一种裁剪Double Q-learning变体，它利用了一个概念，即遭受高估偏差的价值估计可以用作真实价值估计的近似上限。这有利于低估，这在学习过程中不会传播，因为策略避免了低值估计的动作。</p>
<p>​    鉴于噪声与高估偏差的联系，本文包含许多解决方差减少的组件。 首先我们表明目标网络是deep Q-learning方法中的一种常用方法，它通过减少错误的积累对于减少方差至关重要。 其次，为了解决价值和策略的耦合问题，我们建议延迟策略更新，直到价值估计收敛。 最后，我们引入了一种新的正则化策略，其中 SARSA-style update bootstraps类似的动作估计以进一步减少方差。</p>
<p>​    我们的修改应用于连续控制的最先进的 actor-critic 方法，即深度确定性策略梯度算法 (DDPG) (Lillicrap et al., 2015)，以形成Twin Delayed Deep Deterministic policy gradient算法（TD3）， actor-critic 算法考虑了策略和值更新中函数逼近误差之间的相互作用。 我们在来自 OpenAI gym(Brockman et al., 2016）的七个连续控制领域上评估我们的算法，在那里我们大大超过了最先进的技术。</p>
<p>​    鉴于最近对重现性的关注 (Henderson et al., 2017)，我们在大量seeds上进行实验，对每个贡献进行消融研究，我们将代码和学习曲线进行开源(<a target="_blank" rel="noopener" href="https://github.com/sfujim/TD3)。">https://github.com/sfujim/TD3)。</a></p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>Multi-step的方法在累积估计偏差和策略和环境引起的方差之间进行权衡。当step增加，会增加方差，减小偏差。<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/299738989">解释</a></p>
<h1 id="涉及paper"><a href="#涉及paper" class="headerlink" title="涉及paper"></a>涉及paper</h1><ul>
<li>Double Q-learning <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/96100933">概述</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">多智能体强化学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-07-13 20:49:57 / Modified: 22:31:39" itemprop="dateCreated datePublished" datetime="2021-07-13T20:49:57+08:00">2021-07-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV18z411q7Kc">网易伏羲多智能体强化学习</a></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic1.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic2.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic3.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic4.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic5.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic6.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic7.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic8.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic9.png" alt></p>
<p><strong>Distributed AI Agenda</strong>:一个系统里有多个agent，给每个agent分配一个算法，设定学习过程。让agent根据学习算法学习到一些策略，这些策略组合起来就是整个系统的最优策略。即一个系统控制多个agent。</p>
<p><strong>AI Agenda</strong>：单个agent处在一个系统中，不知道环境怎样，对手怎样，需要根据不同的情况作出不同的反应，获取最大收益。</p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic10.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic11.png" alt></p>
<p><strong>Deep MARL:</strong></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic12.png" alt></p>
<p><strong>集中式学习分布式执行</strong></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic13.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic14.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic15.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic16.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic17.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic18.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic19.png" alt></p>
<p><strong>coordination 一致</strong></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic20.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic21.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic22.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic23.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic24.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic25.png" alt></p>
<p><strong>Learning to Communicate</strong></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic26.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic27.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic28.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic29.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic30.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic31.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic32.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic33.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic34.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic35.png" alt></p>
<p><strong>Neural Network Design</strong></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic36.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic37.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic38.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic39.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic40.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic41.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic43.png" alt></p>
<p><strong>Opponent Exploitation</strong></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic42.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic44.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic45.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic46.png" alt></p>
<p><strong>Mmulti-Agent Exploration</strong></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic47.png" alt></p>
<p><img src="/2021/07/13/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/pic48.png" alt></p>
<ul>
<li>VDN</li>
<li>Q-mix</li>
<li>MADDPG</li>
<li>COMA</li>
<li>QTRAN</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/10/Accelerated-Methods-For-Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/10/Accelerated-Methods-For-Deep-Reinforcement-Learning/" class="post-title-link" itemprop="url">Accelerated Methods For Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-10 19:01:30" itemprop="dateCreated datePublished" datetime="2021-07-10T19:01:30+08:00">2021-07-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-07-11 14:03:13" itemprop="dateModified" datetime="2021-07-11T14:03:13+08:00">2021-07-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    深度强化学习(RL)最近取得了许多新的成功，但实验周转时间仍然是研究和实践中的一个关键瓶颈。我们研究如何优化现代计算机的深度RL算法，特别是CPU和GPU的组合。我们证实了策略梯度和Q-value学习算法都可以适应于使用许多并行模拟器实例的学习。我们进一步发现，可以使用比标准尺寸大得多的批量大小进行训练，而不会对样本复杂度或最终性能产生负面影响。我们利用这些事实来建立一个统一的并行化框架，这极大地加速了这两种算法的实验。所有的神经网络计算都使用GPU，加速了数据的收集和训练。我们的结果包括使用整个DGX-1在雅达利游戏上在几分钟内的学习成功策略，使用同步和异步算法。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    本文的贡献是一个并行深度RL的框架，包括推理和训练的GPU加速的新技术。</p>
<h1 id="Parallel-Accelerated-RL-Fraework"><a href="#Parallel-Accelerated-RL-Fraework" class="headerlink" title="Parallel,Accelerated RL Fraework"></a>Parallel,Accelerated RL Fraework</h1><p>​    我们考虑使用深度神经网络的基于 CPU 的模拟器环境和策略。 我们在这里描述了一套完整的深度强化学习并行化技术，这些技术在采样和优化期间都实现了高吞吐量。 我们对 GPU 一视同仁； 每个都执行相同的采样学习过程。 此策略可直接扩展到各种数量的 GPU。</p>
<h2 id="Synchronized-Sampling同步采样"><a href="#Synchronized-Sampling同步采样" class="headerlink" title="Synchronized Sampling同步采样"></a>Synchronized Sampling同步采样</h2><p>​    我们首先将多个 CPU 内核与单个 GPU 相关联。 多个模拟器在多个CPU核上并行运行，这些进程以同步方式执行环境步骤。 在每一步，所有单独的observation都被收集到一个批处理中进行推理，在提交最后一个观察后在 GPU 上调用。 一旦动作返回，模拟器就会再次步进，依此类推。 系统共享内存数组提供actor-server和模拟器进程之间的快速通信。</p>
<p>​    由于落后者效应，同步采样可能会变慢—在每一步等待最慢的进程。 step时间的差异源于不同模拟器状态的不同计算负载和其他随机波动。 随着并行进程数量的增加，落后者效应会恶化，但我们通过为每个进程堆叠多个独立的模拟器实例来减轻它。 对于每个推理批次，每个进程都会（按顺序）执行其所有模拟器。 这种安排还允许推理的批量大小增加到超过进程数（即 CPU 内核）。 示意图如图 1(a) 所示。 可以通过仅在优化暂停期间重置来避免长时间环境重置导致的减速。</p>
<p><img src="/2021/07/10/Accelerated-Methods-For-Deep-Reinforcement-Learning/figurea.png" alt></p>
<p>​    如果模拟和推理负载达到平衡，每个组件将有一半的时间空闲，所以我们组成两个交替的模拟器进程组。当一个组等待下一个操作时，另一个步骤和GPU在为每个组之间交替服务。交替保持了高利用率，并进一步隐藏了两者中最快的计算量的执行时间。</p>
<p>​    我们通过重复模板来组织多个GPU，均匀地分配可用的CPU核心。我们发现固定每个模拟器进程的CPU分配是有益的，并保留一个核心来运行每个GPU。实验部分包含了采样速度的测量值，采样速度随着环境实例数量的增加而增加。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/10/Distributed-Prioritized-Experience-Replay/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/10/Distributed-Prioritized-Experience-Replay/" class="post-title-link" itemprop="url">Distributed Prioritized Experience Replay</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-07-10 13:11:31 / Modified: 18:49:56" itemprop="dateCreated datePublished" datetime="2021-07-10T13:11:31+08:00">2021-07-10</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    我们提出了一种用于大规模深度强化学习的分布式架构，它使代理能够从比以前更多的数量级的数据中有效地学习。 该算法将actor与learner解耦：actor通过根据共享神经网络选择动作与自己的环境实例进行交互，并将由此产生的经验积累在共享经验回放记忆中； learner重放经验样本并更新神经网络。 该架构依赖于优先经验重放，以只关注actor生成的最重要的数据。 我们的架构大大提高了在Arcade Learning Environment的水平，通过短时间训练实现了更好的最终性能。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    深度学习的广泛趋势是将更多计算与更强大的模型和大型数据集相结合可产生更令人印象深刻的结果，强化学习也类似。在本文中描述了一种通过生成更多数据并以优先级方式进行选择来扩大深度强化学习的方法。我们使用这种分布式体系结构来扩展DQN和DDPG的变体。通过允许代理从策略以前版本生成的数据中学习，经验重放也有助于防止过度拟合。</p>
<h1 id="Ape-X"><a href="#Ape-X" class="headerlink" title="Ape-X"></a>Ape-X</h1><p>​    在本文中，我们将优先级的经验重放prioritized experience replay扩展到分布式中，并表明这是一种高度可伸缩的深度强化学习方法，并且我们将我们的方法称为Ape-X。</p>
<p><img src="/2021/07/10/Distributed-Prioritized-Experience-Replay/architecture.png" alt></p>
<p><img src="/2021/07/10/Distributed-Prioritized-Experience-Replay/algorithm1.png" alt></p>
<p><img src="/2021/07/10/Distributed-Prioritized-Experience-Replay/algorithm2.png" alt></p>
<p>​    多个actor，每个actor都有自己的环境实例生成经验，将其添加到共享的经验重放内存中，并计算数据的初始优先级。（单个）learner从此内存中获取样本，并更新网络和内存中经验的优先级。使用learner提供的最新网络参数定期更新actor的网络。</p>
<p>​    原则上，actor和learner都可以分布在多个worker之间。在我们的实验中，数百个actor在CPU上运行以生成数据，而一个learner采样最有用的经验在GPU上运行。更新的网络参数会定期从learner那里传达给actor。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/06/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">李宏毅机器学习笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-06 22:46:32" itemprop="dateCreated datePublished" datetime="2021-07-06T22:46:32+08:00">2021-07-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-07-11 22:41:06" itemprop="dateModified" datetime="2021-07-11T22:41:06+08:00">2021-07-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Wv411h7kN?p=40">视频地址</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/30/Open-ended-Learning-in-Symmetric-Zero-sum-Games/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/30/Open-ended-Learning-in-Symmetric-Zero-sum-Games/" class="post-title-link" itemprop="url">Open-ended Learning in Symmetric Zero-sum Games</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-30 20:06:18" itemprop="dateCreated datePublished" datetime="2021-06-30T20:06:18+08:00">2021-06-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-07-05 20:37:10" itemprop="dateModified" datetime="2021-07-05T20:37:10+08:00">2021-07-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    零和游戏，如国际象棋和扑克，抽象上是评估代理对的功能，例如将它们标记为“赢家”和“输家”。如果博弈是近似传递的，那么自我博弈会生成强度不断增加的代理序列。然而，非传递博弈，如石头剪刀布，可以表现出战略循环，并且不再有明确的目标——我们希望代理人增加实力，但不清楚和谁对战。在本文中，我们引入了一个几何框架，用于在零和游戏中制定代理目标，以构建产生开放式学习的自适应目标序列。该框架使我们能够对非传递博弈中的群体表现进行推理，并能够开发一种新算法（rectifified Nash response，<script type="math/tex">PSRO_{rN}</script>），使用博弈论的利基来构建不同的有效代理群体，产生比现有算法更强大的代理集。我们将<script type="math/tex">PSRO_{rN}</script> 应用于两个高度非传递性的资源分配游戏，发现<script type="math/tex">PSRO_{rN}</script>始终优于现有的替代方案。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    有一个故事，19 世纪中叶的一位剑桥导师曾宣称：“我教的是英国最聪明的男孩。” 他的同事反驳说：“我在教最好的考生。” 根据故事的版本，第一个男孩要么是Lord Kelvin，要么是 James Clerk Maxwell。 第二个男孩，在剑桥中得分最高，早已被遗忘了。</p>
<p>​    现代学习算法是优秀的测试者：一旦将问题打包成合适的目标，深度（强化）学习算法通常会找到好的解决方案。 然而，在许多多智能体领域，采取什么测试或优化什么目标的问题并不明确。 本文提出的算法可以自适应地不断提出新的有用目标，从而在两人零和游戏中实现开放式学习。 此设置的应用范围很广，并且足够通用，可以将函数优化作为特例。</p>
<p>​    在游戏中的学习通常被保守地表述为训练代理，平均打平或击败一套固定的对手。然而，双重的任务，即产生有用的对手来训练和评估，被研究不足。打败你所知道的特工是不够的；产生更好的对手也很重要，他们会表现出你不知道的行为。</p>
<p>​    有一些非常成功的例子通过自我游戏提出并解决一系列日益困难的问题(Silver et al., 2018; Jaderberg et al., 2018; Bansal et al., 2018; Tesauro, 1995)。不幸的是，很容易遇到nontransitive非传递游戏，在这些游戏中，自我游戏通过代理循环而不提高整体代理强度——同时对一个对手改进，对另一个对手恶化。在本文中，我们开发了一个分析非传递博弈的数学框架，并提出了系统地揭示和解决嵌入在博弈中的潜在问题的算法。</p>
<h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><p>​    本文从第 2 部分开始，介绍了函数形式博弈 (FFG) 作为参数化代理（如神经网络）所玩零和博弈的新数学模型。 定理 1 将任何 FFG 分解为传递分量和循环分量的总和。 传递博弈和密切相关的单调博弈是self-play的自然设置，但非传递博弈中存在的循环组件需要更复杂的算法，这激发了本文的其余部分。</p>
<p>​    处理非传递性博弈（不一定有最佳代理）的主要问题是理解目标应该是什么。 在第 3 节中，我们根据游戏场景（编码游戏中代理之间的交互的凸多边形）来制定全局目标。 如果游戏是可传递的或单调的，那么游戏场景就会退化为一维场景。 在非传递游戏中，游戏场景可以是高维的，因为针对一个智能体的训练可能与针对另一个智能体的训练有着根本的不同。</p>
<p>​    在非传递博弈中，衡量个体代理的表现是一件很烦恼的事情。 因此，在第 3 节中，我们开发了分析代理群体的工具，包括群体水平的性能度量defifinition 3。群体水平性能的一个重要特性是它随着游戏场景多面体在非传递游戏中的扩展而传递性增加。 因此，我们重新制定了从寻找最佳代理到扩大游戏景观的游戏学习问题。 为此，我们考虑了两种方法，一种与绩效直接相关，另一种侧重于多样性的度量defifinition 4。至关重要的是，该度量量化了不同的有效行为——我们对不会导致结果差异的策略差异不感兴趣，也对以新的和令人惊讶的方式失败的代理人不感兴趣。</p>
<p>​    第4节介绍了两种算法，一种是旧算法，另一种是新算法，用于扩大游戏场景。这些算法可以看作是在Lanctot et al. (2017)中引入的 policy space response oracle(PSRO)的专门化。第一个算法是Nash response (<script type="math/tex">PSRO_{N}</script>)，它是McMahan et al. (2003)对 double oracle algorithm的函数形式对策的扩展。给定一个种群，Nash response通过平均纳什均衡创造了一个训练的目标。纳什服务可作为“best agent”的代理，这种概念不能保证在一般的零和博弈中存在。第二种互补算法是rectifified Nash response(<script type="math/tex">PSRO_{rN}</script>)。该算法通过自适应地构建博弈论利基，鼓励代理“发挥优势，忽略弱点”来放大代理群体的战略多样性。</p>
<p>​    最后，在第5节中，我们研究了这些算法在 Colonel Blotto (Borel, 1921; Tukey, 1949; Roberson, 2006)中的性能和一个可微模拟，我们称为differentiable Lotto。Blotto-style games涉及到分配有限的资源，并且是高度不可传递的。我们发现<script type="math/tex">PSRO_{rN}</script>的性能优于<script type="math/tex">PSRO_{N}</script>，这两者在这些领域都大大优于self-play。我们还与一种响应均匀分布<script type="math/tex">PSRO_{U}</script>的算法进行了比较，它的性能与<script type="math/tex">PSRO_{N}</script>差不多。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>​    有大量关于新奇搜索、开放进化和好奇心novelty search, open-ended evolution, and curiosity的文献，这些文献的目的是不断拓展代理中游戏知识的前沿(Lehman &amp; Stanley, 2008; Taylor et al., 2016; Banzhaf et al., 2016; Brant &amp; Stanley, 2017; Pathak et al., 2017; Wang et al., 2019)。一个共同的线索是自适应目标，它迫使代理不断改进。例如，在novelty search中，目标会不断变化，因此不能简化为一个固定的目标进行一次性优化。</p>
<p>​    我们在游戏学习方面大量借鉴了前人的研究成果，尤其是Heinrich et al. (2015); Lanctot et al. (2017) ，下文将对此进行讨论。我们的设置类似于多目标优化 (Fonseca &amp; Fleming, 1993; Miettinen, 1998)。然而与多目标优化不同的是，我们同时关注目标的生成和优化。生成性对抗网络 (Goodfellow et al., 2014) 是零和博弈，由于缺乏对称性，不属于本文的研究范围，见附录？？。</p>
<h2 id="符号"><a href="#符号" class="headerlink" title="符号"></a>符号</h2><p>​    向量为列。0和1的常数向量为<strong>0</strong>和<strong>1</strong>。我们有时使用<script type="math/tex">p\left [ i \right ]</script>来表示向量<script type="math/tex">p</script>的第<script type="math/tex">i</script>条目。证明在附录中。</p>
<h1 id="Functional-form-games-FFGs"><a href="#Functional-form-games-FFGs" class="headerlink" title="Functional-form games (FFGs)"></a>Functional-form games (<strong>FFG</strong>s)</h1><p>​    假设，给定任意一对代理，我们可以计算出在围棋、国际象棋或星际争霸等游戏中一个打败对方的概率。我们将设置形式化如下。</p>
<h2 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition 1"></a>Definition 1</h2><p>​    设W是一组由神经网络的权值参数化的代理。对称零和函数形式博弈<strong>symmetric zero-sum functional-form game</strong>(FFG)是一个反对称函数<script type="math/tex">\varnothing \left ( v,w \right )=-\varnothing  \left ( w,v \right )</script>，它评估一对代理</p>
<p><img src="/2021/06/30/Open-ended-Learning-in-Symmetric-Zero-sum-Games/formula1.png" alt></p>
<p>​    较高的<script type="math/tex">\varnothing \left ( v,w \right )</script>，代理<script type="math/tex">v</script>就越好。对于<script type="math/tex">v</script>我们将<script type="math/tex">\varnothing > 0, \varnothing < 0, \varnothing = 0</script>称为赢、输和平手。</p>
<p>​    请注意<script type="math/tex">(i)</script>FFG中的策略是参数化的代理，<script type="math/tex">(ii)</script>代理的参数化被折叠到<script type="math/tex">\varnothing</script>中，因此游戏是代理的架构和环境本身的组合。</p>
<p>​    假设<script type="math/tex">v</script>击败<script type="math/tex">w</script>的概率，表示为<script type="math/tex">P\left ( v\succ w \right )</script>，可以计算或估计。赢/输概率可以通过<script type="math/tex">\varnothing \left ( v,w \right ):=P\left ( v\succ w \right )-\frac{1}{2}</script>或<script type="math/tex">\varnothing \left ( v,w \right ):=log\frac{P\left ( v\succ w \right )}{P\left ( v\prec w \right )}</script>呈现为反对称形式。  <script type="math/tex">\succ</script>可以表示偏好的意思。</p>
<h2 id="Tools-for-FFGs"><a href="#Tools-for-FFGs" class="headerlink" title="Tools for FFGs"></a>Tools for FFGs</h2><p>​        解决FFGs需要不同的方法来解决正常形式的游戏(Shoham &amp; Leyton-Brown, 2008) ，因为它们的连续性质。因此，我们开发了以下基本工具。</p>
<p>​    首先，<strong>curry</strong> operator将一个双人游戏转换为一个从代理到目标的函数</p>
<p><img src="/2021/06/30/Open-ended-Learning-in-Symmetric-Zero-sum-Games/formula2.png" alt>    </p>
<p>​    其次，一个 <strong>approximate best-response oracle</strong> ，给定代理<script type="math/tex">v</script>和目标<script type="math/tex">\varnothing _{w}\left ( \bullet  \right )</script>，如果可能，返回一个新的代理<script type="math/tex">v^{'}:=oracle\left ( v,\varnothing _{w}\left ( \bullet  \right ) \right )</script>和<script type="math/tex">\varnothing _{w}\left ( v^{'} \right ) > \varnothing _{w}\left ( v \right )+\epsilon</script>,oracle可以使用梯度、强化学习或进化算法。</p>
<p>​    第三，给定一个由<script type="math/tex">n</script>个代理组成的总体<script type="math/tex">\ss</script>（指图中的特殊符号），则<script type="math/tex">n \times n</script>反对称评价矩阵为</p>
<p><img src="/2021/06/30/Open-ended-Learning-in-Symmetric-Zero-sum-Games/formula3.png" alt></p>
<p>​    第四，我们将在<script type="math/tex">A_{\ss }</script>指定的零和矩阵对策上使用（不一定是唯一的）纳什均衡。</p>
<p>​    最后，我们使用下面的博弈分解。假设<script type="math/tex">W</script>是一个具有概率测量的compact set紧集。然后<script type="math/tex">W</script>上的可积反对称函数集形成了一个向量空间。附录D显示了如下内容：</p>
<pre><code>## Theorem 1(博弈分解)
</code></pre><p>​    每个FFG都会分解成一个传递博弈和循环博弈的和。</p>
<p><img src="/2021/06/30/Open-ended-Learning-in-Symmetric-Zero-sum-Games/formula4.png" alt></p>
<p>相对于一个适当定义的内积。</p>
<p>​    下面讨论了传递性对策和循环对策。很少有游戏是纯粹的传递的或循环的。然而理解这些情况很重要，因为一般算法至少应该在这两种特殊情况下都起作用。</p>
<h2 id="Transitive-games"><a href="#Transitive-games" class="headerlink" title="Transitive games"></a>Transitive games</h2><p>​    如果游戏有一个“rating function” <script type="math/tex">f</script>，因此游戏的性能就是ratings的差异，那么游戏就是过渡的：</p>
<p><img src="/2021/06/30/Open-ended-Learning-in-Symmetric-Zero-sum-Games/formula5.png" alt></p>
<p>​    换句话说，如果<script type="math/tex">\varnothing</script>允许“减法因式分解”。</p>
<p><strong>Optimization (training against a fifixed opponent)</strong>解决一个Transitive的游戏被简化为发现</p>
<p><img src="/2021/06/30/Open-ended-Learning-in-Symmetric-Zero-sum-Games/formula6.png" alt></p>
<p>​    至关重要的是，对对手<script type="math/tex">w</script>的选择对解决方案没有任何区别。因此最简单的学习算法是对固定对手进行训练，请参见算法1。</p>
<p><img src="/2021/06/30/Open-ended-Learning-in-Symmetric-Zero-sum-Games/alg1.png" alt></p>
<p><strong>Monotonic games</strong>一般的transitive games。如果有一个单调的函数<script type="math/tex">\sigma</script>，那么FFG是单调的</p>
<p><img src="/2021/06/30/Open-ended-Learning-in-Symmetric-Zero-sum-Games/formula7.png" alt></p>
<p>例如，Elo（1978）模拟了一个代理击败另一个代理的概率</p>
<p><img src="/2021/06/30/Open-ended-Learning-in-Symmetric-Zero-sum-Games/formula8.png" alt></p>
<p>对于一些<script type="math/tex">\sigma > 0</script>，其中<script type="math/tex">f</script>将Elo评级分配给代理。该模型在国际象棋、围棋等游戏中得到了广泛的应用。</p>
<p>在单调的游戏中，优化对抗固定对手的表现很差。具体地说，如果Elo的模型成立，那么对更弱的对手的训练就不会产生学习信号，因为一旦当<script type="math/tex">f\left ( v_{t} \right )> > f\left ( w \right )</script>，sigmoid饱和时，梯度<script type="math/tex">\bigtriangledown _{v}\varnothing \left ( v_{t},w \right )\approx 0</script>就会消失。</p>
<p><strong>Self-play (algorithm 2)</strong> 生成一系列的对手。对一系列强度增强的对手进行训练可以防止梯度因较大的技能差异而导致梯度消失，因此自我发挥非常适合由等式(1)建模的游戏。self-play在国际象棋、围棋和其他游戏中已被证明有效 (Silver et al., 2018; Al-Shedivat et al., 2018)。</p>
<p><img src="/2021/06/30/Open-ended-Learning-in-Symmetric-Zero-sum-Games/formula9.png" alt></p>
<p>Self-play是一种开放式的学习算法：它提出和掌握了一系列的目标，而不是优化一个预先指定的目标。然而，self-play假设了传递性：本地改进(<script type="math/tex">v_{t+1}</script>击败了<script type="math/tex">v_{t}</script>)意味着全局改进(<script type="math/tex">v_{t+1}</script>beats <script type="math/tex">v_{1},v_{2},...,v_{t}</script>)。这个假设在非传递游戏中失败了，比如下面的nontransitive游戏。由于性能是不可传递的，对一个代理的改进并不能保证对其他代理的改进。</p>
<h2 id="Cyclic-games"><a href="#Cyclic-games" class="headerlink" title="Cyclic games"></a>Cyclic games</h2><p>​    一个游戏是循环的，如果</p>
<p><img src="/2021/06/30/Open-ended-Learning-in-Symmetric-Zero-sum-Games/formula10.png" alt></p>
<p>换句话说，对一些代理人的胜利必然与他人的损失相平衡。当agent同时玩移动或不完美的信息游戏，如石头剪刀布，扑克，或星际争霸时，通常会出现策略循环。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/23/Opponent-Modeling-in-Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/23/Opponent-Modeling-in-Deep-Reinforcement-Learning/" class="post-title-link" itemprop="url">Opponent Modeling in Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-06-23 19:57:45 / Modified: 20:13:51" itemprop="dateCreated datePublished" datetime="2021-06-23T19:57:45+08:00">2021-06-23</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    在多智能体设置中，对手建模是必要的，其中具有竞争目标的二级智能体也会调整他们的策略，但它仍然具有挑战性，因为策略之间会相互作用并发生变化。 以前的大部分工作都侧重于为特定应用开发概率模型或参数化策略。 受到深度强化学习最近成功的启发，我们提出了基于神经的模型，可以共同学习策略和对手的行为。 我们没有明确预测对手的动作，而是将对手的观察编码到deep Q-Network（DQN）中； 然而，我们使用多任务处理保留显式建模（如果需要）。 通过使用 Mixture-of-Experts 架构，我们的模型无需额外监督即可自动发现对手的不同策略模式。 我们在模拟足球比赛和流行的trivia游戏上评估我们的模型，显示出优于 DQN 及其变体的性能。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    在战略环境（例如协作或竞争任务）中工作的智能代理必须预测其他代理的行为并推断他们的意图。 这很重要，因为所有活跃的代理都会影响世界的状态。 例如，如果多人游戏 AI 可以预测他们的坏动作，它就可以利用次优玩家； 谈判代理人如果知道对方的底线，可以更快地达成协议； 自动驾驶汽车必须通过预测汽车和行人的去向来避免事故。 对手建模中的两个关键问题是要建模的变量以及如何使用预测信息。 然而，答案在很大程度上取决于具体的应用程序，而且之前的大多数工作 (Billings et al., 1998a; Southey et al.,2005; Ganzfried &amp; Sandholm, 2011)都专注于需要大量领域知识的扑克游戏。</p>
<p>​    我们的目标是在强化学习环境中建立一个通用的对手建模框架，使代理能够利用各种对手的特质。 首先，为了解释不断变化的行为，我们对对手策略的不确定性进行建模，而不是将其归类为一组刻板印象。 其次，当预测对手与学习世界动态分开时，通常需要领域知识。 因此，我们共同学习策略并概率性地对对手建模。</p>
<p>​    我们基于在第 3 节中最近的深度 Q 网络（Mnih et al. (2015, DQN)开发了一个新模型 DRON(Deep Reinforcement Opponent Network)。 DRON 有一个预测 Q 值的策略学习模块和一个推断对手策略的对手学习模块（Code and data: <a target="_blank" rel="noopener" href="https://github.com/hhexiy/opponent）。DRON">https://github.com/hhexiy/opponent）。DRON</a> 不是明确预测对手属性，而是根据过去的观察和 使用它（除了状态信息之外）来计算自适应响应。 更具体地说，我们提出了两种架构，一种使用简单的串联来组合两个模块，一种基于 Mixture-of-Experts 网络。 虽然我们隐式地模拟对手，但可以通过多任务处理添加额外的监督（例如采取的行动或策略）。</p>
<p>​    与之前专门用于特定应用的模型相比，DRON 的设计具有通用性，不需要了解可能的（参数化）游戏策略。</p>
<p>​    第二个贡献是在多代理设置中学习的 DQN 代理。 深度强化学习在各种任务中都表现出竞争力：arcade games (Mnih et al., 2015), object recognition (Mnih et al., 2014), and robot navigation (Zhang et al., 2015)。 然而，它主要应用于具有固定环境的单代理决策理论设置。 一个例外是Tampuu et al.(2015)，其中由独立 DQN 控制的两个代理在协作和竞争奖励下进行交互。 虽然他们的重点是具有已知控制器的多代理系统的集体行为，但我们从单个代理的角度进行研究，该代理必须在充满未知对手的随机环境中学习反应策略。</p>
<p>​    我们在第 4 节中针对两个任务评估了我们的方法：网格世界中模拟的两人足球游戏，以及针对在线玩家的真实问答游戏。 两款游戏的对手都采用不同的策略，需要不同的反制策略。 我们的模型始终比 DQN 基线取得更好的结果。 此外，我们展示了我们的方法对非平稳策略更稳健； 成功识别对手策略并做出相应反应。</p>
<h1 id="Deep-Q-Learning"><a href="#Deep-Q-Learning" class="headerlink" title="Deep Q-Learning"></a>Deep Q-Learning</h1><h1 id="Deep-Reinforcement-Opponent-Network"><a href="#Deep-Reinforcement-Opponent-Network" class="headerlink" title="Deep Reinforcement Opponent Network"></a>Deep Reinforcement Opponent Network</h1><p>​    在多代理设置中，环境受到所有代理的联合动作的影响。 从一个智能体的角度来看，一个动作在给定状态下的结果不再是稳定的，而是依赖于其他智能体的动作。 在本节中，我们首先分析多个代理对 Q-learning 框架的影响； 然后我们介绍 DRON 及其多任务变体。</p>
<h2 id="Q-Learning-with-Opponents"><a href="#Q-Learning-with-Opponents" class="headerlink" title="Q-Learning with Opponents"></a>Q-Learning with Opponents</h2><p>​    在 MDP 术语中，联合动作空间由<script type="math/tex">A^{M}=A_{1}\times A_{2} \times ...\times A_{n}</script> 定义, 其中 n 是代理的总数。 我们使用 a 表示我们控制的代理（主要代理）的动作，使用 o 表示所有其他代理（二级代理）的联合动作，例如 <script type="math/tex">\left ( a,o \right )\in A^{M}</script>。 类似地，转移概率变为<script type="math/tex">\tau ^{M}\left ( s,a,o,s^{'} \right )=Pr\left ( s^{'}|s,a,o \right )</script>，新的奖励函数为<script type="math/tex">R^{M}\left ( s,a,o,s^{'} \right )</script>。 我们的目标是在与次要代理的联合策略<script type="math/tex">\pi ^{0}</script>交互的情况下为主要代理学习最佳策略。</p>
<p>​    如果<script type="math/tex">\pi ^{0}</script>是固定的，那么多智能体 MDP 就简化为单智能体 MDP：对手可以被认为是世界的一部分。 因此，他们重新定义了转换和奖励：</p>
<p><img src="/2021/06/23/Opponent-Modeling-in-Deep-Reinforcement-Learning/formula1.png" alt></p>
<p>​    因此，一个代理可以忽略其他代理，标准 Q 学习就足够了。</p>
<p>​    然而，假设对手使用固定策略通常是不现实的。 其他代理也可能正在学习或适应以最大化奖励。 例如在策略游戏中，玩家可能会在开始时伪装自己的真实策略来愚弄对手； 获胜的球员通过防守来保护他们的领先优势； 输球的球员打得更积极。 在这些情况下，我们面对的对手的策略<script type="math/tex">\pi _{t}^{o}</script>会随着时间而变化。</p>
<p>​    考虑到其他代理的影响，第 2 节中最优策略的定义不再适用——有效性策略现在取决于次要的代理的策略。因此，我们定义了相对于对手联合策略的最优 Q 函数：<script type="math/tex">Q^{*|\pi ^{o}}=max_{\pi }Q^{\pi |\pi ^{o}}\left ( s,a \right )</script>  <script type="math/tex">\forall _{s}\in S</script>和 <script type="math/tex">\forall _{a}\in A</script>。 Q 值之间的循环关系成立：</p>
<p><img src="/2021/06/23/Opponent-Modeling-in-Deep-Reinforcement-Learning/formula2.png" alt></p>
<h2 id="DQN-with-Opponent-Modeling"><a href="#DQN-with-Opponent-Modeling" class="headerlink" title="DQN with Opponent Modeling"></a>DQN with Opponent Modeling</h2><p>​    给定等式 1，我们可以继续应用 Q-learning 并通过随机更新来估计转移函数和对手的策略。 然而，将对手视为世界的一部分会减缓对适应性对手的反应  (Uther &amp; Veloso, 2003)，因为行为的变化被世界的动态所掩盖。</p>
<p>​    为了明确地编码对手的行为，我们提出了联合<script type="math/tex">Q^{.|\pi ^{o}}</script> 和<script type="math/tex">\pi ^{o}</script>建模的Deep Reinforcement Opponent Network (DRON)。 DRON 是一个 Q 网络（<script type="math/tex">N_{Q}</script>），它评估一个状态和一个学习<script type="math/tex">\pi ^{o}</script>表示的对手网络(<script type="math/tex">N_{o}</script>)的动作。。 剩下的问题是如何结合两个网络以及使用什么监督信号。 为了回答第一个问题，我们研究了两种网络架构：连接<script type="math/tex">N_{Q}</script>和 <script type="math/tex">N_{o}</script>的 DRON-concat，以及应用 Mixture-of-Experts 模型的 DRON-MOE。</p>
<p>​    为了回答第二个问题，我们考虑两种设置：（a）仅预测 Q 值，因为我们的目标是最好的奖励而不是准确地模拟对手； (b) 还可以预测有关对手的额外信息（例如，他们的策略类型）。</p>
<h3 id="DRON-concat"><a href="#DRON-concat" class="headerlink" title="DRON-concat"></a>DRON-concat</h3><p>​    我们从状态 (<script type="math/tex">\phi ^{s}</script> 和对手 (<script type="math/tex">\phi ^{o}</script>) 中提取特征，然后使用带有整流(线性整流函数Relu)或卷积神经网络（<script type="math/tex">N_{Q}</script>和 <script type="math/tex">N_{o}</script>）的线性层将它们嵌入到单独的隐藏空间 (<script type="math/tex">h^{s}</script>和 <script type="math/tex">h^{o}</script>）。 为了将<script type="math/tex">\pi ^{o}</script>的知识整合到 Q 网络中，我们连接了状态和对手的表示（图 1a）然后联合预测 Q 值。 因此，神经网络的最后一层负责理解对手和 Q 值之间的交互。 由于只有一个 Q-Network，该模型需要对手的更具辨别力的表示来学习自适应策略。 为了缓解这种情况，我们的第二个模型基于等式 1 对对手的行为与 Q 值之间的关系进行了更强的先验编码。</p>
<p><img src="/2021/06/23/Opponent-Modeling-in-Deep-Reinforcement-Learning/figure1.png" alt></p>
<h3 id="DRON-MOE"><a href="#DRON-MOE" class="headerlink" title="DRON-MOE"></a>DRON-MOE</h3><p>​    等式 1 的右边部分可以写成 <script type="math/tex">\sum _{o_{t}}\pi _{t}^{o}\left ( o_{t}|s_{t} \right )Q^{\pi }\left ( s_{t},a_{t},o_{t} \right )</script>，这是对不同对手行为的期望。 我们使用  Mixture-of-Experts network (Jacobs et al., 1991)将对手动作明确建模为隐藏变量并对其进行边缘化（图 1b）。 通过组合来自多个专家网络的预测获得预期 Q 值：</p>
<p><img src="/2021/06/23/Opponent-Modeling-in-Deep-Reinforcement-Learning/formula3.png" alt></p>
<p>​    每个专家网络预测当前状态下可能的奖励。 基于对手表示的门控网络计算组合权重（专家分布）：</p>
<p><img src="/2021/06/23/Opponent-Modeling-in-Deep-Reinforcement-Learning/formula4.png" alt></p>
<p>​    这里 <script type="math/tex">f\left ( . \right )</script>是一个非线性激活函数（所有实验都是 ReLU），W 代表线性变换矩阵，b 是偏置项。</p>
<p>​    与 DRON-concat 忽略世界和对手行为之间的相互作用不同，DRON-MOE 知道 Q 值根据<script type="math/tex">\phi ^{o}</script>有不同的分布； 每个专家网络捕获一种类型的对手策略。</p>
<h3 id="Multitasking-with-DRON"><a href="#Multitasking-with-DRON" class="headerlink" title="Multitasking with DRON"></a>Multitasking with DRON</h3><p>​    前两个模型仅预测 Q 值，因此通过来自 Q 值的反馈间接学习对手表示。 关于对手的额外信息可以直接监督 <script type="math/tex">N_{o}</script>。许多游戏除了在游戏结束时显示最终奖励外，还会显示其他信息。 至少代理已经观察到对手在过去状态中采取的行动； 有时他们的私人信息，例如扑克中的隐藏牌。 更高级的信息包括抽象的计划或策略。 这些信息反映了对手的特征，有助于策略学习。</p>
<p>​    与之前学习单独模型来预测对手的这些信息的工作不同 (Davidson, 1999;Ganzfried &amp; Sandholm, 2011; Schadd et al., 2007)，我们应用多任务学习并使用观察作为额外监督来学习共享对手表示<script type="math/tex">h^o</script>。 图 2 展示了多任务 DRON 的架构，其中监督是 yo 。 与显式对手建模相比，多任务处理的优势在于它使用了游戏和对手的高级知识，同时对不足的对手数据和 Q 值建模错误保持鲁棒性。 在第 4 节中，我们使用两种类型的监督信号评估多任务 DRON：未来行动和对手的整体策略。</p>
<p><img src="/2021/06/23/Opponent-Modeling-in-Deep-Reinforcement-Learning/figure2.png" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>​    在本节中，我们在两个任务上评估我们的模型，soccer game和quiz bowl。 这两项任务都有两个玩家互相对抗，对手表现出不同的行为。 我们将 DRON 模型与 DQN 进行比较，并分析它们对不同类型对手的反应。</p>
<p>​    所有系统都在相同的 Q-learning 框架下进行训练。 除非另有说明，实验具有以下配置：折扣因子<script type="math/tex">\gamma</script>为 0.9，参数由 AdaGrad  (Duchi et al., 2011) 优化，学习率为 0.0005，mini-batch大小为 64。我们使用<script type="math/tex">\epsilon</script>-greddy在训练期间探索，从探索率 0.3 开始，在 500,000 步内线性衰减到 0.1。 我们将所有模型训练 50 个 epoch。 交叉熵用作多任务学习中的损失。</p>
<h2 id="Soccer"><a href="#Soccer" class="headerlink" title="Soccer"></a>Soccer</h2><p>​    我们的第一个测试平台是在之前的多人游戏工作之后的soccer变体(Littman, 1994; Collins, 2007; Uther &amp; Veloso, 2003)。 比赛由 A 和 B 两名球员在<script type="math/tex">6\times 9</script>格子（图 3）上进行(尽管游戏是在网格世界中进行的，但我们并没有像以前的工作那样以表格形式表示 Q 函数。 因此它可以推广到更复杂的基于像素的设置)。比赛开始时 A 和 B 在左右半场（球门除外）的随机方格中，球传给其中之一。 玩家从五个动作中选择：移动 N、S、W、E 或站着不动（图 3（1））。 如果一个动作将玩家带到一个阴影方块或边界外，则该动作无效。 如果两个玩家移动到同一个方格，移动前拥有球的玩家将球输给对手（图 3（2）），移动不会发生。</p>
<p><img src="/2021/06/23/Opponent-Modeling-in-Deep-Reinforcement-Learning/figure3.png" alt></p>
<p>​    如果玩家将球带到对手的球门（图 3（3）、（4）），则游戏结束，则该玩家得分一分。 如果双方在一百步内都没有进球，则比赛以零-零平局结束。</p>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>​    我们设计了一个基于规则的两种模式代理作为对手图 3（右）。 在进攻模式下，代理总是优先考虑进攻而不是防守。 在与随机代理的 5000 场比赛中，它获胜的时间为 99.86%，平均episode长度为 10.46。 在防守模式下，代理只专注于捍卫自己的目标。 结果它赢得了 31.80% 的比赛，并58.40% 的比赛平局； 平均episode长度为81.70。 很容易找到在任一模式下击败对手的策略，但是该策略不适用于两种模式，如表 2 所示。因此，智能体在每个游戏中随机选择两种模式来创建不同的策略。</p>
<p><img src="/2021/06/23/Opponent-Modeling-in-Deep-Reinforcement-Learning/table2.png" alt></p>
<p>​    输入状态是一个<script type="math/tex">1\times 15</script>的向量，表示agent、对手、场地的轴边界、球门区域的位置和控球权的坐标。 我们通过五种情况来定义玩家的移动：接近代理、避开代理、接近代理球门、接近自己球门和静止不动。 对手特征包括观察到的对手移动的频率、其最近的移动和动作以及将球丢给对手的频率。</p>
<p>​    基线 DQN 有两个隐藏层，都有 50 个隐藏单元。 我们称这个模型为 DQN-world：对手被建模为世界的一部分。 DRON 中对手网络的隐藏层也有 50 个隐藏单元。 对于多任务处理，我们试验了两个监督信号，当前状态下的对手动作（+action）和对手模式（+type）。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/" class="post-title-link" itemprop="url">DouZero:Mastering DouDizhu with Self-Play Deep Reinforcement Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-22 21:51:20" itemprop="dateCreated datePublished" datetime="2021-06-22T21:51:20+08:00">2021-06-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-06-23 22:52:30" itemprop="dateModified" datetime="2021-06-23T22:52:30+08:00">2021-06-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    游戏是现实世界的抽象，在那里人造的代理学会与其他代理竞争和合作。尽管在各种完美和不完美信息游戏中取得了重大成就，但三人纸牌游戏斗地主（又名斗地主）仍未解决。斗地主是一个非常具有挑战性的领域，竞争、协作、信息不完善、状态空间大，尤其是大量可能的行为（合法行为在不同回合之间差异很大）。不幸的是，现代强化学习算法主要关注简单和小的动作空间，毫不奇怪，在斗地主上没有取得令人满意的进展。在这项工作中，我们提出了一个概念上简单但有效的斗地主 AI 系统，即 DouZero，它通过深度神经网络、动作编码和并行actor增强了传统的蒙特卡洛方法。从零开始，在单台四颗GPU的服务器上，训练天数超过了所有现有的斗地主AI程序，在344个AI代理中在Botzone排行榜中名列第一。通过构建 DouZero，我们展示了经典的 Monte-Carlo 方法可以在具有复杂动作空间的硬领域中提供强大的结果。发布了代码和在线演示<a target="_blank" rel="noopener" href="https://github.com/kwai/DouZero">repo</a>，希望这种见解可以激发未来的工作。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    游戏通常作为 AI 的基准，因为它们是许多现实世界问题的抽象。在完全信息博弈方面取得了显著成果。例如，AlphaGo (Silver et al., 2016)、AlphaZero (Silver et al., 2018) 和 MuZero (Schrittwieser et al., 2020) 已经在围棋游戏中取得了最先进的性能。最近的研究已经发展到更具挑战性的不完美信息游戏，其中代理在部分可观察的环境中与其他人竞争或合作。双人游戏中取得了鼓舞人心的进展，例如简单的 Leduc Hold’em and limit/no-limit Texas Hold’em (Zinkevich et al., 2008; Heinrich &amp; Silver, 2016; Moravck et al., 2017; Brown &amp; Sandholm, 2018)，多人游戏，例如多人Texas hold’em (Brown &amp; Sandholm, 2019b)、 Starcraft (Vinyals et al., 2019), DOTA (Berner et al., 2019), Hanabi (Lerer  et al., 2020), Mahjong (Li et al., 2020a), Honor of Kings (Ye et al., 2020b;a), and No-Press Diplomacy (Gray et al., 2020)。</p>
<p>​    这项工作旨在为斗地主（又名斗地主）构建人工智能程序，斗地主是中国最受欢迎的纸牌游戏，每天有数亿活跃玩家。斗地主有两个有趣的特性，它们对人工智能系统构成了巨大的挑战。首先，斗地主中的玩家需要在一个部分可观察的环境中与其他人竞争和合作，交流有限。具体来说，两名农民玩家将组队对抗地主玩家。扑克游戏的流行算法，例如 Counterfactual Regret Minimization (CFR) (Zinkevich et al., 2008)）及其变体，在这种复杂的三人游戏环境中通常没有声音。其次，斗地主拥有大量平均大小非常大的信息集，并且由于卡片的组合而具有非常复杂和庞大的动作空间，多达<script type="math/tex">10^{4}</script>个可能的动作（Zha et al., 2019a）。与德州扑克不同，斗地主中的动作不容易抽象，这使得搜索计算量大且常用的强化学习算法效果不佳。由于高估问题，Deep Q-Learning(DQN) (Mnih et al., 2015) 在非常大的动作空间中存在问题 (Zahavy et al., 2018)；策略梯度方法，例如 A3C (Mnih et al., 2016)，无法利用斗地主中的动作特征，因此无法像 DQN 那样自然地泛化不可见的动作 (Dulac-Arnold et al., 2015)。毫不奇怪，之前的工作表明，DQN 和 A3C 在斗地主上并没有取得令人满意的进展。在 (You et al., 2019) 中，即使经过 20 天的训练，DQN 和 A3C 对抗简单的基于规则的代理的胜率也低于 20%； (Zha et al., 2019a)中的 DQN 仅比均匀采样合法移动的随机代理略好。</p>
<p>​    之前已经做了一些努力，通过将人类启发式学习和搜索相结合来构建斗地主 AI。Combination Q-Network (CQN) (You et al., 2019)建议通过将动作解耦为分解选择和最终移动选择来减少动作空间。然而，分解依赖于人类启发式，并且非常缓慢。在实践中，经过二十天的训练，CQN 甚至无法击败简单的启发式规则。DeltaDou (Jiang et al., 2019)是第一个与顶级人类玩家相比达到人类水平表现的人工智能程序。它通过使用贝叶斯方法来推断隐藏信息并根据他们自己的策略网络对其他玩家的行为进行采样，从而实现了类似于 AlphaZero 的算法。为了抽象动作空间，DeltaDou 基于启发式规则预训练了一个kicker network(选择带牌，三带一，四带二等)。然而，kicker在斗地主中扮演着重要的角色，不能轻易抽象。kicker的错误选择可能会直接导致输掉比赛，因为它可能会破坏其他一些卡片类别，例如单顺。此外，贝叶斯推理和搜索在计算上是昂贵的。即使在使用监督回归启发式算法初始化网络时，训练 DeltaDou 也需要两个多月的时间(Jiang et al., 2019)。因此，现有的斗地主 AI 程序在计算上很昂贵，并且可能是次优的，因为它们高度依赖于人类知识的抽象。</p>
<p>​    在这项工作中，我们展示了 DouZero，这是一个概念上简单但有效的斗地主 AI 系统，没有抽象状态/动作空间或任何人类知识。 DouZero 使用深度神经网络、动作编码和并行actor增强了传统的蒙特卡罗方法 (Sutton &amp; Barto, 2018)。 DouZero 有两个可取的特性。首先，与 DQN 不同，它不容易受到高估偏差的影响。其次，通过将动作编码到 card matrices（卡片矩阵）中，它可以自然地泛化在整个训练过程中不常见的动作。这两个属性对于处理斗地主庞大而复杂的动作空间都至关重要。与许多树搜索算法不同，DouZero 基于采样，这使我们能够使用复杂的神经架构并在计算资源相同的情况下每秒生成更多数据。与之前许多依赖特定领域抽象的扑克 AI 研究不同，DouZero 不需要任何领域知识或底层动态知识。在只有 48 个内核和 4 个 1080Ti GPU 的单台服务器上从头开始训练，DouZero 在半天之内就超过了 CQN 和启发式规则，在两天内击败了我们的内部监督代理，并在 10 天内超越了 DeltaDou。广泛的评估表明，DouZero是迄今为止最强的斗地主人工智能系统。</p>
<p>​    通过构建 DouZero 系统，我们证明了经典的 Monte-Carlo 方法可以在需要推理巨大状态和动作空间上的竞争和合作的大型复杂纸牌游戏中取得出色的结果。我们注意到，一些工作还发现蒙特卡罗方法可以实现有竞争力的表现 (Mania et al., 2018; Zha et al., 2021a)并有助于稀疏奖励设置 (Guo et al., 2018; Zha et al.,  2021b)。与这些专注于简单和小型环境的研究不同，我们展示了蒙特卡洛方法在大型纸牌游戏中的强大性能。希望这一见解可以促进未来对解决多智能体学习、稀疏奖励、复杂动作空间和不完美信息的研究，我们发布了我们的环境和训练代码。与许多需要数千个 CPU 进行训练的扑克 AI 系统不同，例如 DeepStack (Moravcık et al., 2017)和 Libratus (Brown &amp; Sandholm, 2018)，DouZero 实现了合理的实验管道，只需要几天的训练大多数研究实验室都负担得起的单个 GPU 服务器。我们希望它可以激发该领域的未来研究并作为强大的基线。</p>
<h1 id="斗地主背景"><a href="#斗地主背景" class="headerlink" title="斗地主背景"></a>斗地主背景</h1><p>​    斗地主是一款流行的三人纸牌游戏，易学难精。 它在中国吸引了数亿玩家，每年举办许多比赛。 这是一个shedding-type（脱落类型）的游戏，玩家的目标是在其他玩家之前清空自己手中的所有牌。 两名农民玩家组队与另一位地主玩家作战。 如果农民玩家中的任何一个是第一个没有剩余牌的，则农民获胜。 每场比赛都有一个叫牌阶段，玩家根据手牌的强弱来竞标地主，还有一个打牌阶段，玩家轮流打牌。 我们在附录 A 中提供了详细介绍。</p>
<p>​    DouDizhu 仍然是多智能体强化学习的未解决基准 (Zha et al., 2019a; Terry et al., 2020)。两个有趣的特性使得斗地主特别难于解决。首先，农民需要合作对抗地主。例如，图10显示了一个典型的情况，底部的农民可以选择打一个小单张来帮助右边的农民获胜。其次，斗地主因卡牌组合而具有复杂而庞大的动作空间。有 27, 472种可能的组合，其中这些组合的不同子集对于不同的手牌是合法的。图1是一手牌的例子，它有391种合法组合，包括Solo（单牌）、Pair(对子)、Trio（三张）、Bomb（炸弹）、Plane（飞机）、Quad等。 动作空间不能轻易抽象，因为不正确的打牌可能会打破其他类别并直接导致在输掉一场比赛中。因此，构建斗地主AI具有挑战性，因为斗地主中的玩家需要在巨大的动作空间上进行竞争和合作的推理。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/fiture1.png" alt></p>
<h1 id="Deep-Monte-Carlo"><a href="#Deep-Monte-Carlo" class="headerlink" title="Deep Monte-Carlo"></a>Deep Monte-Carlo</h1><p>​    在本节中，我们重新审视Monte-Carlo (MC) 方法并介绍Deep Monte-Carlo (DMC)，它将 MC 与深度神经网络泛化为函数逼近。 然后我们将 DMC 与策略梯度方法（例如 A3C）和 DQN 进行讨论和比较，这些方法在斗地主中被证明是失败的 (You et al., 2019; Zha et al., 2019a)。</p>
<h2 id="Monte-Carlo-Methods-with-Deep-Neural-Networks"><a href="#Monte-Carlo-Methods-with-Deep-Neural-Networks" class="headerlink" title="Monte-Carlo Methods with Deep Neural Networks"></a>Monte-Carlo Methods with Deep Neural Networks</h2><p>​    蒙特卡洛 (MC) 方法是基于平均样本回报的传统强化学习算法 (Sutton &amp; Barto, 2018)。 MC 方法是为episodic任务设计的，其中experiences可以分为episodes并且所有episodes最终都终止。 为了优化策略<script type="math/tex">\pi</script>，每次访问 MC 可用于通过迭代执行以下过程来估计 Q-table <script type="math/tex">Q(s, a)</script>：</p>
<ol>
<li>使用<script type="math/tex">\pi</script>生成episode。</li>
<li>对于每个<script type="math/tex">s,a</script>出现在episode中，计算并更新<script type="math/tex">Q(s, a)</script>，并使用关于<script type="math/tex">s,a</script>的所有样本的平均回报。</li>
<li>对episode中的每个<script type="math/tex">s</script>，<script type="math/tex">\pi \left ( s \right )\leftarrow arg max_{a}Q\left ( s,a \right )</script>。</li>
</ol>
<p>​    Step 2 中的平均回报通常是通过折现累积奖励获得的。 与依赖bootstrapping的 Q-learning 不同，MC 方法直接逼近目标 Q 值。 在步骤 1 中，我们可以使用 epsilon-greedy 来平衡探索和利用。 上述过程可以与深度神经网络自然结合，从而导致深度蒙特卡罗（DMC）。 具体来说，我们可以用神经网络替换 Q 表，并在步骤 2 中使用均方误差 (MSE) 更新 Q 网络。</p>
<p>​    虽然 MC 方法被批评不能处理不完整的episode，并且由于高方差而被认为效率低下 (Sutton &amp; Barto, 2018)，但 DMC 非常适合斗地主。 首先，斗地主是一个episode任务，所以我们不需要处理不完整的episode。 其次，DMC 可以很容易地并行化，以每秒有效地生成许多样本，以缓解高方差问题。</p>
<h2 id="Comparison-with-Policy-Gradient-Methods"><a href="#Comparison-with-Policy-Gradient-Methods" class="headerlink" title="Comparison with Policy Gradient Methods"></a>Comparison with Policy Gradient Methods</h2><p>​    策略梯度方法，例如REINFORCE (Williams, 1992), A3C (Mnih et al., 2016), PPO (Schulman et al., 2017),  and IMPALA (Espeholt et al., 2018)，在强化学习中非常流行.他们的目标是直接使用梯度下降来建模和优化策略。在策略梯度方法中，我们经常使用类似分类器的函数逼近器，其中输出随动作数量线性缩放。虽然策略梯度方法在大动作空间中运行良好，但它们不能使用动作特征来推理以前未见过的动作 (Dulac-Arnold et al., 2015)。在实践中斗地主中的动作可以自然地编码成卡片矩阵，这对推理至关重要。例如，如果智能体因为选择了一个不错的kicker而获得了动作<script type="math/tex">3KKK</script>的奖励，它也可以将这些知识推广到未来看不见的动作，例如 <script type="math/tex">3JJJ</script>。此属性对于处理非常大的动作空间和加速学习至关重要，因为许多动作在模拟数据中并不常见。</p>
<p>​    DMC 可以通过将动作特征作为输入来自然地利用动作特征来概括看不见的动作。 虽然如果动作规模较大，执行复杂度可能会很高，但在斗地主的大多数状态下，只有一部分动作是合法的，因此我们不需要迭代所有动作。 因此，DMC 总体上是一种有效的斗地主算法。 虽然可以将动作特征引入到 actor-critic 框架中（例如，通过使用 Q 网络作为critic），但类分类器的 actor 仍然会受到大动作空间的影响。 我们的初步实验证实，这种策略不是很有效（见图 7）。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure7.png" alt></p>
<h2 id="Comparison-with-Deep-Q-Learning"><a href="#Comparison-with-Deep-Q-Learning" class="headerlink" title="Comparison with Deep Q-Learning"></a>Comparison with Deep Q-Learning</h2><p>​    最流行的基于值的算法是 Deep Q-Learning (DQN) (Mnih et al., 2015)，这是一种 bootstrapping方法，根据下一步中的Q 值更新Q 值。 虽然 DMC 和 DQN 都近似于 Q-values，但 DMC 在斗地主中有几个优点。</p>
<p>​    首先，在使用函数逼近时，由逼近 DQN 中的最大动作值引起的高估偏差难以控制（Thrun &amp; Schwartz 1993；Hasselt，2010），并且在动作空间非常大时变得更加明显 (Zahavy et al., 2018) . 虽然一些技术，例如double Q-learning (van Hasselt et al., 2016) 和experience replay (Lin, 1992)，可能会缓解这个问题，但我们在实践中发现 DQN 非常不稳定，并且经常在斗地主中发散。 然而蒙特卡洛估计不易受到偏差的影响，因为它无 bootstrapping即可直接逼近真实值 (Sutton &amp; Barto, 2018)。</p>
<p>​    其次，斗地主是一项具有长视野和稀疏奖励的任务，即智能体需要在没有反馈的情况下通过很长的状态链，并且只有在游戏结束时才会产生非零奖励。 这可能会减慢 Q-learning 的收敛速度，因为估计当前状态的 Q 值需要等到下一个状态的值接近其真实值 (Szepesvari, 2009; Beleznay et al., 1999)。与 DQN 不同，蒙特卡罗估计的收敛性不受episode长度的影响，因为它直接接近真实的目标值。</p>
<p>​    第三，由于动作空间大且可变，不方便在斗地主中高效实施 DQN。 具体来说，DQN 在每个更新步骤中的最大操作将导致高计算成本，因为它需要在非常昂贵的深度 Q 网络上迭代所有合法操作。 而且不同状态下的合法动作不同，这使得批量学习不方便。 结果我们发现 DQN 在 wall-clock time（现实时间）方面太慢了。 虽然蒙特卡洛方法可能存在高方差 (Sutton &amp; Barto, 2018)，这意味着它可能需要更多样本才能收敛，但它可以轻松并行化以每秒生成数千个样本，以缓解高方差问题并加速训练。 我们发现 DMC 的高方差被它提供的可扩展性大大抵消了，而且 DMC 在wall-clock time上非常有效。</p>
<h1 id="DouZero-System"><a href="#DouZero-System" class="headerlink" title="DouZero System"></a>DouZero System</h1><p>​    在本节中，我们通过首先描述state/action表示和神经架构来介绍 DouZero 系统，然后详细说明我们如何将 DMC 与多个进程并行化以稳定和加速训练。</p>
<h2 id="Card-Representation-and-Neural-Architecture"><a href="#Card-Representation-and-Neural-Architecture" class="headerlink" title="Card Representation and Neural Architecture"></a>Card Representation and Neural Architecture</h2><p>​    我们使用one-hot <script type="math/tex">4\times 15</script> 矩阵对每个卡片组合进行编码（图 2）。 由于花色在斗地主中是无关紧要的，我们用每一行来代表牌值（rank 和joker）。 图 3 显示了 Q-network的架构。 对于状态，我们提取几个卡片矩阵来表示手牌、其他玩家手牌的并集和最近的动作，以及一些one-hot向量来表示其他玩家的手牌数量和到目前为止炸弹的数量。 同样，我们使用一张卡片矩阵来编码动作。 对于神经架构，LSTM 用于对历史动作进行编码，并将输出与其他 state/action特征连接起来。 最后我们使用隐藏大小为 512 的六层 MLP 来生成Q-values。 我们在附录 C.1 中提供了更多详细信息。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure2.png" alt></p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure3.png" alt></p>
<h2 id="Parallel-Actors"><a href="#Parallel-Actors" class="headerlink" title="Parallel Actors"></a>Parallel Actors</h2><p>​    我们将 Landlord(地主)表示为 L，将在 Landlord 之前移动的玩家（上家）表示为 U，将在 Landlord 之后移动的玩家（下家）表示为 D。我们将 DMC 与多个actor进程和一个learner进程并行化，分别在<strong>Algorithm1</strong>和<strong>Algorithm2</strong>中进行了总结。 学习器为三个位置维护三个全局Q-network，并使用 MSE 损失更新网络，以根据actor进程提供的数据来近似目标值。 每个参与者维护三个本地Q-network，它们定期与全局网络同步。 actor将重复从游戏引擎中采样轨迹并计算每个state-action对的累积奖励。 learner和actor的交流是通过三个共享缓冲区实现的。 每个缓冲区被分成几个条目，其中每个条目由几个数据实例组成。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/algorithm1.png" alt></p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/algorithm2.png" alt></p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>​    这些实验旨在回答以下研究问题。 RQ1：DouZero 与现有的 DouDizhu 程序相比如何，例如基于规则的策略、监督学习、基于 RL 的方法和基于 MCTS 的解决方案（第 5.2 节）？ RQ2：如果我们考虑叫分阶段（第 5.3 节），DouZero 的表现如何？ RQ3：DouZero 的训练效率如何（第 5.4 节）？ RQ4：DouZero 与bootstrapping和actor critic方法（第 5.5 节）相比如何？ RQ5：DouZero 学习的打牌策略是否符合人类知识（第 5.6 节）？ RQ6：与现有程序（第 5.7 节）相比，DouZero 在推理方面是否具有计算效率？ RQ7：DouZero的两个农民能学会互相合作吗（5.8节）？</p>
<h2 id="实验装置"><a href="#实验装置" class="headerlink" title="实验装置"></a>实验装置</h2><p>​    扑克游戏中常用的策略强度衡量标准是可利用性(Johanson et al., 2011)。 但是在斗地主中，由于斗地主拥有巨大的状态/动作空间，并且有三个玩家，因此计算可利用性本身就很棘手。 为了评估性能，在 (Jiang et al., 2019)之后，我们发起了包括地主和农民两个对手的锦标赛。 我们通过对每一副牌打两次来减少差异。 具体来说，对于两个相互竞争的算法 A 和 B，对于给定的牌组，它们将首先分别扮演地主和农民的位置。 然后他们换边，即A占农民位置，B占地主位置，再次玩同一副牌。 为了模拟真实环境，在第 5.3 节中，我们进一步训练了一个带有监督学习的 bidding network（叫分网络），代理将根据手牌的强度在每场比赛中对地主进行叫分（更多细节见附录 C.2）。 我们考虑以下竞争算法。</p>
<ul>
<li><p><strong>DeltaDou:</strong>一个强大的人工智能程序，它使用贝叶斯方法来推断隐藏信息并使用 MCTS 搜索动作（Jiang 等，2019）。 我们使用作者提供的代码和预训练模型。 该模型经过两个月的训练，并显示出与顶级人类玩家相当的表现。</p>
</li>
<li><p><strong>CQN:</strong>Combinational Q-Learning (You et al., 2019) 是一个基于卡片分解和 Deep Q-Learning的程序。 我们使用开源代码和作者提供的预训练模型<a target="_blank" rel="noopener" href="https://github.com/qq456cvb/doudizhu-C">github</a>。</p>
</li>
<li><p><strong>SL:</strong> 监督学习基线。 我们在我们的斗地主游戏手机应用程序中内部收集了 226230 场来自联赛最高级别玩家的人类行家比赛。 然后我们使用与 DouZero 相同的状态表示和神经架构来训练监督代理，其中包含从这些数据生成的 49990075 个样本。 有关详细信息，请参阅附录 C.2。</p>
</li>
<li><p><strong>Rule-Based Programs:</strong>我们收集了一些基于启发式的开源程序，包括 RHCP4<a target="_blank" rel="noopener" href="https://blog.csdn.net/sm9sun/article/ details/70787814">link</a>，一个称为 RHCP-v2<a target="_blank" rel="noopener" href="https://github.com/deecamp2019-group20/">link</a></p>
<p>的改进版本，以及 RLCard 包中的规则模型<a target="_blank" rel="noopener" href="https://github.com/datamllab/rlcard">link</a> (Zha et al., 2019a)。 此外我们考虑了一个 Random 程序，它可以均匀地采样合法的移动。</p>
</li>
</ul>
<p><strong>Metrics</strong>下面 (Jiang et al., 2019)，给定算法 A 和对手 B，我们使用两个指标来比较 A 和 B 的性能：</p>
<ul>
<li><strong>WP</strong> (Winning Percentage):A赢的局数除以总局数。</li>
<li><strong>ADP</strong> (Average Difference in Points):A 和 B 之间每场比赛得分的平均差异。基础分数为 1。每个炸弹将使得分翻倍。</li>
</ul>
<p>我们在实践中发现这两个指标鼓励不同风格的策略。 例如如果使用 ADP 作为奖励，agent 往往会非常谨慎地出炸弹，因为出炸弹是有风险的，可能会导致更大的 ADP 损失。 相比之下，以 WP 为目标，agent 往往会积极地出炸弹，即使它会失败，因为炸弹不会影响 WP。 我们观察到，就 ADP 而言，使用 ADP 训练的代理的性能略好于使用 WP 训练的代理，反之亦然。 接下来，我们分别以 ADP 和 WP 为目标来训练和报告两个 DouZero 代理的结果（对于 WP，我们根据智能体是赢了还是输了游戏，对最后的时间步长给出了 +1 或 -1 的奖励。 对于 ADP，我们直接使用 ADP 作为奖励。 DeltaDou 和 CQN 分别以 ADP 和 WP 为目标进行训练）。 附录 D.2 中提供了对这两个目标的更多讨论。</p>
<p>​    我们首先通过让每对算法玩 10,000 副套牌来启动预赛。 然后，我们通过玩 100,000 副套牌来计算前 3 种算法的 Elo 评分，以进行更可靠的比较，即 DouZero、DeltaDou 和 SL。 如果算法在该套牌上进行的两场比赛中获得更高的 WP 或 ADP，则该算法将赢得该套牌。 我们用不同的随机采样套牌重复这个过程五次，并报告 Elo 分数的平均值和标准差。 对于叫分阶段的评估，每副牌打六次，不同位置的 DouZero、DeltaDou 和 SL 有不同的扰动。 我们用 100,000 副牌报告结果。</p>
<p><strong>Implementation Details.</strong>我们在具有 48 个 Intel(R) Xeon(R) Silver 4214R CPU @ 2.40GHz 处理器和四个 1080 Ti GPU 的服务器上运行所有实验。 我们使用 45个actor，这些actor分布在三个 GPU 上。 我们在剩余的 GPU 中运行一个学习器来训练Q-networks。 我们的实现基于TorchBeast framework (Kuttler et al., 2019)。 详细的训练曲线在附录 D.5 中提供。 每个共享缓冲区有 <script type="math/tex">B = 50</script>个条目，大小 <script type="math/tex">S = 100</script>，批量大小<script type="math/tex">M = 32</script> ，并且 <script type="math/tex">\epsilon= 0.01</script>。 我们设置折扣因子 <script type="math/tex">\gamma = 1</script>，因为斗地主只有一个非零的奖励在最后一个时间步，并且早期的动作非常重要。 我们使用 ReLU 作为 MLP 每一层的激活函数。 我们采用 RMSprop 优化器，其学习率 <script type="math/tex">\psi = 0.0001</script>，平滑常数为 0.99 且 <script type="math/tex">\epsilon = 10−5</script>。 我们训练 DouZero 30 天。</p>
<h2 id="Performance-against-Existing-Programs"><a href="#Performance-against-Existing-Programs" class="headerlink" title="Performance against Existing Programs"></a>Performance against Existing Programs</h2><p>​    为了回答 RQ1，我们将 DouZero 与离线基线进行比较，并在Botzone (Zhou et al., 2018)上报告其结果，这是一个 DouDizhu 比赛的在线平台（更多细节在附录 E 中提供）。</p>
<p>​    表 1 总结了 DouZero 和所有基线之间头对头完成的 WP 和 ADP。我们提出三点意见。首先，DouZero 主导了所有基于规则的策略和监督学习，这证明了在 DouDizhu 中采用强化学习的有效性。其次，DouZero 的性能明显优于 CQN。回想一下，CQN 类似地使用动作分解和 DQN 训练 Q 网络。 DouZero 的优越性表明 DMC 确实是在 DouDizhu 中训练 Q-networks 的有效方法。第三，DouZero优于DeltaDou，这是文献中最强的Doudizhu AI。我们注意到斗地主有很高的方差，即赢得一场比赛依赖于初始手牌的强度，这高度依赖于运气。因此，0.586 的 WP 和 0.258 的 ADP 表明对 DeltaDou 的显着改进。此外，DeltaDou 需要在训练和测试时进行搜索。而 DouZero 不做搜索，这验证了 DouZero 学习的 Q-networks 非常强大。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/table1.png" alt></p>
<p>​    图 4 的左侧显示了 DouZero、DeltaDou 和 SL 在玩 100, 000 副套牌时的 Elo 评分。 我们观察到 DouZero 在 WP 和 ADP 方面都明显优于 DeltaDou 和 SL。 这再次证明了DouZero的强劲表现。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure4.png" alt></p>
<p>​    图 4 的右侧说明了 DouZero 在 Botzone 排行榜上的表现。 我们注意到 Botzone 采用了不同的评分机制。 除了 WP 之外，它还为一些特定的卡片类别提供了额外的奖励，例如 Chain of Pair 和 Rocket（详见附录 E）。 如果以 Botzone 的评分机制为目标，DouZero 很有可能获得更好的性能，但我们直接上传了以 WP 为目标训练的 DouZero 的预训练模型。 我们观察到这个模型足够强大，可以击败其他机器人。</p>
<h2 id="Comparison-with-Bidding-Phase"><a href="#Comparison-with-Bidding-Phase" class="headerlink" title="Comparison with Bidding Phase"></a>Comparison with Bidding Phase</h2><p>​    为了研究 RQ2，我们使用人类专家数据训练一个具有监督学习的叫分网络。 我们将排名前 3 的算法，即 DouZero、DeltaDou 和 SL，放入 DouDizhu 游戏的三个席位。 在每场比赛中，我们随机选择第一个叫分者，并使用预先训练的叫分网络模拟叫分阶段。 为了公平比较，所有三种算法都使用相同的出价网络。 结果总结在表 2 中。虽然 DouZero 是在没有叫分网络的情况下在随机生成的牌组上训练的，但我们观察到 DouZero 在 WP 和 ADP 中都主导其他两种算法。 这证明了 DouZero 在需要考虑投标阶段的现实世界比赛中的适用性。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/table2.png" alt></p>
<h2 id="Analysis-of-Learning-Progress"><a href="#Analysis-of-Learning-Progress" class="headerlink" title="Analysis of Learning Progress"></a>Analysis of Learning Progress</h2><p>​    为了研究 RQ3，我们在图 5 中可视化了 DouZero 的学习进度。我们使用 SL 和 DeltaDou 作为对手来绘制 WP 和 ADP随着训练天数的变化。我们做出以下两点观察。首先，DouZero 在 WP 和 ADP 方面分别在一天和两天的训练中优于 SL。我们注意到 DouZero 和 SL 使用完全相同的神经架构进行训练。因此，我们将 DouZero 的优越性归功于自我对弈强化学习。虽然 SL 也表现良好，但它依赖于大量数据，不灵活并且可能限制其性能。其次，DouZero 在 WP 和 ADP 方面分别在 3 天和 10 天的训练中优于 DeltaDou。我们注意到 DeltaDou 是用启发式的监督学习初始化的，并且训练了两个多月。而 DouZero 从零开始，只需要几天的训练就可以击败 DeltaDou。这表明没有搜索的无模型强化学习在斗地主中确实有效。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure5.png" alt></p>
<p>​    我们进一步分析了使用不同数量的actor时的学习速度。 图 6 报告了使用 15、30 和 45 个 actor 时针对 SL 的性能。 我们观察到，使用更多的 actor 可以加速wall-clock time的训练。 我们还发现所有三种设置都显示出相似的样本效率。 未来，我们将探索在多台服务器上使用更多actor的可能性，以进一步提高训练效率。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure6.png" alt></p>
<h2 id="Comparison-with-SARSA-and-Actor-Critic"><a href="#Comparison-with-SARSA-and-Actor-Critic" class="headerlink" title="Comparison with SARSA and Actor-Critic"></a>Comparison with SARSA and Actor-Critic</h2><p>​    为了回答 RQ4，我们实现了两个基于 DouZero 的变体。 首先，我们用Temporal-Difference (TD)  目标替换 DMC 目标。 这导致了SARSA的深度版本。 其次，我们实现了一个具有动作特征的 Actor-Critic 变体。 具体来说，我们使用 Q-network 作为具有动作特征的critic，并训练策略作为具有动作掩码的演员来消除非法动作。</p>
<p>​    图 7 显示了 SARSA 和 Actor-Critic 单次运行的结果。 首先，我们没有观察到使用 TD 学习的明显好处。 我们观察到 DMC 在wall-clock time和样本效率方面的学习速度略快于 SARSA。 可能的原因是 TD 学习在稀疏奖励设置中不会有太大帮助。 我们相信需要更多的研究来了解 TD 学习何时会有所帮助。 其次，我们观察到 Actor-Critic 失败了。 这表明简单地向评论家添加动作特征可能不足以解决复杂的动作空间问题。 未来，我们将研究是否可以将动作特征有效地融入到Actor-Critic框架中。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure7.png" alt></p>
<h2 id="Analysis-of-DouZero-on-Expert-Data"><a href="#Analysis-of-DouZero-on-Expert-Data" class="headerlink" title="Analysis of DouZero on Expert Data"></a>Analysis of <strong>DouZero</strong> on Expert Data</h2><p>​    对于 RQ5，我们在整个训练过程中计算 DouZero 在人类数据上的准确性。 我们将使用 ADP 训练的模型报告为客观模型，因为收集人类数据的游戏应用程序也采用 ADP。 图 8 显示了结果。 我们做了以下两个有趣的观察。 首先，在早期阶段，即训练的前五天，准确率不断提高。 这表明智能体可能已经学会了一些与人类专业知识相一致的策略，纯粹是自我游戏。 其次，经过五天的训练，准确率急剧下降。 我们注意到 ADP 对 SL 的影响在五天后仍在改善。 这表明智能体可能已经发现了一些人类无法轻易发现的新颖更强的策略，这再次验证了自我对弈强化学习的有效性。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure8.png" alt></p>
<h2 id="Comparison-of-Inference-Time"><a href="#Comparison-of-Inference-Time" class="headerlink" title="Comparison of Inference Time"></a>Comparison of Inference Time</h2><p>​    为了回答 RQ6，我们在图 9 中报告了每一步的平均推理时间。为了公平比较，我们在 CPU上评估了所有算法。 我们观察到 DouZero 比 DeltaDou、CQN、RHCP 和 RHCP-v2 快几个数量级。 这是意料之中的，因为 DeltaDou 需要执行大量的蒙特卡罗模拟，而 CQN、RHCP 和 RHCP-v2 需要昂贵的卡片分解。 而 DouZero 在每一步只执行一次神经网络的前向传递。 DouZero 的高效推理使我们能够每秒生成大量样本用于强化学习。 它还使得在实际应用程序中部署模型变得经济实惠。</p>
<h2 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h2><p>​    为了调查 RQ7，我们进行了案例研究以了解 DouZero 所做的决定。 我们从 Botzone 转储比赛日志，并用预测的 Q 值可视化最重要的动作。 我们在附录 F 中提供了大部分案例研究，包括好的和坏的案例。</p>
<p>​    图 10 显示了两个农民合作击败地主的典型案例。 右边的农民只剩下一张牌了。 在这里，最底层的农民可以玩一个小Solo来帮助其他农民获胜。 在研究 DouZero 预测的前三个动作时，我们进行了两个有趣的观察。 首先，我们发现DouZero 输出的所有top动作都是小Solo，有很高的取胜信心，说明DouZero 的两个Peasant可能已经学会了合作。 其次，动作 4 的预测 Q 值 (0.808) 远低于动作 3 (0.971) 的 Q 值。 一个可能的解释是仍然有一个 4，所以玩 4 可能不一定有助于农民获胜。 在实践中，在这种特定情况下，其他农民的唯一牌的等级不高于 4。 总的来说，在这种情况下，行动 3 确实是最好的举措。</p>
<p><img src="/2021/06/22/DouZero-Mastering-DouDizhu-with-Self-Play-Deep-Reinforcement-Learning/figure10.png" alt></p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>​    <strong>Search for Imperfect-Information Games.</strong>Counterfactual Regret Minimization (CFR) (Zinkevich et al., 2008)是一种领先的扑克游戏迭代算法，有许多变体 (Lanctot et al., 2009; Gibson et al., 2012; Bowling et al., 2015; Moravcık et al., 2017; Brown &amp; Sandholm, 2018; 2019a; Brown et al., 2019; Lanctot et al., 2019; Li et al., 2020b)。 然而，遍历斗地主的博弈树是计算密集型的，因为它有一棵巨大的树，有很大的分支因子。 此外，大多数先前的研究都集中在零和设置上。 虽然已经做出了一些努力来解决合作环境，例如blueprint policy (Lerer et al., 2020)，但对竞争和合作的推理仍然具有挑战性。 因此，斗地主还没有看到有效的类似 CFR 的解决方案。</p>
<p>​    <strong>RL for Imperfect-Information Games.</strong>最近的研究表明，强化学习 (RL) 可以在扑克游戏中获得有竞争力的表现 (Heinrich et al., 2015; Heinrich &amp; Silver, 2016; Lanctot et al., 2017)。 与 CFR 不同，强化学习基于采样，因此可以轻松推广到大型游戏。 RL 已成功应用于一些复杂的不完美信息游戏，例如Starcraft (Vinyals et al., 2019), DOTA (Berner et al., 2019) and Mahjong (Li et al., 2020a)）。 最近，RL+search 被探索并证明在扑克游戏中是有效的 (Brown et al., 2020)。 DeltaDou 采用了类似的思路，首先推断隐藏信息，然后使用 MCTS 将 RL 与n DouDizhu中的搜索结合起来（Jiang et al., 2019）。 然而，DeltaDou 的计算成本很高，并且严重依赖于人类的专业知识。 在实践中，即使没有搜索，我们的 DouZero 在几天的训练中也优于 DeltaDou。</p>
<h1 id="总结和未来工作"><a href="#总结和未来工作" class="headerlink" title="总结和未来工作"></a>总结和未来工作</h1><p>​    这项工作为斗地主提供了一个强大的人工智能系统。 一些独特的特性使得斗地主特别难以解决，例如，巨大的状态/动作空间以及关于竞争和合作的推理。 为了应对这些挑战，我们通过深度神经网络、动作编码和并行actor增强了经典的蒙特卡洛方法。 这导致了一个纯 RL 解决方案，即 DouZero，它在概念上简单但有效且高效。 广泛的评估表明，DouZero是迄今为止斗地主最强的人工智能程序。 我们希望简单的蒙特卡洛方法可以在如此困难的领域产生强大的策略的见解将激励未来的研究。</p>
<p>​    对于未来的工作，我们将探索以下方向。 首先，我们计划尝试其他神经架构，例如卷积神经网络和 ResNet (He et al., 2016)。 其次，我们将在强化学习的循环中参与叫分。 第三，我们将在训练和/或测试时将 DouZero 与搜索相结合 (Brown et al., 2020)，并研究如何平衡 RL 和搜索。 第四，探索off-policy学习，提高训练效率。 具体来说，我们将研究是否以及如何通过经验回放来提高wall-clock time和样本效率(Lin, 1992; Zhang &amp; Sutton, 2017; Zha et al., 2019b; Fedus et al., 2020)。 第五，我们将尝试对农民的合作进行明确建模（ (Panait &amp; Luke, 2005; Foerster et al., 2016; Raileanu et al., 2018; Lai et al., 2020)。 第六，我们计划尝试可扩展的框架，例如 SEED RL (Espeholt et al., 2019)。 最后但并非最不重要的一点是，我们将测试 Monte-Carlo 方法在其他任务上的适用性。</p>
<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h1 id="CQN-代码阅读记录doudizhu-c"><a href="#CQN-代码阅读记录doudizhu-c" class="headerlink" title="CQN 代码阅读记录doudizhu-c"></a>CQN 代码阅读记录doudizhu-c</h1><h2 id="TensorPack"><a href="#TensorPack" class="headerlink" title="TensorPack"></a>TensorPack</h2><h3 id="AutoEncoder"><a href="#AutoEncoder" class="headerlink" title="AutoEncoder"></a>AutoEncoder</h3><h4 id="main-py"><a href="#main-py" class="headerlink" title="main.py"></a>main.py</h4><p>训练编码解码器，将onehot60的走步编码为</p>
<h3 id="MA-Hierachical-Q"><a href="#MA-Hierachical-Q" class="headerlink" title="MA_Hierachical_Q"></a>MA_Hierachical_Q</h3><h4 id="env-py"><a href="#env-py" class="headerlink" title="env.py"></a>env.py</h4><h5 id="Env"><a href="#Env" class="headerlink" title="Env"></a>Env</h5><p>包含所有只能体的历史出牌，手牌，地主，地主牌，当前玩家等信息。</p>
<ul>
<li>get_state_prob：返回剩余牌分别属于另外两个玩家的概率</li>
</ul>
<h4 id="expreplay-py"><a href="#expreplay-py" class="headerlink" title="expreplay.py"></a>expreplay.py</h4><p>产生数据放入缓冲区中，进行经验回放。</p>
<p><strong>ReplayMemory</strong></p>
<p>缓冲区，用于存储训练数据。具有添加数据、取数据等方法。</p>
<p><strong>ExpReplay</strong></p>
<ul>
<li>get_combinations：手牌大于10张时：1 通过舞蹈链算法得到手中牌能组成的所有组合。2 找出手中牌能得到的所有合法走步（大过上家牌） 3 从所有组合中挑选出具有第二步中合法走步的组合。被动出牌时，_fine_mask是一个长度为一个组合的最大走步数量的值（MAX_NUM_GROUPS = 21）,如果组合中对应下标的走步能管上上家牌则为True，否则为False。主动出牌时fine_mask为空。当手牌小于等于10张时，采取另一种方式，详情见代码。fine_mask的大小为(comb数量，MAX_NUM_GROUPS )</li>
<li>get_state_and_action_spaces：首先通过get_combinations得到所有的组合情况。当组合情况数量大于指定的数量时（MAX_NUM_COMBS = 100），进行采样出最大数量组合。然后得到所有组合情况中的走步，然后通过encoding进行编码为state。当得到的组合情况下于MAX_NUM_COMBS 时，需要将最后一个组合重复多次，达到MAX_NUM_COMBS 。最后的state的大小为(MAX_NUM_COMBS ,MAX_NUM_GROUPS ,)</li>
<li>_populate_exp:返回值old_s表示状态,get_state_and_action_spaces的结果，act表示动作下标，reward表示奖励，isOver表示游戏是否结束，_comb_mask游戏结束为True，未结束为False。fine_mask表示与get_combinations解释一样。</li>
</ul>
<h4 id="DQNModel-py"><a href="#DQNModel-py" class="headerlink" title="DQNModel.py"></a>DQNModel.py</h4><p>总共分为两层模型。</p>
<p>第一层模型输入：手牌、上家牌、下家牌、其它两家每张牌的概率prob_state、当前手牌的所有组合、当前手牌所有组合中合法走步对应的fine_mask，输出最好的组合下标。</p>
<p>第二层模型输入：输入第一层模型输出的最好组合下标所对应的state，重复很多次作为输入。输出为最好的action所对应的下标。</p>
<h4 id="predictor-py"><a href="#predictor-py" class="headerlink" title="predictor.py"></a>predictor.py</h4><ul>
<li>predict通过模型输出最好走步，预测步骤同上DQNModel.py。</li>
</ul>
<h2 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h2><ul>
<li>经验回放采用prioritized replay dqn的方式</li>
</ul>
<h2 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h2><ul>
<li>模拟采取的方式是把其它两个玩家的牌混合在一起，然后提前发好牌，然后当作明牌进行模拟。上面这个过程会进行指定次数，而不是只进行一次。并且在模拟的过程中会设置一个提前结束的阈值，即达到该阈值后提前退出。在探索利用的过程使用的是UCT公式。</li>
<li>将doudizhu_base所得到得.so动态库放入每个项目build文件夹中</li>
<li>第一个模型传入所有的组合（一个组合有很多走步），通过模型选择出最好的拆分方式。通过手牌等历史信息再通过第二个模型选择最优走步。</li>
<li>模型配置在core/table.py里面</li>
<li>训练：python  main.py  —gpu 0 —load /home/pc/doudizhu-C-master/TensorPack/MA_Hierarchical_Q/train_log/DQN-60-MA-SELF_PLAY/checkpoint</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/21/Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="QilongPan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="潘其龙">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/21/Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks/" class="post-title-link" itemprop="url">Bag of Tricks for Image Classification with Convolutional Neural Networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2021-06-21 22:28:16 / Modified: 22:49:30" itemprop="dateCreated datePublished" datetime="2021-06-21T22:28:16+08:00">2021-06-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/" itemprop="url" rel="index"><span itemprop="name">图像分类</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    最近在图像分类研究中取得的大部分进展都归功于训练过程的改进，例如数据增强和优化方法的变化。 然而，在文献中，大多数改进要么作为实现细节被简要提及，要么仅在源代码中可见。 在本文中，我们将检查这些改进的集合，并通过消融研究凭经验评估它们对最终模型准确性的影响。 我们将证明，通过将这些改进结合在一起，我们能够显着改进各种 CNN 模型。 例如，我们将 ResNet-50 在 ImageNet 上的 top-1 验证准确率从 75.3% 提高到 79.29%。 我们还将证明图像分类精度的提高导致在其他应用领域（如目标检测和语义分割）中更好的迁移学习性能。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    自 2012 年 AlexNet [15] 推出以来，深度卷积神经网络已成为图像分类的主要方法。 从那时起，人们提出了各种新架构，包括 VGG [24]、NiN [16]、Inception [1]、ResNet [9]、DenseNet [13] 和 NASNet [34]。 同时，我们看到了模型精度提升的稳定趋势。 例如，ImageNet [23] 上的 top-1 验证准确率已从 62.5% (AlexNet) 提高到 82.7% (NASNet-A)。</p>
<p>​    然而这些进步不仅仅来自改进的模型架构，训练过程的改进，包括损失函数的变化、数据预处理和优化方法也发挥了重要作用。 过去几年已经提出了大量这样的改进，但受到的关注相对较少。 在文献中，大多数只是作为实现细节被简要提及，而其他的只能在源代码中找到。</p>
<p>​    在本文中，我们将研究一系列训练过程和模型架构改进，这些改进提高了模型的准确性，但几乎没有改变计算复杂性。 其中许多都是小“trick”，例如修改特定卷积层的步幅大小或调整学习率计划。 然而总的来说，它们有很大的不同。 我们将在多个网络架构和数据集上评估它们，并报告它们对最终模型准确性的影响。</p>
<p>​    我们的经验评估表明，一些技巧可以显着提高精度，将它们组合在一起可以进一步提高模型精度。 我们将应用所有技巧后的 ResNet-50 与表 1 中的其他相关网络进行比较。请注意，这些技巧在ImageNet上将 ResNet50 的 top-1 验证准确率从 75.3% 提高到 79.29%。 它还优于其他更新和改进的网络架构，例如 SE-ResNeXt-50。 此外，我们表明我们的方法可以推广到其他网络（Inception V3 [1] 和 MobileNet [11]）和数据集（Place365 [32]）。 我们进一步表明，使用我们的技巧训练的模型在其他应用领域（如目标检测和语义分割）中带来了更好的迁移学习性能。</p>
<p><img src="/2021/06/21/Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks/table1.png" alt></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">QilongPan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">32</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QilongPan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
