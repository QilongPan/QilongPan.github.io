<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>潘其龙</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="潘其龙">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="潘其龙">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="QilongPan">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="潘其龙" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">潘其龙</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Continuous-Control-With-Deep-Reinforcement-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/20/Continuous-Control-With-Deep-Reinforcement-Learning/" class="article-date">
  <time class="dt-published" datetime="2021-05-20T14:21:20.000Z" itemprop="datePublished">2021-05-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/05/20/Continuous-Control-With-Deep-Reinforcement-Learning/">Continuous Control With Deep Reinforcement Learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="CONTINUOUS-CONTROL-WITH-DEEP-REINFORCEMENT-LEARNING"><a href="#CONTINUOUS-CONTROL-WITH-DEEP-REINFORCEMENT-LEARNING" class="headerlink" title="CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING"></a>CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们将Deep Q-Learning成功的背后思想应用到连续的行动领域。我们提出了一个actor-critic,model-free基于确定性策略梯度的算法，可以在连续动作空间上运行。使用相同的学习算法、网络架构和超参数，我们的算法稳健地解决了20多个模拟物理任务，包括卡杆摆动、灵巧操作、腿运动和汽车驾驶等经典问题。我们的算法能够找到性能与完全访问动态系统的规划算法及其衍生物相当的策略。我们进一步证明，对于许多任务，该算法可以直接通过输入原始像素学习“end-to-end”策略。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    人工智能领域的主要目标之一是从未处理的、高维的、感官的输入中解决复杂的任务。最近，通过将深度学习中的感官处理与强化学习相结合，取得了重大进展，从而形成了DQN算法 。该算法能够在许多Atari电子游戏中，使用未经处理的像素作为输入，达到了人类水平的性能。 为此，使用了深度神经网络函数逼近器来估算动作值函数。</p>
<p>​    但是，尽管DQN解决了高维观测空间的问题，但它只能处理离散和低维动作空间。 许多有趣的任务，尤其是物理控制任务，具有连续的（实际值）和高维度的动作空间。 DQN不能直接应用于连续域，因为它依赖于找到可以最大程度地提高动作值函数，在连续值情况下，每个步骤都需要迭代优化过程。</p>
<p>​    使诸如DQN之类的深度强化学习方法适应连续领域的一种明显方法是简单地离散动作空间。但是，这有很多局限性，最显着的是维度灾难：动作的数量随自由度的增加而呈指数增长。例如，对于每个关节具有最粗糙的离散$$a_{i}\in\left { -k,0,k \right }$$的,关节数量为7的系统（如在人类手臂中）会导致具有以下维度的动作空间：$$3^7=2187$$ 。对于需要对动作进行精细控制的任务，情况甚至更糟，因为它们需要相应地更精细的离散化，从而导致离散动作数量激增。 如此大的动作空间很难有效地探索，因此在这种情况下成功地训练类似DQN的网络可能很棘手。 此外，动作空间的单纯离散化会不必要地丢弃有关动作域结构的信息，这对于解决许多问题可能是必不可少的。</p>
<p>​    在这项工作中，我们提出了一种使用深度函数逼近器的model-free，off-policy的actor-critic算法，该算法可以学习高维，连续动作空间中的策略。我们的工作基于确定性策略梯度（DPG）算法（其自身类似于NFQCA，并且可以找到类似的想法）。 然而，正如我们在下面显示的那样，这种针对行为者的方法与神经函数近似器的单纯应用对于具有挑战性的问题是不稳定的。</p>
<p>​    在这里，我们结合了行动者批评方法和最近从Deep Q Network（DQN）成功获得的见解。 在DQN之前，通常认为使用大型非线性函数逼近器学习价值函数既困难又不稳定。 由于以下两项创新，DQN能够使用此类函数逼近器以稳定且健壮的方式学习价值函数：1.通过从重播缓冲区中抽取样本，对网络进行off-policy训练，以最大程度地减少样本之间的相关性； 2.用目标Q网络训练网络，以在时间差备份期间给出一致的目标。 在这项工作中，我们利用了相同的思想以及批处理规范化，这是深度学习的最新进展。</p>
<p>​    为了评估我们的方法，我们构造了各种具有挑战性的物理控制问题涉及复杂的多关节运动，不稳定和丰富的接触动力学以及步态行为。其中包括经典的问题，例如cartpole swing-up问题以及许多新领域。 机器人控制的长期挑战是直接从原始的感官输入（例如视频）中学习动作策略。 因此，我们将固定的视点相机放置在模拟器中，并尝试使用低维观测（例如，关节角度）以及直接从像素中进行所有任务。</p>
<p>​    我们的无模型方法，我们称为深度DPG(DDPG)，可以使用低维观测(笛卡尔坐标或关节角)来学习我们所有任务的竞争策略。使用相同的超参数和网络结构。在许多情况下，我们还能够直接从像素中学习好的策略，再次保持超参数和网络结构不变。</p>
<p>​    该方法的一个关键特征是它的简单性：它只需要简单的actor-critic体系结构和学习算法，而很少有“运动部件”，因此易于实现和扩展更困难的问题和更大的网络。 对于物理控制问题，我们将我们的结果与规划器计算的基线进行比较，该规划器可以完全访问基础模拟动力学及其派生函数（请参阅补充信息）。 有趣的是，在某些情况下，即使从像素中学习时，DDPG有时也会找到超出规划器性能的策略（规划器始终在底层的低维状态空间上进行规划）。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>​    我们考虑一个标准的强化学习设置，该设置由一个在离散的时间步中与一个环境$$E$$交互的代理组成。 代理在每个时间步长$$t$$收到观察值$$x_{t}$$，采取行动，并收到标量奖励$$r_{t}$$。 在这里考虑的所有环境中，这些动作的实值均为$$a_{t}\in \mathbb{R}^{N}$$。 通常，可能会部分观察环境，因此可能需要观察的整个历史记录，动作对$$s_{t}=(x_{1},a_{1},…,a_{t-1},x_{t})$$来描述状态。 在这里，我们假设环境是完全观察到的，因此$$s_{t}=x_{t}$$。</p>
<p>​    代理的行为由策略$$\pi$$定义，该策略将状态映射到行为$$ \pi $$的概率分布：$$S\rightarrow P(A)$$。环境$$E$$也可能是随机的。 我们将其建模为马尔可夫模型状态空间为$$S$$的决策过程，动作空间$$A=\mathbb{R}^{N}$$，初始状态分布为$$p(s_{1})$$，转移概率$$p(s_{t+1}|s_{t},a_{t})$$和奖励函数$$r(s_{t},a_{t})$$。</p>
<p>​    从状态的收益被定义为具有折扣因子$$\gamma \in [0,1]$$的折扣未来奖励的总和$$R_{t}=\sum_{i=t}^{T}\gamma ^{(i-t)}r(s_{i},a_{i})$$。 请注意，返回值取决于选择的操作，因此在策略$$ \pi $$上，并且可能是随机的。 强化学习的目标是学习一种策略，该策略将从初始分布最大化的预期回报。 我们表示策略$$ \pi $$的折扣状态访问分布为$$ \rho ^{\pi }$$。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/05/20/Continuous-Control-With-Deep-Reinforcement-Learning/" data-id="ckox3cfeg0001agva04si999w" data-title="Continuous Control With Deep Reinforcement Learning" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Soft-Actor-Critic" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/20/Soft-Actor-Critic/" class="article-date">
  <time class="dt-published" datetime="2021-05-20T13:14:17.000Z" itemprop="datePublished">2021-05-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/05/20/Soft-Actor-Critic/">Soft Actor-Critic</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/05/20/Soft-Actor-Critic/" data-id="ckox3cff00009agvadkxp110e" data-title="Soft Actor-Critic" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-graph-network-study" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/17/graph-network-study/" class="article-date">
  <time class="dt-published" datetime="2021-05-17T14:50:56.000Z" itemprop="datePublished">2021-05-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/05/17/graph-network-study/">graph_network_study</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/05/17/graph-network-study/" data-id="ckox3cff8000dagvaa3nthh36" data-title="graph_network_study" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-docker-study" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/12/docker-study/" class="article-date">
  <time class="dt-published" datetime="2021-05-12T13:00:59.000Z" itemprop="datePublished">2021-05-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/05/12/docker-study/">docker study</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/koktlzz/p/14105026.html#%E4%BB%80%E4%B9%88%E6%98%AF%E5%AE%B9%E5%99%A8%E6%95%B0%E6%8D%AE%E5%8D%B7%EF%BC%9F">狂神docker</a></p>
<p>docker run -it -v /home/pql/company_code/tmp:/root/repos/tmp hub.digi-sky.com/aid/aicloud:dante-1.8.0-cuda11.1-cudnn8-runtime /bin/bash</p>
<p>使用-v挂载文件夹时，假如docker容器里已存在挂载的文件夹，并且宿主主机也存在挂载文件夹，宿主主机的文件夹内容会覆盖docker容器里的内容。如果宿主主机不存在挂载文件夹，会新建文件夹，并让docker容器里的文件夹同步（删除之前的内容）。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/05/12/docker-study/" data-id="ckox3cff5000bagva5bc6bpxn" data-title="docker study" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/" class="article-date">
  <time class="dt-published" datetime="2021-05-07T12:10:31.000Z" itemprop="datePublished">2021-05-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/">IMPALA:Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/" data-id="ckox3cfev0004agva7yy3fcr1" data-title="IMPALA:Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Asynchronous-Methods-for-Deep-Reinforcement-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/06/Asynchronous-Methods-for-Deep-Reinforcement-Learning/" class="article-date">
  <time class="dt-published" datetime="2021-05-06T12:26:41.000Z" itemprop="datePublished">2021-05-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/05/06/Asynchronous-Methods-for-Deep-Reinforcement-Learning/">Asynchronous Methods for Deep Reinforcement Learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Asynchronous-Methods-for-Deep-Reinforcement-Learning"><a href="#Asynchronous-Methods-for-Deep-Reinforcement-Learning" class="headerlink" title="Asynchronous Methods for Deep Reinforcement Learning"></a>Asynchronous Methods for Deep Reinforcement Learning</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​    我们提出了一个概念上简单和轻量级的深度强化学习框架，它使用异步梯度下降来优化深度神经网络控制器。我们提出了四种标准强化学习算法的异步变体，并表明并行actor-learner对训练有稳定的效果，使所有四种方法都能够成功地训练神经网络控制器。性能最好的方法是actor-critic的异步变体，它在一个多核CPU而不是GPU上训练一半的时间，超过了Atari领域上最先进的方法。此外，我们还证明了异步actor-critic在各种连续电机控制问题以及使用视觉输入导航随机三维迷宫的新任务上取得了成功。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    深度神经网络提供了丰富的表示，可以使强化学习(RL)算法能够有效地执行。然而，以前人们认为，简单的online RL算法与深度神经网络的结合从根本上是不稳定的。相反，已经提出了各种解决方案来稳定该算法（(Riedmiller, 2005; Mnih et al., 2013; 2015; Van Hasselt et al., 2015; Schulman et al., 2015a)。这些方法有一个共同的想法：online RL代理遇到的观察数据序列是非平稳的，与online RL更新有很强的相关性。通过将代理的数据存储在经验回放内存中，数据可以从不同的时间步长进行分组 (Riedmiller, 2005; Schulman et al., 2015a) 或随机采样 (Mnih et al., 2013; 2015; Van Hasselt et al., 2015) 。以这种方式聚合内存可以减少非平稳性和去相关的更新，但同时将这些方法限制为off-policy强化学习算法。</p>
<p>​    基于经验重放的深度RL算法在Atari2600等具有挑战性的领域中取得了前所未有的成功。然而，经验回放有几个缺点：它每次实际交互使用更多的内存和计算；它需要off-policy的学习算法，可以从旧策略生成的数据进行更新。</p>
<p>​    在本文中，我们提供了一个非常不同的深度强化学习范式。我们在环境的多个实例上异步地并行执行多个代理，而不是经验重放。这种并行性还将代理的数据重新关联到一个更平稳的过程中，因为在任何给定的时间步长中，并行代理都将经历各种不同的状态。这个简单的想法使基本的策略RL算法，比如Sarsa,n-step方法,actor-critic方法以及off-policy的RL算法，如Q-learning,将使用深度神经网络稳健和有效地应用。</p>
<p>​    我们的并行强化学习范式也提供了实际的好处。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/05/06/Asynchronous-Methods-for-Deep-Reinforcement-Learning/" data-id="ckox3cfcz0000agvagh1ccd4i" data-title="Asynchronous Methods for Deep Reinforcement Learning" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Grandmaster-level-in-StarCraftII-using-multi-agent-reinforcement-learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/06/Grandmaster-level-in-StarCraftII-using-multi-agent-reinforcement-learning/" class="article-date">
  <time class="dt-published" datetime="2021-05-06T12:16:47.000Z" itemprop="datePublished">2021-05-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/05/06/Grandmaster-level-in-StarCraftII-using-multi-agent-reinforcement-learning/">Grandmaster level in StarCraftII using multi-agent reinforcement learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/05/06/Grandmaster-level-in-StarCraftII-using-multi-agent-reinforcement-learning/" data-id="ckox3cfem0002agva0exh7kou" data-title="Grandmaster level in StarCraftII using multi-agent reinforcement learning" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/04/26/Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning/" class="article-date">
  <time class="dt-published" datetime="2021-04-26T13:10:23.000Z" itemprop="datePublished">2021-04-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/04/26/Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning/">Genetic State-Grouping Algorithm for Deep Reinforcement Learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning"><a href="#Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning" class="headerlink" title="Genetic State-Grouping Algorithm for Deep Reinforcement Learning"></a>Genetic State-Grouping Algorithm for Deep Reinforcement Learning</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​    虽然强化学习已经被认为是机器学习中最重要和最著名的技术之一，但由于其较长的初始学习时间和学习不稳定，它在现实问题中的适用性仍然有限。特别是在实时约束下，大量分支因子的问题仍然无法克服，这需要一种针对下一代强化学习的新方法。本文提出了基于深度强化学习的Genetic State-Grouping Algorithm。其核心思想是将整个状态集划分为几个状态组。每一组都由相互相似的状态组成，从而代表了它们的共同特征。然后使用遗传优化器来处理状态组，从而发现杰出的操作。这些步骤有助于深度Q网络避免过度的探索，从而导致不能显著减少初始学习时间。对实时战斗电子游戏(FightingICE)的实验证明了该方法的有效性。</p>
<p>关键词：强化学习，遗传算法，混合方法，蒙特卡罗技术</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​    强化学习方法(RL)击败了世界上最好的人类围棋玩家之一，引起了专家们的认真关注。它的基于试验和错误的迭代而不是大量的预定义数据集进行培训的性质在全世界引起了热烈的讨论和兴趣。然而，除了强化学习的所有成就外，它还暴露了许多技术困难，如学习不稳定和进入局部最优。具体地说，很难定义一个找到复杂问题中的全局解的奖励函数，它比其他类型的机器学习需要更多的计算资源和初始学习时间。之前的研究提出了各种解决这些缺点的解决方案，有些是利用实时视频游戏环境与现实世界相似，表明它们适合RL对内存和时间的苛刻需求。然而，这些研究的主要贡献仅限于只有几个分支因素的问题，因为提高它们的数量会呈指数级地增加所需的计算资源数量。</p>
<p>​    本文介绍了通过深度强化学习增强的Genetic State-Grouping Algorithm (GSGA)。它是遗传算法和State Grouping的集成，我们真正的方法，以追求提高深度强化学习在时间和资源效率方面的学习效率。我们展示了它在FightingICE中的表现，这是一个实时战斗视频游戏，由IEEE CoG完成。我们认为它是一个适当的试验场来验证我们提出的结构的有效性，因为它包含了各种类似于现实世界的环境因素。</p>
<p>​    FightingICE由 Intelligent Computer Entertainment Lab in Ritsumeikan University, Japan开发。因为它是一个为构建通用战斗游戏ai而设计的环境，所以在属性和风格方面，它是一种相当典型的类型。在每一轮中，两个玩家代理在二维空间中相互战斗，他们之间实例化一个物理距离。游戏向每个代理提供多达56个可执行操作。此外，根据交战规则，每个玩家被允许一次只使用一个线程，以复制真实世界的问题条件，这需要大量使用计算资源。总的来说,FightingICE玩家只利用单个线程在16.67ms的每个时间帧内必须考虑多达3136个动作状态。游戏中的物理空间由$$960\times 640$$像素组成，分为5像素大小的房间，每个代理都可以放置在里面。因此，每个代理总共有$$192\times128$$的状态空间来重新定位。</p>
<p>​    本文的排列格式如下：第二章我们介绍了一些与本文主要思想有关的研究。在第三章中，我们介绍了我们的技术的核心理论，以及关于电子游戏环境的细节。第4章介绍了我们的模型及其竞争对手的过程和实验结果，第五章提供了我们对模型整体性能的评论和关注。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>​    在强化学习过程中，人们多次尝试减少长初始学习时间，稳定整体学习的过程。Justesen等人提出一种强化学习代理，有效解决实时问题的时间划分和对抗环境，Langford提出一种有效的探索方法。一些研究选择通过增强学习稳定性来降低结构复杂性，尽管他们的方法对涉及多个分支因素的问题缺乏足够的适用性，但在这些问题中，环境高度复杂，或者为代理提供了大量的操作选择。此外，(Justesen等人，2016)提出了一个解决在线进化的巨大分支因素的解决方案，尽管它几乎不适用于实时视频游戏。在具有多变量的实时问题中，大量的分支因素特别吓人，其中分支的数量增长得非常快。尽管RL具有探索和经验学习的性质，适合于开发实时自适应AI，但在现实世界中常见的多分支因子问题中，RL的学习效率会大大降低。</p>
<p>​    其他几项研究采用蒙特卡罗树搜索（MCTS），结合进化算法，作为强化学习的补充在开发视频游戏AI时，证明了其在足够时间内找到最佳解决方案的能力。尽管如此，在大多数方面强化学习需要大量计算资源和时间的本质仍然是不可妥协的。</p>
<h2 id="提出的方法"><a href="#提出的方法" class="headerlink" title="提出的方法"></a>提出的方法</h2><p>​    强化学习是一种机器学习技术，它在代理与未知环境交互时为他们找到最佳的学习策略。这种过程通常被形式化为马尔可夫决策过程(MDPs)，它可以由4个元素$$(S,A,P,R)$$来定义。在每个时间戳$$t$$，agent观察一组状态$$s_{t}\in S$$，并采取一组操作$$a_{t}\in A$$。与转换函数$$P$$相关联，所有状态都为马尔可夫的环境，决定奖励$$r_{t}\sim R(s_{t},a_{t})$$和后续或以下状态$$s_{t+1}\sim P(s_{t},a_{t})$$。我们将根据一阶马尔可夫假设来解释所提出的方法。</p>
<p>​    图1从概念上表示了我们提议的GSGA的总体结构。它由三个主体组成：1）state grouping，2） genetic optimizer，和3) deep Q network。其详细的程序描述如下。</p>
<p><img src="/2021/04/26/Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning/figure1.png"></p>
<p>​        State Grouping(SG)是一种技术，通过将相似的状态结合在一起，压缩状态线的大小，包括物理距离。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/04/26/Genetic-State-Grouping-Algorithm-for-Deep-Reinforcement-Learning/" data-id="ckox3cfet0003agva6mmu2cgi" data-title="Genetic State-Grouping Algorithm for Deep Reinforcement Learning" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-TLeague" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/03/17/TLeague/" class="article-date">
  <time class="dt-published" datetime="2021-03-17T13:12:44.000Z" itemprop="datePublished">2021-03-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/03/17/TLeague/">TLeague</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Tleague-A-Framework-for-Competitive-Self-Play-based-Distributed-Multi-Agent-Reinforcement-Learning"><a href="#Tleague-A-Framework-for-Competitive-Self-Play-based-Distributed-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Tleague:A Framework for Competitive Self-Play based Distributed Multi-Agent Reinforcement Learning"></a>Tleague:A Framework for Competitive Self-Play based Distributed Multi-Agent Reinforcement Learning</h1><p>多智能体强化学习需要大量的数据，为了解决这个问题实现了TLeague框架。TLeague的目标是实现大规模训练以及主流的Competitive Self-Play (CSP)算法。训练可以在单机和多机。在 StarCraft II, ViZDoom and Pommerman中展示了它的效率和有效性。<a target="_blank" rel="noopener" href="https://github.com/tencent-ailab/tleague_projpage">code</a></p>
<p>该框架用于多智能体竞争self-play的学习。框架如下图所示</p>
<p><img src="/2021/03/17/TLeague/framework.png"></p>
<ul>
<li>Learner</li>
</ul>
<p>每个Learner从与之对应的Actor接收trajectories进行学习。每个Learner对应一个ReplayMem和DataServer，进行数据预处理和存储在ReplayMem中。学习开始时会从LeagueMgr接收到task，可以从中知道需要学习的策略。在学习过程中也会更新策略到ModelPool中。</p>
<ul>
<li>InfServer</li>
</ul>
<p>从不同的Actor中收集一批observation进行预测，并返回给对应的Actor；</p>
<ul>
<li>ModelPool</li>
</ul>
<p>存储策略参数作为对手模型池。Actor和Learner会读取和写入策略参数。</p>
<ul>
<li>LeagueMgr</li>
</ul>
<p>用于协调其它模块。GameMgr有不同的对手选取算法。HyperMgr进行超参数的管理。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/03/17/TLeague/" data-id="ckox3cff4000aagvahmqfeko9" data-title="TLeague" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Sample-Factory-Egocentric-3D-Control-from-Pixels-at-100000-FPS-with-Asynchronous-Reinforcement-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/01/20/Sample-Factory-Egocentric-3D-Control-from-Pixels-at-100000-FPS-with-Asynchronous-Reinforcement-Learning/" class="article-date">
  <time class="dt-published" datetime="2021-01-20T12:25:13.000Z" itemprop="datePublished">2021-01-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/01/20/Sample-Factory-Egocentric-3D-Control-from-Pixels-at-100000-FPS-with-Asynchronous-Reinforcement-Learning/">Sample Factory:Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.11751">paper</a></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    强化学习实验的规模不断扩大，使得研究人员在训练复杂的电子游戏代理和在机器人的模拟到现实转移方面都取得了前所未有的成果。 通常，这种实验依赖于大型分布式系统，需要昂贵的硬件设置，限制了对这一令人兴奋的研究领域的更广泛访问。 在本工作中，我们旨在通过优化强化学习算法的效率和资源利用来解决这个问题，而不是依赖分布式计算。 我们提出了“Sample Factory”，这是一个为单机情况下优化的高吞吐量训练系统。 我们的体系结构结合了一个高效的、异步的、基于GPU的采样器和off-policy校正技术，使我们能够在3D中实现高于$$10^{5}$$个环境帧/秒的吞吐量，而个环境帧/秒的吞吐量。 我们扩展了Sample Factory，以支持self-play和 population-based的训练，并应用这些技术来为多人第一人称射击游戏训练高能力的代理。</p>
<p>[Github](<a target="_blank" rel="noopener" href="https://github.com/">https://github.com/</a> alex-petrenko/sample-factory )</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>​    模拟环境中的训练代理是当代强化学习研究的基石。 近年来，通过应用强化学习方法在这些快速和高效的环境中训练代理，无论是解决复杂的计算机游戏 (Dosovitskiy &amp; Koltun, 2017; Jaderberg et al., 2019; Vinyals et al., 2019) ，还是通过模拟到真实的转移来解决复杂的机器人控制问题 (Müller et al., 2018;Hwangbo et al., 2019; Molchanov et al., 2019; Andrychowicz et al., 2020)。</p>
<p>​    尽管现代学习方法的采样效率有了很大的提高，但其中大多数仍然需要大量的数据。 在大多数情况下，近年来的结果水平已经上升，因为实验的规模增加，而不是学习的效率。 在复杂环境中进行的数十亿规模的实验现在相对普遍 (Horgan et al., 2018; Espeholt et al., 2018; Kapturowski et al., 2019)，最先进的成果在一次训练中消耗了数万亿次环境转变 (Berner et al., 2019)。</p>
<p>​    为了尽量减少这些大规模实验的周转时间，常用的方法是使用由数百台单独机器组成的分布式超级计算系统 (Berner et al., 2019)。 在这里，我们表明，通过优化结构和提高强化学习算法的资源利用率，我们可以在数十亿个环境转变上训练代理，即使在单个计算节点上也是如此。 我们提出了“Sample Factory”，一个为单机场景优化的高吞吐量训练系统。 Sample Factory基于异步近端策略优化(APPO)算法，是一种强化学习架构，它允许我们在一个只有一个GPU的多核计算节点上积极地并行化经验收集并实现高达130000FPS（每秒环境帧）的吞吐量。 我们描述了理论和实践优化，使我们能够在广泛可用的商品硬件上实现极端帧速率。</p>
<p>​    我们在一组具有挑战性的3D环境中评估我们的算法，并演示如何利用大量的模拟经验来训练达到高技能水平的代理。 然后，我们扩展了Sample Factory，以支持self-play和population-based训练，并应用这些技术来为一个完整的多人游戏的Doom培训高能力的代理 (Kempka et al., 2016)。</p>
<h1 id="Sample-Factory"><a href="#Sample-Factory" class="headerlink" title="Sample Factory"></a>Sample Factory</h1><p>​    Sample factory是一种在单机上进行高吞吐量强化学习的体系结构。 在设计系统时，我们专注于使所有关键计算完全异步，以及最小化组件之间的延迟和通信成本，充分利用快速本地消息传递。</p>
<p>​    一个典型的强化学习场景涉及三个主要的计算工作负载：环境模拟、模型推理和反向传播。 我们的主要动机是构建一个系统，其中最慢的三个工作负载从来不需要等待任何其他进程提供执行下一次计算所需的数据，因为算法的总体吞吐量最终由吞吐量最低的工作负载定义。 为了最小化进程等待的时间，我们需要保证输入的新部分总是可用的，即使在下一步的计算即将开始之前。 计算密集型工作负载从不闲置的系统可以达到最高的资源利用率，从而接近最佳性能。</p>
<h2 id="High-level-design"><a href="#High-level-design" class="headerlink" title="High-level design"></a>High-level design</h2><p>​    最小化所有关键计算的空闲时间的愿望激发了系统的高级设计（图1）。 我们将每个计算工作负载与三种专用类型的组件之一相关联。 这些组件使用基于FIFO队列和共享内存的快速协议相互通信。 队列机制为连续和异步执行提供了基础，只要队列中有要处理的东西，就可以立即启动下一个计算步骤。 将每个工作负载分配给专用组件类型的决定也允许我们独立地并行化它们，从而实现优化的资源平衡。 这与以前的工作不同 (Mnih et al., 2016; Espeholt et al., 2018)，其中单个系统组件，如actor通常负有多重责任。 涉及的三种类型的组件是 rollout workers, policy workers, and learners。</p>
<p><img src="/2021/01/20/Sample-Factory-Egocentric-3D-Control-from-Pixels-at-100000-FPS-with-Asynchronous-Reinforcement-Learning/figure1.png"></p>
<p>Rollout workers单独负责环境模拟。 每个rollout worker托管$$k≥1$$个环境实例，并依次与这些环境交互，收集观察$$x_{t}$$并奖励$$r_{t}$$。 请注意，rollout worker没有自己的策略副本，这使得它们非常轻量级，允许我们在现代多核CPU上大规模并行化收集。</p>
<p>然后将观察$$x_{t}$$和代理的隐藏状态$$h_{t}$$发送给policy worker，policy worker从多个rollout worker收集$$x_{t}$$、$$h_{t}$$的批次，并调用策略$$\pi$$，由神经网络参数化$$\theta <em>{\pi }$$以计算动作分布$$\mu \left ( a</em>{t}|x_{t},h_{t} \right )$$和更新的隐藏状态$$h_{t+1}$$。 然后从分布$$\mu$$中采样$$a_{t}$$的动作，并与$$h_{t+1}$$一起通信回相应的rollout worker。 这个rollout worker使用动作$$a_{t}$$推进模拟和收集下一个观察$$x_{t+1}$$和奖励$$r_{t+1}$$。</p>
<p>Rollout workers保存每个环境转换到共享内存中的轨迹缓冲区。 一旦模拟了$$T$$环境步骤，观察、隐藏状态、动作和奖励的轨迹$$\tau = x_{1},h_{1},a_{1},r_{1},…,x_{T},h_{T},a_{T},r_{T}$$就可以提供给learner。 learner不断地处理批量的轨迹，并更新actor $$\theta _{\pi }$$和critic $$\theta _{V}$$的参数。 这些参数更新一旦可用就发送给policy worker，这减少了前一个版本模型收集的经验量，最小化了平均policy lag。 这就完成了一次训练迭代。</p>
<p>Parallelism。如前所述，rollout worker不拥有该策略的副本，因此基本上是环境实例周围的薄包装。 这允许它们大规模并行化。 此外，Sample Factory还将policy worker并行化。 这可以实现，因为所有当前轨迹数据$$(x_{t},h_{t},a_{t},…)$$都存储在所有进程都可以访问的共享张量中。 这允许policy workers本身处于无状态，因此，他们中的任何一个都可以很容易地处理来自单个环境的连续轨迹步骤。 在实际场景中，2到4个policy worker实例很容易使rollout worker操作饱和，加上一个特殊的采样器设计（第3.2节），使我们能够消除这一潜在的瓶颈。</p>
<p>Learner是我们运行单一副本的唯一组件，至少只要涉及单一策略训练（多策略训练在3.5节中讨论）。 然而，我们可以通过利用Learner上的多个加速器数据并行训练和Hogwild风格的参数更新(Recht et al., 2011)。 加上在复杂环境中稳定训练通常需要的大批处理大小，这使Learner有足够的吞吐量来匹配经验收集率，除非计算图是高度非平凡的。</p>
<h2 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h2><p>​    Rollout workers和policy workers共同组成采样器。 采样子系统对RL算法的吞吐量影响最大，因为它往往是瓶颈。 我们提出了一种具体的实现采样器的方法，它允许通过最小化rollout workers的空闲时间来优化资源利用。</p>
<p>​    首先，注意训练和经验收集是解耦的，因此可以在反向传播步骤中收集新的环境转换。 Rollout workers也没有参数更新，因为操作生成的工作是卸载给policy worker的。 然而，如果没有解决，这仍然使rollout workers等待policy workers产生的行动，并通过进程间通信转移回来。</p>
<p>​    为了缓解这种低效率，我们使用 Double-Buffered Sampling（图2）。 与其只在rollout worker上存储单个环境，不如存储环境$$E_{1},…,E_{k}$$的向量，其中k甚至是为了简单起见。 我们将这个向量分成两组$$E_{1},…,E_{k/2}$$，$$E_{k/2+1},…,E_{k}$$，并在它们之间交替进行。 当第一组环境正在step，第二组的操作是根据policy worker计算的，反之亦然。 有了足够快的policy worker和$$k$$的正确调优值，我们可以完全屏蔽通信开销，并确保在采样期间充分利用CPU核心，如图2所示。 用于双缓冲器的最大性能采样我们希望$$k/2&gt; \left \lceil t_{inf}/t_{env} \right \rceil$$，其中$$t_{inf}$$和$$t_{env}$$分别是平均推理和模拟时间。</p>
<p><img src="/2021/01/20/Sample-Factory-Egocentric-3D-Control-from-Pixels-at-100000-FPS-with-Asynchronous-Reinforcement-Learning/figure2.png"></p>
<h2 id="Communication-between-components"><a href="#Communication-between-components" class="headerlink" title="Communication between components"></a>Communication between components</h2><p>​    解锁本地单机设置的全部潜力的关键是利用系统组件之间的快速通信机制。 如图1所示，信息流有四条主要途径：rollout和policy worker之间的双向沟通，rollout将完整的轨迹传递给learner，以及将参数更新从learner转移到policy worker。 对于前三种交互，我们使用基于PyTorch的共享内存张量机制 (Paszke et al., 2019)。 我们注意到，RL算法中使用的大多数数据结构可以表示为固定形状的张量，无论它们是轨迹、观测还是隐藏状态。 因此，我们在系统RAM中预先分配足够数量的张量。 每当组件需要通信时，我们都会将数据复制到共享张量中，并且只通过FIFO队列发送这些张量的索引，使得消息与传输的总体数据量相比很小。</p>
<p>​    对于参数更新，我们使用GPU上的内存共享。 每当需要更新模型时，policy worker只需将权重从共享内存复制到模型的本地副本。</p>
<p>​    与许多流行的异步和分布式实现不同，我们不执行任何类型的数据序列化作为通信协议的一部分。 在全节流阀下，SampleFactory每秒产生和消耗超过1GB的数据，即使是最快的序列化/去序列化机制也会严重阻碍吞吐量。</p>
<h2 id="Policy-lag"><a href="#Policy-lag" class="headerlink" title="Policy lag"></a>Policy lag</h2><p>​    Policy lag是异步RL算法的固有特性，它是收集经验（行为策略）的策略与学习的目标策略之间的差异。 这种差异的存在为off-policy训练提供了条件。off-policy学习对于策略梯度方法来说是很困难的，其中模型参数通常按照$$\bigtriangledown log\mu (a_{s}|x_{s})q(x_{s},a_{s})$$的方向更新，其中$$q(x_{s},a_{s})$$是策略state-action值的估计。 策略滞后越大，就越难从行为策略中使用一组样本$$x_{s}$$正确估计这个梯度。 从经验上讲，这在学习涉及循环策略、高维观测和复杂行动空间的问题方面变得更加困难，在这些问题中，即使是非常相似的政策也不太可能在很长的轨迹上表现出相同的性能。</p>
<p>​    异步RL方法中的策略滞后可能是通过使用旧策略在环境中的作用引起的，或者在一次迭代中从并行环境中收集更多的轨迹，而不是学习者在一个batch中摄入的轨迹，导致部分经验在处理时变得不符合策略。 我们处理第一个问题，一旦有了新的参数立即更新政策工作者的模型。 在Sample factory中，参数更新很便宜，因为模型存储在共享内存中。 一个典型的更新需要小于1ms，因此我们收集了与“master”不同的策略非常少的经验。</p>
<p>​    然而，不一定能够消除第二个原因。 在RL中，并行收集来自许多环境实例的训练数据是有益的。 这不仅使经验相互关联，它还允许我们利用多核CPU，并且具有更大的$$k$$值（每个核心的环境），充分利用双缓冲采样器。 在一次经验收集的“迭代”中，n个rollout workers，每个运行k个环境，将产生总共$$N_{iter}=n\times k\times T$$个样本。 由于我们在学习者步骤之后立即更新策略工作者，可能在轨迹的中间，这导致轨迹中最早的样本平均滞后于$$N_{iter}/N_{batch}-1$$策略更新，而最新的样本没有滞后。</p>
<p>​    可以通过减小$$T$$或增加最小批量大小$$N_{batch}$$来最小化策略滞后。 两者都对学习有影响。 我们通常想要更大的T，在$$2^5-2^7$$范围内，通过反向传播与时间循环策略，大的最小批量可能会降低样本效率。 最佳批次大小取决于特定的环境，更大的批次被证明适合于具有噪声梯度的复杂问题(McCandlish et al., 2018)。</p>
<p>​    此外，还有两大类用于处理off-policy学习的技术。 第一个想法是应用信任区域方法 (Schulman et al., 2015; 2017)通过在学习期间保持接近行为策略，我们提高了使用该策略的样本获得的梯度估计的质量。 另一种方法是使用重要性采样来纠正目标价值函数$$V^\pi$$以改进目标政策下折扣奖励之和的近似 (Harutyunyan et al., 2016). </p>
<p>IMPALA (Espeholt et al., 2018) 介绍了V-trace算法，该算法使用截断的重要性抽样权重来修正值目标。 这有助于提高off-policy学习的稳定性和样本效率。</p>
<p>​    这两种方法都可以独立应用，因为V-trace纠正了我们的训练目标，信任区域防止破坏性参数更新。 因此，我们在Sample Factory实现了V-trace和PPO clipping。 是否使用这些方法可以被认为是特定实验的超参数选择。 我们发现PPO clipping和V-trace的结合在任务之间很好地工作，并产生稳定的训练，因此我们决定在本文报道的所有实验中使用这两种方法。</p>
<h2 id="Multi-agent-learning-and-self-play"><a href="#Multi-agent-learning-and-self-play" class="headerlink" title="Multi-agent learning and self-play"></a><strong>Multi-agent learning and self-play</strong></h2><p>​    通过多智能体强化学习和self-play，在深度RL方面取得了一些最先进的最新成果 (Bansal et al., 2018; Berner et al., 2019)。 通过self-play训练的特工比在固定场景中训练的对手表现出更高的技能水平 (Jaderberg et al.,2019)。 随着策略在self-play过程中的改进，它们产生了一个逐渐增加复杂性的训练环境，自然为代理提供了一个课程，并允许他们逐步学习更复杂的技能。 复杂行为(例如合作和工具使用)已被证明出现在这些培训情景中 (Baker et al., 2020)。</p>
<p>​    还有证据表明，在多智能体环境中一起训练的代理群体可以避免常规的自玩设置所经历的一些失败模式，例如早期收敛到局部最优或过度拟合。 不同的培训人群可以使代理人接触到更广泛的对抗性政策，并产生更强大的代理人，在复杂的任务中达到更高的技能水平(Vinyals et al., 2019; Jaderberg et al., 2019)。</p>
<p>​    为了释放我们系统的全部潜力，我们增加了对多代理环境的支持，以及对代理的培训人群。Sample Factory自然扩展到多智能体和多策略学习。 由于rollout workers只是环境实例的包装器，他们完全不知道提供操作的策略。 因此，为了在培训过程中增加更多的政策，我们只是催生了更多的policy workers和更多的learners来支持他们。 在rollout workers中，对于每个多代理环境中的每个代理，我们在每一集开始时从族群中抽样一个随机的策略$$\pi_{i}$$。 然后使用一组FIFO队列将操作请求路由到相应的policy workers，每$$\pi_{i}$$一个队列。 我们在这项工作中使用的基于族群的设置在第4节中得到了更详细的解释。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/01/20/Sample-Factory-Egocentric-3D-Control-from-Pixels-at-100000-FPS-with-Asynchronous-Reinforcement-Learning/" data-id="ckox3cfez0008agva90tabluw" data-title="Sample Factory:Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/05/20/Continuous-Control-With-Deep-Reinforcement-Learning/">Continuous Control With Deep Reinforcement Learning</a>
          </li>
        
          <li>
            <a href="/2021/05/20/Soft-Actor-Critic/">Soft Actor-Critic</a>
          </li>
        
          <li>
            <a href="/2021/05/17/graph-network-study/">graph_network_study</a>
          </li>
        
          <li>
            <a href="/2021/05/12/docker-study/">docker study</a>
          </li>
        
          <li>
            <a href="/2021/05/07/IMPALA-Scalable-Distributed-Deep-RL-with-Importance-Weighted-Actor-Learner-Architectures/">IMPALA:Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 QilongPan<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>